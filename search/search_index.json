{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"pages/components/","title":"Components","text":"<p>Atlas is composed of multiple components, all connected through data pipelines.</p> <p>The following sections introduce each component and describe its role in Atlas.</p>"},{"location":"pages/components/activity-database/","title":"Activity Database","text":"<p>Note</p> <p>This section is draft, pending development of the Activity Database 2.0.</p> <p>Note</p> <p>The Activity DB table will serve as the primary source of truth for Activity IDs. It is critical that these IDs do not change once they are read in to the Mission Database. The form UI for the Activity DB and restricted control of the database table in Google Sheets should ensure this.</p> <p>Warning</p> <p>Deletions from the Activity DB will not be carried forward into the Mission Database automatically. We believe this behavior is the desired behavior. Records can be deleted manually from the Mission Database.</p>"},{"location":"pages/components/activity-database/#data-pipelines","title":"Data Pipelines","text":"<p>Data from the Activity Database Google Sheet are queried in the <code>Query - for DB Web Frontend</code> tab to exclude Pre-Awards and Personal Services Contracts and limit data read by the front end to the highest-priority fields.</p> <p>From the <code>Query - for DB Web Frontend</code> tab, data are imported to the TableGen Google Sheet <code>Data</code> tab.</p>"},{"location":"pages/components/activity-database/#assign-domain-owners","title":"Assign domain owners","text":"<p>Assign domain owners to each section so that the Data Steward can track which team is the official source of each domain (e.g., funding is owned by OAA). Domain owners can help design each form section to meet their needs.</p>"},{"location":"pages/components/activity-database/#sub-components","title":"Sub components","text":"<ul> <li>Activity DB data backups: a script container-bound to the Activity Database Google Sheet runs daily and stores backups in this folder</li> <li>Activity DB file uploads: any file uploaded through the Activity Database Form is store here with the Activity ID prepended.</li> <li>FormGen: creates the form and adds fields to the Activity Database Google Sheet.</li> <li>TableGen: creates the table displayed in the application front end and serves the filtered data from the Activity Database Google Sheet.</li> </ul>"},{"location":"pages/components/activity-folder/","title":"Activity Folder Template","text":"<p>A well-organized Activity Folder supports reporting of data by partners and aggregation of data across activities by the Data Steward. Atlas requires the standard Activity Folder template is adopted by all Activities. The Activity Folder Template is designed to be flexible in light of the unique nature of each activity while also including standard components that support the Data Steward to access and aggregate data.</p> <p>Detailed instructions for working with A/CORs and partners are provided here.</p> <p>Note</p> <p>Some Implementing Partners may not be able to create Google accounts, which are required to access restricted-access Google Folders. In these cases, the Data Steward should work with the A/COR to develop an alternative solution.</p>"},{"location":"pages/components/activity-folder/#folder-structure","title":"Folder Structure","text":"<p>The structure outlined below is the recommended Activity Folder structure. Additional folders and subfolders may be included to meet the specific needs of the activity.</p> <pre><code>*\n+-- datasets/\n    +-- implementation-activities.gsheet\n    +-- beneficiaries.gsheet\n+-- archive/\n+-- Data-Inventory.gsheet\n+-- Activity-MECLA-Plan-DMP.gdoc\n+-- Activity-Location-Data-Template.gsheet\n+-- Activity-PM-Tracker.gsheet // optional\n</code></pre>"},{"location":"pages/components/activity-folder/#components","title":"Components","text":"<ul> <li>Datasets Folder: the datasets folder stores all data assets collected or utilized by the Implementing Partner. Here we illustrate two very common data assets collected by Implementing Partners: a list of the implementation activities and a list of beneficiaries. These datasets support both activity location data and performance monitoring.</li> <li>Archive Folder: store past Activity Monitoring, Evaluation and Learning Plans (AMPELPs) and other expired documents.</li> <li>Data Inventory: this Google Sheet lists and provides metadata for each data asset. The Data Inventory is created during development of the Data Management Plan.</li> <li>Activity MECLA &amp; Data Management Plan: this document helps Implementing Partners plan for MECLA activities and data management.</li> <li>Activity Location Data Template: this Google Sheet is the standard template for collecting Activity Location Data.</li> <li>Activity PM Tracker: this Google Sheet provides a standard template for reporting performance monitoring data, however its use is optional.</li> </ul>"},{"location":"pages/components/activity-folder/#data-steward-access","title":"Data Steward Access","text":"<p>The Data Steward accesses the Activity Folders programmatically to aggregate data including activity location data, thematic data, and performance monitoring data. The Data Steward logs each Activity Folder as it is created in the Activity Folder Inventory. The Data Steward Admin Tool uses this inventory to   access the data sources in each Activity Folder for various use cases, such as compiling all activity location data in the Activity Location Data Compiler.</p>"},{"location":"pages/components/activity-location-compiler/","title":"Activity Location Data Compiler","text":"<p>The Activity Location Data Compiler aggregates Activity Location Data from all Activity Location Data Trackers. A script in the Data Steward Admin Tool, <code>compile-ald.gs</code>, reads data from each Activity's Tracker and saves it to the Compiler (see here for full details). </p> <p>The Activity Location Data Compiler converts exact site locations (latitude and longitude) to hexagon indices using the library H3. These generalized locations protect stakeholders from identification while still allowing stakeholders to collaborate when working in similar geographic areas. </p> <p>The Activity Location Data Compiler also identifies the Municipality and Department in which each coordinate is located.</p>"},{"location":"pages/components/activity-location-tracker/","title":"Activity Location Data Tracker","text":"<p>The Activity Location Data Tracker is a template for partners to report Activity Location Data. It is stored within the Activity Folder template; a copy is created for each new Activity (see the workflow here).</p> <p>The Activity Location Data Tracker is a Google Sheets equivalent of the batch upload template available on DIS. Atlas does not currently support customization of this template.</p>"},{"location":"pages/components/ald-portal-public/","title":"Activity Location Data Portal (Public)","text":"<p>Share Activity Location Data with partners publicly</p> <p>The Activity Location Data Portal aggregates exact site location data using the h3 hexagonal grid to generalize data for public consumption. Implementing Partners and other stakeholders can identify where USAID is operating and collaborate with others. The Activity Location Data Portal (Public) is a component of the USAID/Guatemala Data Hub. (The Activity Location Data Portal is not currently available).</p> <p></p>"},{"location":"pages/components/atlas-distro/","title":"Atlas Distribution Folder","text":"<p>The Atlas Distribution Folder allows new users of Atlas to quickly set up the components stored in Google Drive. A script in the Data Steward Admin Tool (<code>copy_folder.gs</code>) includes a function to copy all files and folders into the user's own directory (<code>copyAtlas</code>), see setup instructions here. </p> <p>Scripts are copied into the user's root Google Drive folder and are most easily accessed at https://script.google.com/home.</p> <p>The directory has the following structure:</p> <pre><code>Atlas Distro/\n+-- Modules/\n    +-- Thematic Data/\n        +-- Internal/\n            +-- raw/\n            +-- SENSITIVE/\n            +-- Reports/\n            +-- Spatial/\n            +-- Tabular/\n            +-- internal - Data Catalog.gsheet\n            +-- README.gdoc\n        +-- Shared Externally/\n            +-- Reports/\n            +-- Spatial/\n            +-- Tabular/\n            +-- Data Catalog.gsheet\n    +-- Activity Database/\n        +-- Activity Database.gsheet\n        +-- FormGen.gsheet\n        +-- TableGen.gsheet\n        +-- adb_stage.gsheet\n    +-- Activity Location Data/\n        +-- Activity Location Data Compiler.gsheet\n        +-- Activity Location Data (Public).gsheet\n        +-- ALD Inventory.gsheet\n        +-- ald_stage.gsheet\n    +-- Performance Measures/ \n        +-- // Contents TBD\n    +-- Activity Folder [Template]/\n        +-- datasets/\n        +-- Activity MECLA Plan DMP.gdoc\n        +-- Activity Data Inventory.gsheet\n        +-- Activity Location Data Tracker vX.x.gsheet\n        +-- Activity PM Tracker.gsheet // TBD\n+-- Admin/\n    +-- SQL/\n    +-- tmp/\n+-- Atlas Data Inventory.gsheet\n+-- README.gdoc\n</code></pre>"},{"location":"pages/components/data-catalog/","title":"Data Catalog","text":"<p>A searchable Google Sheets-based data catalog</p> <p>The Data Catalog is simple Google Apps Script search front end for a collection of data assets with Bootstrap styling.  The Data Catalog is a component of the USAID/Guatemala Data Hub. Find it at sites.google.com/usaid.gov/guatemaladatahub.</p> <p></p>"},{"location":"pages/components/data-steward-admin/","title":"Data Steward Admin Tool","text":"<p>The Data Steward Admin Tool supports the Data Steward to manage and maintain the semi-automated data pipelines. </p> <p>The tool contains multiple script files:</p> <ul> <li><code>utils.gs</code>: utilities functions for each script</li> <li><code>activity-data.gs</code>: creates the file <code>load_activity_data.sql</code> in the Data Admin SQL folder.</li> <li><code>activity-location-data.gs</code>: creates the file <code>load_activity_location_data.sql</code> in the Data Admin SQL folder.</li> <li><code>thematic-data.gs</code>: creates the file <code>load_thematic_data.sql</code> in the Data Admin SQL folder.</li> <li><code>copy-folder.gs</code>: copies a folder and subfolder to another location. Used to copy Atlas during initial installation and to copy Activity Folders during activity start up.</li> <li><code>compile-ald.gs</code>: aggregates Activity Location Data from all Activity Location Data Trackers and retrieves hexagonal index for each exact site location.</li> </ul> <p>Each <code>load*.sql</code> file can be read by pgAdmin using psql to update records in the Mission Database.</p>"},{"location":"pages/components/map-viewer/","title":"Map Viewer","text":"<p>Create choropleths from tabular data</p> <p>Map Viewer is simple Google Apps Script map platform built on Leaflet.  Users select a data asset and attribute to quickly create a choropleth map. The Map Viewer is a component of the USAID/Guatemala Data Hub. Find it at sites.google.com/guatemala-data-hub.</p> <p></p>"},{"location":"pages/components/mission-database/","title":"Mission Database","text":"<p>The Mission Database is a PostgreSQL database designed to provide quick access for the Data Steward to core Mission data. This component is optional. </p> <p>See the database schema on dbdocs.io.</p> <p>PostgreSQL was chosen as the relational database management system for Atlas for multiple reasons:</p> <ul> <li>Extensions: PostgreSQL includes extensions for PostGIS, a geospatial data management solution, H3, which is used to generalize activity location data, and UUID for universally unique identifiers. </li> <li>Scalable: PostgreSQL is a robust, scalable solution that can grow with the Mission's needs and can be easily deployed on the cloud should that option become important.</li> </ul> <p>Additional detail on the rationale for selecting PostgreSQL can be found here.</p> <p>For detailed guidance on setting up the Mission Database, see Setup.</p>"},{"location":"pages/components/thematic-database/","title":"Thematic Database","text":"<p>Thematic data is a catch-all term to refer to all data assets not otherwise defined. Thematic data that have secondary analysis value<sup>1</sup> for the Mission are stored in the Thematic Database. The Thematic Database serves data to Web Applications for consumption by stakeholders and with the Mission Database to enrich Mission-wide analyses.</p>"},{"location":"pages/components/thematic-database/#public-and-partner-datasets","title":"Public and Partner Datasets","text":"<p>The Thematic Database stores both public and partner datasets.</p>"},{"location":"pages/components/thematic-database/#public-datasets","title":"Public Datasets","text":"<p>The Data Steward will acquire publicly available datasets from public institutions and other organizations that are relevant to the Mission. These datasets can be included in the Thematic Database. Any datasets included by the Data Steward should be updated regularly to ensure the Thematic Database is up-to-date and trustworthy.</p>"},{"location":"pages/components/thematic-database/#partner-datasets","title":"Partner Datasets","text":"<p>Thematic data are produced by partners during the course of their work. Any datasets used to support an Intellectual Work<sup>2</sup>, calculate Performance Measures, or not otherwise defined are considered thematic data. Not all thematic data produced by partners will have secondary analysis value. The Data Steward will determine which datasets to include in the Thematic Database and include them once cleared.</p> <p>See Usage: Update the Thematic Database for detailed instructions on adding data to the Thematic Database.</p>"},{"location":"pages/components/thematic-database/#internal-external-datasets","title":"Internal &amp; External Datasets","text":"<p>The Thematic Database includes both internal and external datasets. </p>"},{"location":"pages/components/thematic-database/#external-datasets","title":"External Datasets","text":"<p>External datasets are those that can be shared freely. These are served publicly through the Data Catalog and Map Viewer in the Data Hub. </p>"},{"location":"pages/components/thematic-database/#internal-datasets","title":"Internal Datasets","text":"<p>Internal datasets are those that should not be shared publicly and/or should be shared on a \"need to know\" basis. These might include datasets that are governed by a data sharing agreement, data that include sensitive information such as identifying information, or those otherwise deemed to require protection. Data that fall under one of the principled exceptions should be considered internal.</p>"},{"location":"pages/components/thematic-database/#organization","title":"Organization","text":"<p>The Thematic Database is stored in a Google Drive Folder and organized in this way:</p> <pre><code>Thematic Data/\n+-- Internal/\n    +-- raw/\n    +-- SENSITIVE/\n    +-- Spatial/\n    +-- Tabular/\n    +-- internal - Data Catalog.gsheet\n    +-- README.gdoc\n+-- Shared Externally/\n    +-- Reports/\n    +-- Spatial/\n    +-- Tabular/\n    +-- Data Catalog.gsheet\n</code></pre> <p>The <code>Internal</code> folder stores all internal data, either in the <code>Spatial</code> folder or <code>Tabular</code> folder. Raw data should are stored in the <code>raw</code> folder (always store the raw files for reference if needed; data often requires processing before it is formatted appropriately for the Thematic Database).</p> <p>The <code>internal - Data Catalog.gsheet</code> is an inventory of all data assets in the Thematic Database, both internal and external.</p> <p>Public datasets are stored in the <code>Shared Externally</code> folder, either in the <code>Spatial</code> folder or <code>Tabular</code> folder. Reports and other non-tabular information are stored in the <code>Reports</code> folder. </p> <p>The <code>Data Catalog.gsheet</code> imports only the external datasets from the <code>internal - Data Catalog.gsheet</code>. This file is used by the Data Catalog of the Data Hub.</p> <ol> <li> <p>Secondary analysis value indicates when a dataset might be used for purposes other than for which it was originally collected.\u00a0\u21a9</p> </li> <li> <p>Intellectual work includes all works that document the implementation, monitoring, evaluation, and results of international development assistance activities developed or acquired under the award, which may include program and communications materials, evaluations and assessments, information products, research and technical reports, progress and performance reports required under this award (excluding administrative financial information), and other reports, articles and papers prepared by the contractor under the award, whether published or not.\u00a0\u21a9</p> </li> </ol>"},{"location":"pages/design/","title":"Design","text":""},{"location":"pages/design/Architecture/","title":"Architecture","text":"<p> Download architecture diagram</p> <p>Atlas serves as the single-source-of-truth for the Mission. </p> <p>Atlas leverages Google Workplace tools like Google Sheets to create an integrated solution for data collection and management. Team members and Implementing Partners continue to use the tools they know while Atlas works in the background to bring everything together. All data continues to be stored on Google Drive.</p> <p>Custom-built web applications like the Activity Database and Data Catalog make data discoverable to those who need it, when they need it. Web applications are built on Google's Apps Script platform, a cloud-based JavaScript platform that integrates with and automates tasks across Google products. Custom-built applications can be developed and deployed easily with all of your data in one place. Applications may also be built with Tableau, Esri products or any other platform (subject to license availability).</p> <p>The Mission Database, built on the powerful and open source PostgreSQL, aggregates all data together for rapid query and analysis. Data are piped into the Mission Database using data pipelines managed and maintained by the Data Steward. Basic geospatial analysis is supported and, optionally, Esri ArcGIS or the free and open source QGIS can be used for more advanced geospatial visualization and analysis.</p>"},{"location":"pages/design/Architecture/Activity%20Data/","title":"Activity Data","text":"<p> Download activity-data diagram</p> <p>Activity Data describe key characteristics of activities such as the activity name, start date, implementing partner, and total estimated ceiling. Activity Data provides a useful illustration of the general structure of Atlas. </p> <p>Data are stored in a Google Sheet referred to as the Activity DB Tracker, a flat-file containing all activity data. Data are input and edited by A/CORs and other stakeholders in the Activity DB Form, a custom-built Google Apps Script application. Data are piped into the Mission Database through a data pipeline managed by the Data Steward for aggregation with other data assets. Read more about data pipelines here.</p> <p>A unique identifier is provided for each activity in the Activity DB Tracker to help maintain referential integrity across other data assets. Activities are always referred to by these unique identifiers across all data assets as required by the Data Standards. Other products (not pictured) reference data in the Activity DB Tracker, especially the unique identifiers for each Activity.</p> <p>Note</p> <p>We humans are pretty good are figuring out what someone is referring to when we use a nickname or other shorthand. Computers prefer references that are more exact. Referential integrity requires always referring to an entity by the same unique identifier. That helps the system recognize an entity when it's referenced by multiple data sources.</p>"},{"location":"pages/design/Architecture/Activity%20Location%20Data/","title":"Activity Location Data","text":"<p> Download ald-component diagram</p> <p>Activity Location Data include both the location of implementation activities and the location of intended beneficiaries. Implementation activities should be reported at the finest level of granularity possible, usually the exact site location (i.e., latitude and longitude). The location of intended beneficiaries is typically reported at the municipality level. Because these data may contain sensitive information, care should be taken when storing, processing, and sharing.</p> <p>The Development Information Solution (DIS) is the Agency's preferred platform for reporting Activity Location Data. The Data Steward must add all Activities to the DIS to facilitate later reporting. The DIS provides a template for reporting Activity Location Data, the Activity Location Template (not shown). This template is the basis for the Activity Location Tracker, a component of the template Activity Folder.   </p> <p>During Activity Start Up, the Data Steward will work with the A/COR and Implementing Partner to determine the appropriate frequency and level of geographic detail for reporting Activity Location Data, which will be documented in the Activity MECLA Plan &amp; DMP (not shown). The Data Steward will set up the Activity Folder and copy the team's Activity Location Tracker into the folder. </p> <p>At the agreed frequency, the Implementing Partner will submit Activity Location Data using the tracker. </p> <p>Activity Location Data are automatically imported from each Activity Location Tracker into a central Activity Location Compiler. The Data Steward will ingest the data in Activity Location Compiler into the Mission Database through a semi-automated pipeline. The Mission Database generalizes the Activity Location Data and removes sensitive information so that it can be shared through the ALD Portal - Public, a component of the Data Hub.</p> <p>When required, the Implementing Partner will also submit the Activity Location Data to the DIS using the Activity Location Template with the DIS batch upload process. </p>"},{"location":"pages/design/Architecture/Performance%20Measures/","title":"Performance Measures","text":"<p> Download pm-component diagram</p> <p>The Mission Data Management System proposes a new approach to Performance Measure (PM) reporting. </p> <p>Currently, A/CORs update the PPR Tracker for each Activity based on Annual Reports submitted by Implementing Partners at the end of the fiscal year. Some teams, but not all, utilize additional activity-specific performance measure trackers to collect data more frequently or collect additional disaggregates for specific performance measures. The MECLA Team submits the data to FactsInfo after drafting summary narratives and receiving necessary clearances. </p> <p>In the proposed process, the Data Steward would work with the A/COR and Implementing Partner during Activity Start Up to set up a shared Activity Folder on Google Drive. The Implementing Partner will use the guidance in the Activity MECLA Plan &amp; Data Management Plan (DMP) to submit datasets in the appropriate format and the required frequency in the Thematic Datasets subfolder. Data from the Activity Folder can then be easily read into the Mission Database. (Recall that datasets which inform PMs are termed \"Thematic Data\" in the Agency, see more here.)</p> <p>However, the current process will need to be maintained at least until all Activities have converted to the new process. The TrackerGEN is a Google Apps Script tool that can be used by the Data Steward to automatically generate an Activity PM Tracker for any Activity. Using TrackerGEN will ensure the PM data can be easily ingested by the Mission Database.</p> <p>Importantly, the Mission Database must be set up to properly store PM results and aggregate PM results in a meaningful way. The DIS will serve as the point-of-reference for PM metadata such as disaggregates and aggregation schemes. For any PM used in the Mission, the Data Steward will set up the PM in DIS and then download the PM Template to extract the necessary information using the PM Template Reader (not shown).</p> <p>Finally, the Data Steward will query the Mission Database to summarize performance data. Until the Mission adopts the DIS for reporting, the PPR Tracker will be updated by the PM data of the Mission Database for reporting to FactsInfo. Any reports, presentations, and analysis generated in review of the PM data, for example during the Performance Plan &amp; Report (PPR), will be uploaded to the MECLA Library and included in the Mission Knowledge Management system.</p>"},{"location":"pages/design/Architecture/Thematic%20Data/","title":"Thematic Data","text":"<p> Download theme-component diagram</p> <p>Thematic data describes data that informs the calculation of performance measures, data collected in support of an intellectual work, and most other datasets collected by Implementing Partners. Thematic data does not include data incidental to award management, such as accounting information.</p> <p>During Activity Start Up, the Data Steward will work with the A/COR and Implementing Partner to inventory all anticipated datasets to be collected, which will be documented in the Data Inventory, a Google Sheet stored in the Activity Folder. The Data Steward will use this opportunity to ensure the data will be collected and shared in a way that will be easily ingested into the Mission Database, share any existing datasets that might be useful, and confirm that proper consideration is given to any risk from collecting the data. This guidance will be documented in the Activity MECLA Plan &amp; DMP.</p> <p>During the course of the award or agreement, the Implementing Partner will upload thematic datasets and intellectual works to the Thematic Datasets and Intellectual Works folders within their Activity Folder. In addition, they will update the Data Inventory with links and short descriptions of each dataset. Sharing data and intellectual works via a well-structured Activity Folder, rather than email, makes it easier to maintain the information over time.</p> <p>Once reviewed and cleared by the A/COR, the Implementing Partner will  upload intellectual works and datasets to the Development Experience Clearinghouse (DEC) and Development Data Library (DDL), respectively.</p> <p>The Data Steward will upload relevant datasets into the Thematic Database on a regular basis for use in Mission-wide analyses. At the same time, the Data Steward will upload new thematic datasets from the Thematic Database into the Mission Database using a semi-automated data pipeline. Preferably, datasets will be uploaded from the DDL with back-references for metadata and other data documentation. However, if a dataset is not uploaded to the DDL or as the Mission is transitioning to using the DDL more reliably, datasets can be ingested directly from the Activity's Thematic Datasets folder. Public Data may also be included when a relevant public data source is identified. </p> <p>The Thematic Database serves as the data storage back-end for the Data Catalog, a searchable front-end for the Thematic Database and the Map Viewer, an application for quickly visualizing the thematic data on a map. Both the Data Catalog and Map Viewer are made available publicly through the Data Hub.</p>"},{"location":"pages/design/Pipelines/","title":"Pipelines","text":"<p>Data pipelines transfer data between components. Data pipelines create a web across all components so that data can be entered once and used across multiple components.</p> <p>The Data Steward is responsible for running data pipelines on a regular basis and developing new pipelines as needed. </p>"},{"location":"pages/design/Pipelines/#pipelines-between-google-sheets","title":"Pipelines between Google Sheets","text":"<p>Google Sheets supports data transfer between Sheets using the <code>IMPORTRANGE</code> function. This is the simplest option for transferring data between Sheets. <code>IMPORTRANGE</code> is used to move data between Sheets in the Thematic Database, to stage data for Web Applications, and to pull data from the Activity Database into trackers. In some cases, the <code>IMPORTRANGE</code> function is combined with the <code>QUERY</code> function to import a subset of data.</p> <p>In some cases, it is better to use a Google Apps Script project to aggregate data instead of writing one long <code>IMPORTRANGE</code> function. Google Apps Script is used to aggregate data from many activity-level templates, such as the Activity Location Data Template.</p> <p>Warning</p> <p>The <code>IMPORTRANGE</code> function can lead to leaks of sensitive data if not used with caution. When a public user has edit access to a spreadsheet that imports data from another spreadsheet, they can change the <code>IMPORTRANGE</code> function to access any other data from that spreadsheet. It is best practice to pass data through an intermediate spreadsheet when sharing data from an internal source to a public source, referred to here as staging.</p>"},{"location":"pages/design/Pipelines/#pipelines-to-the-mission-database","title":"Pipelines to the Mission Database","text":"<p>All data stored in Atlas is piped into the Mission Database for aggregation and querying. Data pipelines to the Mission Database are managed through the Data Steward Admin Tool. </p> <p>Atlas employs two common data pipeline strategies to load data into the Mission Database: truncate and load and upsert.</p>"},{"location":"pages/design/Pipelines/#truncate-load","title":"Truncate &amp; Load","text":"<p>Truncate and load simply means deleting all existing records and loading the data fresh from the data source.</p>"},{"location":"pages/design/Pipelines/#upsert","title":"Upsert","text":"<p>Upsert (a portmanteau of update and insert) will update existing records and insert new records. Upsert is used by Atlas when necessary to maintain referential integrity with foreign key constraints. For example, activity data from the Activity Database is upserted into the Mission Database so that other tables that reference the Activity ID field are not affected. When using upsert, it's essential that the record identifier in the data source is never changed.</p>"},{"location":"pages/design/Pipelines/#pipelines-to-web-applications","title":"Pipelines to Web Applications","text":"<p>Most Atlas web apps are built in Google Apps Script, which enables them to easily read data from their respective data sources in Google Drive. In some cases, data are served to web apps in intermediate layers using the strategies discussed in Pipelines between Google Sheets.</p> <p>To understand the rationale for data pipelines, see here. Continue reading for detailed descriptions of each data pipeline.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/","title":"Activity Data","text":"<p> Download activity-data-pipeline diagram</p> <p>Activity Data is a core component of the Mission data model and must be shared across many sources, including the Mission Database, Web Applications, and any team-owned trackers.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/#a","title":"A","text":"<p>The record owner inputs new activities and updates existing activities through the Activity DB Form. Updates are made as needed but no less than quarterly. The record owner is often the Agreement Officers Representative or Contracting Officers Representative (A/COR); however, each Technical Office may designate an alternative for this role. The designated point of contact is stored in the \"Record Owner\" field.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/#b","title":"B","text":"<p>Domain experts are individuals with knowledge of or areas of responsibility towards sections of the Activity DB. For example, a Digital Development Advisor is the domain expert for questions related to digital components for an activity. The domain expert will review and if necessary update records for the fields in their domain at least quarterly.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/#c","title":"C","text":"<p>The Activity Form upserts directly to the Activity Tracker; the Activity Form is a containerized Google Apps Script within the Activity DB Tracker.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/#d","title":"D","text":"<p>The Activity DB Tracker is automatically backed up to another Google Sheet by a scheduled Google Apps Script task.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/#e","title":"E","text":"<p>Activity data are staged in a Google Sheet in the Thematic Database, <code>activity data.ghseet</code>, for downstream applications including the Mission Database and Web Applications. </p> <p>Data are staged in multiple Activity DB Tracker sheets to limit disruptions arising from changes in the Activity DB Tracker and to protect the data in the Activity DB Tracker from accidental leaks, as some data are procurement-sensitive. These best practices are described here.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/#f","title":"F","text":"<p>Using the Data Steward Admin tool, the Data Steward runs a semi-automated data pipeline to upsert data from <code>activity data.ghseet</code> to the Mission Database on a regular basis.</p> <p>This pipeline creates the <code>create_activity_data.sql</code> and <code>load_activity_data.sql</code> files stored in the Data Admin SQL folder. The <code>load_activity_data.sql</code> file contains the SQL commands to upsert data into the Mission Database. The Data Steward uses psql to load data in pgAdmin4.</p> <p>(The <code>create_activity_data.sql</code> file can be used for data definition (e.g., developing the entity-relationship diagram) but is not required.)</p> <p>See Usage for detailed instructions on running semi-automated data pipelines.</p>"},{"location":"pages/design/Pipelines/Activity%20Data/#g","title":"G","text":"<p>Web applications read data from from <code>activity data.ghseet</code> as needed. </p> <p>Tip</p> <p>Many teams use activity data. It is common for teams to duplicate these data across multiple trackers, which can lead to data becoming out of sync and requires significant \"human automation\" to keep up to date. However, teams should be empowered to continue to build custom tools that meet their needs. The Data Steward should work with these teams to integrate the Activity DB into their trackers using the guidance provided here.</p>"},{"location":"pages/design/Pipelines/Activity%20Location%20Data/","title":"Activity Location Data","text":"<p> Download activity-location-data-pipeline diagram</p> <p>Activity Location Data are provided by Implementing Partners on a quarterly basis, stored in the Mission Database for aggregation and query, and served publicly through the Activity Location Data Portal once generalized to protect sensitive information.</p>"},{"location":"pages/design/Pipelines/Activity%20Location%20Data/#a","title":"A","text":"<p>The Implementing Partner inputs activity location data into their Activity Location Tracker no less than quarterly according to the guidance provided in their Data Management Plan. The Activity Location Tracker is stored in the Activity Folder, which is accessible to the Data Steward.</p>"},{"location":"pages/design/Pipelines/Activity%20Location%20Data/#b","title":"B","text":"<p>The Data Steward logs each new Activity Folder in the Atlas Data Inventory after setting up the folder. The Inventory is used by a Google Apps Script in the Data Steward Admin Tool to identify all Activity Location Trackers and aggregate the data into the Activity Location Data Compiler. This script is on a time-based trigger set to run each day.</p> <p>The script also generalizes the exact site locations (i.e., latitude and longitude) to hexagonal bins. This protects implementing partners and beneficiaries. See more here.</p>"},{"location":"pages/design/Pipelines/Activity%20Location%20Data/#c","title":"C","text":"<p>Using the Data Steward Admin Tool, the Data Steward runs a semi-automated data pipeline to truncate and load data from the Activity Location Compiler to the Mission Database on a regular basis.</p> <p>The Data Steward Admin Tool runs a script on a time-based trigger each day that creates the <code>create_activity_location_data.sql</code> and <code>load_activity_location_data.sql</code> files stored in the <code>Admin/SQL/</code> directory in Atlas. The <code>load_activity_location_data.sql</code> file contains the SQL commands to truncate and load data into the Mission Database. The Data Steward uses psql to load data in pgAdmin4.</p> <p>See Usage for detailed instructions on running semi-automated data pipelines.</p>"},{"location":"pages/design/Pipelines/Activity%20Location%20Data/#d","title":"D","text":"<p>Generalized Activity Location Data are imported by the Activity Location Data (Public) Google Sheet using IMPORTRANGE. Data are staged in an intermediate Google Sheet, <code>ald_stage.gsheet</code>, before import to protect sensitive data from accidental leaks.</p>"},{"location":"pages/design/Pipelines/Activity%20Location%20Data/#e","title":"E","text":"<p>The Activity Location Data Portal - Public reads data directly from the Activity Location Data (Public) Google Sheet.</p>"},{"location":"pages/design/Pipelines/Thematic%20Data/","title":"Thematic Data","text":"<p> Download theme-pipelines diagram</p> <p>Thematic data are collected by the Data Steward and uploaded to the Thematic Database for use by Web Applications and the Mission Database.</p>"},{"location":"pages/design/Pipelines/Thematic%20Data/#a","title":"A","text":"<p>The Data Steward identifies useful public datasets and thematic data collected by Implementing Partners and manually uploads data to the Thematic Database.</p> <p>See How to Update the Thematic Database for detailed instructions.</p>"},{"location":"pages/design/Pipelines/Thematic%20Data/#b","title":"B","text":"<p>Using the Data Steward Admin Tool, the Data Steward runs a semi-automated data pipeline to truncate and load Data Assets from the Thematic Database to the Mission Database when new data are added.</p> <p>This pipeline creates the <code>create_thematic_data.sql</code> and <code>load_thematic_data.sql</code> files stored in the Data Admin SQL folder. The <code>load_thematic_data.sql</code> file contains the SQL commands to truncate and load data into the Mission Database. The Data Steward uses psql to load data in pgAdmin4.</p> <p>See Usage for detailed instructions on running semi-automated data pipelines. </p>"},{"location":"pages/design/Pipelines/Thematic%20Data/#c","title":"C","text":"<p>Web applications read data directly from the Thematic Database. For example, the Data Catalog provides a searchable user interface for the Data Inventory. The Map Viewer maps Data Assets when the data include geographical data.</p>"},{"location":"pages/get-started/overview/","title":"Setup Overview","text":"<p>Initial set up of Atlas is quick and easy. You will realize many benefits from Atlas after compiling and cataloging the data you already have in the Mission. Rolling out all components of Atlas to the Mission and partners is a long term project which will develop the capacity and culture required to empower teams with a data-informed, geographic approach to development.</p> <p>The following pages walk you through the initial deployment of Atlas. Follow the instructions in the order presented (see the Next button on the page footer). The section Optional Setup describes optional components and options for advanced users not required for basic use of Atlas.</p>"},{"location":"pages/get-started/requirements/","title":"Requirements","text":"<p>Atlas requires only Google Workplace and the right personnel. Optional components of Atlas have specific hardware and software requirements, described in detail below.</p>"},{"location":"pages/get-started/requirements/#personnel","title":"Personnel","text":"<p>To get the most out of Atlas, you should have available the following capacities. These capacities may be found in a single person or, more likely, a team with complementary skills:</p> <ul> <li>Data Steward: the Data Steward manages and maintains the Atlas system, works with Mission staff and partners to use Atlas tools and templates appropriately, and manages data reporting tasks. This role is sometimes also supported by an Information Coordinator. Knowledge of database management systems and SQL are important. The Data Steward will also work with technical teams and partners to train them on best practices and integrate their data with Atlas, so interpersonal skills are also important.</li> <li>GIS Specialist: the GIS Specialist provides the technical expertise necessary to manage geospatial data, conducts spatial analyses to support technical teams, and consults with partners on best practices for geospatial data collection and use.</li> <li>Apps Script Developer: the Developer uses their specialized expertise to maintain and expand the functionality of the Atlas tools built on Google Apps Script. </li> </ul>"},{"location":"pages/get-started/requirements/#optional-requirements","title":"Optional Requirements","text":""},{"location":"pages/get-started/requirements/#hardware","title":"Hardware","text":"<p>Optional components of Atlas, namely the Mission Database, must be installed on a laptop or desktop that meets these requirements:</p> <ul> <li>1 GHz processor</li> <li>2 GB of RAM</li> <li>512 MB of HDD</li> </ul> <p>Additional disk space is required for data or supporting components. The Mission Database may be migrated to a remote or cloud-based server if necessary.</p>"},{"location":"pages/get-started/requirements/#software","title":"Software","text":"<p>Advanced users may install PostgreSQL, Miniconda and Git (including GitBash) for additional functionality. Create a GitHub account to customize and share your own version of this documentation. </p> <p>Warning</p> <p>Coordinate with your systems administrator before installing any software.</p>"},{"location":"pages/get-started/setup/authorize-apps-script/","title":"Authorize Google Apps Script","text":"<p>When prompted, click Review permissions.</p> <p></p> <p>Choose your Google Workspace account.</p> <p>Select Allow.</p> <p></p> <p>For some users, Google will warn that \"Google hasn't verified this app\". Click Advanced and select Go to Data Steward Admin Tool (unsafe).</p> <p></p> <p>Then continue with the above step.</p>"},{"location":"pages/get-started/setup/setup-admin/","title":"Copy Data Steward Admin Tool","text":"<p>Tip</p> <p>Atlas is currently growing and many new features are available. To ensure you get the latest version, contact erianderson@usaid.gov before setting up Atlas.</p> <p>Before you begin, make sure you are logged into your work-issued Google Workspace account where the Atlas project will be stored.</p> <ol> <li> <p>Follow this link to access the Data Steward Admin Tool.</p> </li> <li> <p>In the Overview window (see the left sidebar), click Make a Copy.     </p> </li> <li> <p>The script project will be copied to your repository on script.google.com as <code>Copy of Data Steward Admin</code>. </p> </li> <li> <p>Rename the project to <code>Data Steward Admin</code>.</p> </li> <li> <p>Close the original Data Steward Admin project.</p> </li> <li> <p>Open the Triggers window (see left sidebar).</p> </li> <li> <p>Click Add Trigger to add a new trigger. Select the function <code>update_compiler</code> to run the <code>Head</code> deployment as a <code>Time-driven</code>, <code>Day timer</code> run overnight. Select <code>Notify me daily</code> in the event of a failure. Click Save.</p> </li> </ol> <p></p>"},{"location":"pages/get-started/setup/setup-code/","title":"Set Up Code Files","text":"<p>The Data Steward Admin Tool contains code files that pipe data from multiple spreadsheets and save files to specific folders.</p> <p>In your version of the Data Steward Admin Tool, update variables referencing sheets or folders in Atlas to the correct spreadsheet IDs from your copy of the Atlas directory. Replace the <code>{path}</code> section of the code below for each file with the correct spreadsheet or folder ID indicated by the path described in the curly braces.</p>"},{"location":"pages/get-started/setup/setup-code/#utilsgs","title":"utils.gs","text":"<pre><code>// update when setting up Atlas\nconst admin_sql_folder = '{Atlas &gt; Admin &gt; SQL/}';\n</code></pre>"},{"location":"pages/get-started/setup/setup-code/#activity-datags","title":"activity-data.gs","text":"<pre><code>// update when setting up Atlas\nconst activity_data_id = '{Atlas &gt; Modules &gt; Thematic Data &gt; Internal &gt; Tabular &gt; activity data}';\n</code></pre>"},{"location":"pages/get-started/setup/setup-code/#thematic-datags","title":"thematic-data.gs","text":"<pre><code>// update when setting up Atlas\nconst thematic_data_id = '{Atlas &gt; Modules &gt; Thematic Data &gt; Shared Externally &gt; Data Catalog}';\n</code></pre>"},{"location":"pages/get-started/setup/setup-code/#activity-location-datags","title":"activity-location-data.gs","text":"<pre><code>// update when setting up Atlas\nconst activity_location_data_id = '{Atlas &gt; Modules &gt; Actiivty Location Data / Activity Location Data Compiler}';\n</code></pre>"},{"location":"pages/get-started/setup/setup-code/#compile-aldgs","title":"compile-ald.gs","text":"<pre><code>// update when setting up Atlas\nconst inventory_id = '{Atlas &gt; Atlas Data Inventory}';\nconst compile_id = '{Atlas &gt; Modules &gt; Actiivty Location Data / Activity Location Data Compiler}';\nconst tmp_folder = '{Atlas &gt; Admin &gt; tmp/}';\n</code></pre>"},{"location":"pages/get-started/setup/setup-drive/","title":"Copy Atlas Directory","text":"<p>Next you will create a copy of the Atlas directory on your Google Drive. This will copy all folders, files and templates needed to get started.</p> <ol> <li> <p>Create a new folder on your work-issued Google Drive or Shared Drive called <code>Atlas</code>.</p> </li> <li> <p>Copy the folder ID to your clipboard. The folder ID is shown in the URL after <code>folders/</code> and before any question marks (<code>?</code>), like below.</p> <p>https://drive.google.com/drive/folders/{folderID}?lfhs=2</p> </li> <li> <p>Open your copy of the Data Steward Admin Tool (located at script.google.com).</p> </li> <li> <p>In the Editor window, locate the file <code>copy-folder.gs</code> and select it.</p> </li> <li> <p>Update the variable <code>destination</code> with the folder id you copied to the clipboard.</p> <p>let destination = \"{folderID}\";</p> </li> <li> <p>Save the script project (<code>Ctrl + S</code>)</p> </li> <li> <p>In the script editor menu bar, select the <code>copyAtlas</code> function.    </p> </li> <li> <p>Click Run.</p> </li> <li> <p>You will need to authorize the script to access Google Drive. For step-by-step instructions, see here.</p> </li> </ol>"},{"location":"pages/get-started/setup/setup-imports/","title":"Set Up Imports","text":"<p>Atlas uses a number of <code>IMPORTRANGE</code> functions to pipe data between spreadsheets. In this step, you will update the spreadsheet IDs in each <code>IMPORTRANGE</code> function to the IDs of your copy of each file.</p> <p>First, we'll update a single IMPORTRANGE function to demonstrate the workflow. Next, you'll repeat the process for all <code>IMPORTRANGE</code> functions shown in the table below.</p> <ol> <li> <p>Navigate to the Google Sheet <code>adb_stage</code> (Atlas &gt; Modules &gt; Activity Database). This is the source sheet.</p> </li> <li> <p>Copy the spreadsheet ID of the source sheet. The folder ID is shown in the URL after <code>folders/</code> and before any question marks (<code>?</code>), like below.</p> <p>https://drive.google.com/drive/folders/{folderID}?lfhs=2</p> </li> <li> <p>Navigate to the Google Sheet <code>activity_data</code> (Atlas &gt; Modules &gt; Thematic Data &gt; Internal &gt; Tabular). This is the destination sheet.</p> </li> <li> <p>Open the spreadsheet <code>activities_import</code>.</p> </li> <li> <p>In cell <code>A1</code>, replace the current spreadsheet ID with the ID copied in step 2. The formula will take the general form:</p> </li> </ol> <pre><code>=IMPORTRANGE(\"{folderID}\", \"data!A:BI\")\n</code></pre> <ol> <li> <p>Hit Enter to commit the change.</p> </li> <li> <p>A <code>#REF</code> error will show in cell A1. Select the cell to display a popup message requesting you to Allow Access to the source sheet. </p> </li> </ol> <p></p> <ol> <li>Click Allow Access.</li> </ol> <p>Repeat this process for all <code>IMPORTRANGE</code> functions in Atlas. The table below shows the source sheet, destination sheet, and cell reference for all <code>IMPORTRANGE</code> functions.</p> Source Destination Cell Reference adb_stage: dataAtlas &gt; Modules &gt; Activity Database activity_data: _activities_importAtlas &gt; Modules &gt; Thematic Data &gt; Internal &gt; Tabular A1 Activity Database: dataAtlas &gt; Modules &gt; Activity Database adb_stage: dataAtlas &gt; Modules &gt; Activity Database A1 Atlas Data Inventory: _importAtlas (root) activity_data: _activities_importAtlas &gt; Modules &gt; Thematic Data &gt; Internal &gt; Tabular A1 Data Catalog: providersAtlas &gt; Modules &gt; Thematic Data &gt; Internal Data Catalog: providersAtlas &gt; Modules &gt; Thematic Data &gt; Shared Externally A1 Data Catalog: directoryAtlas &gt; Modules &gt; Thematic Data &gt; Internal Data Catalog: directoryAtlas &gt; Modules &gt; Thematic Data &gt; Shared Externally A1 Activity Location Data Complier: _exportAtlas &gt; Modules &gt; Activity Location Data adb_stage: stageAtlas &gt; Modules &gt; Activity Location Data A1 adb_stage: stageAtlas &gt; Modules &gt; Activity Location Data Activity Location Data (Public): dataAtlas &gt; Modules &gt; Activity Location Data A1"},{"location":"pages/get-started/setup/setup-next-steps/","title":"Next Steps","text":"<p>Basic setup of Atlas is now complete. </p> <p>Advanced users may choose to continue with the optional set up steps in the next section.</p> <p>Review the workflows described in the Usage section to begin using Atlas.</p>"},{"location":"pages/get-started/setup/setup-webapps/","title":"Set up Web Applications","text":"<p>Web applications make data discoverable by stakeholders. Many of the applications are available in the USAID/Guatemala Data Hub. The primary web applications available currently include</p> <ul> <li>Data Catalog: Search this catalog to find data to support evidence-based decision-making.</li> <li>Map Viewer: See spatial distribution of key context indicators.</li> <li>Activity Location Data Portal (Public): Share Implementing Partner Activity Location Data generalized for public consumption.</li> </ul> <p>All web applications used by Atlas are available from the gtm-apps GitHub repository. Select the best options for your Mission and follow the instructions there to install. </p>"},{"location":"pages/get-started/setup-optional/install-postgres/","title":"Install PostgreSQL","text":"<p>Follow the instructions below to install PostgreSQL. Before installing any software, request permission from your system administrator if required.</p> <ol> <li> <p>Download the installer for your operating system (for Windows, as of this writing, the official installer was hosted on enterprisedb.com). </p> </li> <li> <p>When prompted during installation, include pgAdmin 4, Stack Builder, and Command Line Tools. The default installation options are acceptable for all other features. You'll be prompted to provide a password for the superuser. Write this down and don't lose it!</p> </li> <li> <p>After installing Postgres, you will be prompted to launch Stack Builder to download additional extensions. Install PostGIS, a spatial extension for geographic data.</p> </li> </ol> <p>View the installation screens here.</p>"},{"location":"pages/get-started/setup-optional/postgres-install-screens/","title":"PostgreSQL Installation Screens","text":""},{"location":"pages/get-started/setup-optional/setup-missiondb/","title":"Set up Mission Database","text":"<p>The Mission Database is a PostgreSQL relational database that brings all your data together. To set it up, follow the instructions below.</p>"},{"location":"pages/get-started/setup-optional/setup-missiondb/#create-the-database","title":"Create the database","text":"<p>Open PgAdmin and input the password specified when installing PostgreSQL.</p> <p>In the top menu, select Object &gt; Create &gt; Database. Input the name for your Mission Database, we suggest <code>missiondb</code>. The default settings should work for most users. Click Save.</p> <p></p>"},{"location":"pages/get-started/setup-optional/setup-missiondb/#add-extensions","title":"Add extensions","text":"<p>Extensions provide additional functionality for PostgreSQL databases. </p> <p>With the <code>missiondb</code> selected in the Browser tree, open the Query Tool (Tools &gt; Query Tool or <code>Alt+Shift+Q</code>). Paste the following SQL code into the Query window.</p> <pre><code>CREATE EXTENSION postgis;\nCREATE EXTENSION postgis_raster;\n</code></pre> <p>Execute the command (<code>F5</code>). You should see a message print out in the Messages window that the query returned successfully.</p>"},{"location":"pages/knowledge/base/","title":"Knowledge Base","text":"<p>The Knowledge Base is a searchable knowledge management system for Atlas. Use the search bar in the top right to find what you need. Advanced users may add to the knowledge base as they build out Atlas. The Knowledge Base is intended primarily for the Data Steward. To share information with other stakeholders, consider repackaging information for your intended audience. </p>"},{"location":"pages/knowledge/base/GeoJSON/","title":"GeoJSON","text":"<p>GeoJSON and TopoJSON are useful file formats for storing geospatial data. Because they are JSON serializable, they can be used in web applications, and are the preferred formats for the Map Viewer.</p> <p>If you then need to convert another format to TopoJSON, use mapshaper.org. You can also simplify the polygons, reducing the file size, to facilitate quicker loading and/or analysis.</p>"},{"location":"pages/knowledge/base/SQL/","title":"SQL","text":"<p>Structured Query Language (SQL) is the language used by relational databases to define and manipulate data. Each relational database management system has it's own unique flavor of SQL. This article discusses the PostgreSQL flavor of SQL.</p>"},{"location":"pages/knowledge/base/SQL/#common-sql-commands","title":"Common SQL commands","text":""},{"location":"pages/knowledge/base/SQL/#create-a-table","title":"Create a table","text":"<p>Use the CREATE TABLE command to create a table.</p> <pre><code>CREATE TABLE persons (\n    id BIGSERIAL PRIMARY KEY NOT NULL,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50)\n);\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#create-a-view","title":"Create a view","text":"<p>Views are calculated tables. Create a view by simply adding the following line to the top of your SQL query.</p> <pre><code>CREATE VIEW IF NOT EXISTS view_name AS\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#update-a-record","title":"Update a record","text":"<p>Use the primary key to access the record. You can update multiple fields for any record by comma separating each <code>field = value</code> pair. You can also update multiple records at the same time, depending on the <code>WHERE</code> query. Always use the <code>WHERE</code> clause!</p> <pre><code>UPDATE &lt;tablename&gt; SET &lt;field&gt; = &lt;value&gt;, &lt;field_2&gt; = &lt;value_2&gt; WHERE id = &lt;id&gt;;\n</code></pre> <p>Tip</p> <p>Be sure to not edit records that will be overwritten on the next import. Instead, edit the data source, drop the existing table, and read in the new one. Because Atlas is primarily a data aggregator, not a data source, updates to most records should be done in the data source on Google Drive.</p>"},{"location":"pages/knowledge/base/SQL/#delete-a-record","title":"Delete a record","text":"<p>As with updating a record, always use the <code>WHERE</code> clause when deleting records. </p> <pre><code>DELETE FROM &lt;tablename&gt; WHERE id = &lt;id&gt;;\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#drop-tables-or-views","title":"Drop tables or views","text":"<p>Sometimes you'll want to drop many tables or all tables in a database and start fresh with the same schema. Use\u00a0<code>CASCADE</code>\u00a0to remove foreign keys from related tables.</p> <pre><code>-- DROP TABLE \nDROP TABLE IF EXISTS table_name CASCADE;\n\n-- DROP VIEW\nDROP VIEW IF EXISTS view_name;\n</code></pre> <p>Warning</p> <p>Using the <code>CASCADE</code> command to overcome the foreign key constraint when deleting records is not best practice. <code>CASCADE</code> will delete any record in your database that includes that foreign key. Instead, you should manually delete or update related records to have full control over what is deleted.</p>"},{"location":"pages/knowledge/base/SQL/#primary-keys","title":"Primary keys","text":"<p>Primary keys are unique identifiers for each record. Include the key word <code>PRIMARY KEY NOT NULL</code> when defining the column to specify it as a primary key. </p>"},{"location":"pages/knowledge/base/SQL/#sequence-as-primary-key","title":"Sequence as primary key","text":"<p>Use the data type BIGSERIAL or SERIAL when specifying integer type primary keys so that they will auto increment.</p>"},{"location":"pages/knowledge/base/SQL/#compound-primary-key","title":"Compound primary key","text":"<p>You can also set a compound primary key (e.g., two or more fields together). Be careful to specify fields that will never contain duplicates!</p> <pre><code>-- SPECIFY IN CREATE TABLE\nCREATE TABLE persons (\n    first_name VARCHAR(50) NOT NULL,\n    last_name VARCHAR(50) NOT NULL,\n    PRIMARY KEY (first_name, last_name)\n);\n\n-- OR USE ALTER TABLE\nALTER TABLE persons PRIMARY KEY (first_name, last_name);\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#uuid-primary-key","title":"UUID primary key","text":"<p>A universally unique identifier (UUID) is a unique identifier that can be used as a primary key. The benefit of a UUID, as opposed to a sequence, is that it is unique across all tables (and even other databases). It also prevents hackers from mining a database by guessing primary keys. </p> <p>To use in Postgres, you must first install the extension.</p> <pre><code>CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"\n</code></pre> <p>Use <code>\\df</code> to list the UUID generation algorithms available. You'll need to select one of these to generate a UUID when inserting a record. I recommend <code>uuid_generat_v4()</code>.</p> <p>To create a primary key using UUID simply list <code>UUID</code> as the data type when creating the table. Foreign key references to a <code>UUID</code> should also use the data type <code>UUID</code>. When inserting a record, call the UUID function to create a unique identifier. You do not need to check for duplicates since these are guaranteed to be universally unique.</p> <pre><code>CREATE TABLE cars (\n    car_uid UUID NOT NULL PRIMARY KEY \n);\n\n-- foreign key references should also be UUID\nCREATE TABLE persons (\n    person_uid UUID\n    car_uid UUID references car(car_uid)\n);\n\n-- use a function to generate a uuid for each record\nINSERT INTO persons (person_uid)\nVALUES (uuid_generate_v4());\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#transactions","title":"Transactions","text":"<p>It is best practice to wrap commands in a transaction. Transactions will preserve database integrity by rolling back all commands executed if one fails. To create a transaction, simply begin and end the series of commands with <code>BEGIN</code> and <code>COMMIT</code>.</p> <pre><code>BEGIN;\n-- SQL Commands\nCOMMIT;\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#constraints","title":"Constraints","text":"<p>Constraints prevent database operations that detoriate the integrity of the data.</p>"},{"location":"pages/knowledge/base/SQL/#unique-constraint","title":"Unique constraint","text":"<p>Unique contraints enforce uniqueness for any column, and are enforced by default with primary keys.</p> <pre><code>-- SPECIFY IN CREATE TABLE\nCREATE TABLE &lt;tablenamd&gt; (\n    col1 TEXT,\n    col2 TEXT,\n    CONSTRAINT &lt;constraint_name&gt; UNIQUE (col1)\n);\n\n-- OR USE ALTER TABLE\nALTER TABLE &lt;tablename&gt; ADD CONSTRAINT &lt;contraint_name&gt; UNIQUE (&lt;col&gt;);\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#check-constraint","title":"Check constraint","text":"<p>Use check for validation of database inputs. See also enumerators. </p> <pre><code>-- SPECIFY IN CREATE TABLE\nCREATE TABLE &lt;tablename&gt; (\n    gender TEXT CHECK (gender IN ('Female', 'Male'))\n);\n\n-- OR USE ALTER TABLE\nALTER TABLE &lt;tablename&gt; ADD CONSTRAINT &lt;contraint_name&gt; CHECK (gender IN ('Female', 'Male'));\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#on-conflict-do-nothing","title":"On conflict do nothing","text":"<p>Use the <code>ON CONFLICT</code> command combined with <code>DO NOTHING</code> to skip records that violate constraints. For example, inserting a record with an existing primary key violates the <code>NOT NULL</code> constraint and will fail. </p> <pre><code>INSERT INTO &lt;tablename&gt; (first_name, last_name)\nVALUES ('John', 'Doe')\nON CONFLICT (id) DO NOTHING;\n</code></pre> <p>This is useful when updating a table where some of the records in the data source are already in the table.</p>"},{"location":"pages/knowledge/base/SQL/#upsert","title":"Upsert","text":"<p>Use <code>ON CONFLICT</code> command combined with <code>DO UPDATE</code> to update a record when a constraint is violated. This is commonly referred to as an \"upsert\".</p> <pre><code>INSERT INTO &lt;tablename&gt; (first_name, last_name)\nVALUES ('John', 'Doe')\nON CONFLICT (id) DO UPDATE SET first_name = EXCLUDED.first_name, last_name = EXCLUDED.last_name;\n</code></pre> <p>This is useful when a user is editing a record in your application or when you have edited many records in the data source.</p> <p>Some caveats</p> <ul> <li>If using a partial index, you must constrain to only the indexed part of the table using a <code>WHERE</code> statement (see here). </li> </ul>"},{"location":"pages/knowledge/base/SQL/#enumerators","title":"Enumerators","text":"<p>Enumerated (enum) data types are lists of valid options for a field, for example the days of the week or responses on a Likert scale.</p> <p>To create an enumerator use the <code>CREATE TYPE</code> command. Once created, you can use the enumerator just like any other data type when creating a table.</p> <pre><code>CREATE TYPE likert AS ENUM ('strongly agree', 'agree', 'neutral', 'disagree', 'strongly disagree');\n\nCREATE TABLE survey_results (\n    question INT\n    result likert\n);\n</code></pre> <p>Enumerators are ordered sets, and so comparison operators are valid. For example, you could create an enumerator for the list of valid [[ordinal]] (i.e., ordered categorical) values and use a comparison operator to query records.</p> <pre><code>SELECT * FROM survey_results WHERE result &gt;= 'agree' ORDER BY likert;\n</code></pre> <p>Each enumerated type is separate. The same value in two enumerators are not equivalent. Enumerators are unlike numeric and text data types, so regular numeric string operators and function won't work.</p> <p>Alternatives to enumerators include using a [[#check]] constraint or creating a full table for the enumerated options and relating the field through a foreign key. For this second case, consider using the value itself as the primary key, as shown below. The downside of these options is that they are not ordered in the same way an enumerator is, and so comparison operators will not work.</p> <pre><code>CREATE TABLE likert (\n    response TEXT PRIMARY KEY\n);\n\nINSERT INTO likert (response) \nVALUES ('strongly agree', 'agree', 'neutral', 'disagree', 'strongly disagree');\n\nCREATE TABLE survey_results (\n    question INT\n    result TEXT REFERENCES likert (response) ON UPDATE CASCADE\n);\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#relationships","title":"Relationships","text":"<p>If your primary key uses BIGSERIAL, use BIGINT as the type for the foreign key (field that defines the relationship). If using SERIAL, use INT as the type for the field with the relationship.</p> <p>There are two patterns for specifying relationships. First, you can write the reference inline when specifying the table. Alternatively, you can use an ALTER TABLE command to add the relationship. When using the first strategy, the table to which you are making the relationship must already exist (or be specified first in the <code>.sql</code> file).</p>"},{"location":"pages/knowledge/base/SQL/#joins","title":"Joins","text":"<p>Joins allow you to combine data from multiple tables.</p> <pre><code>SELECT * FROM &lt;table1&gt; \nJOIN &lt;table2&gt; ON &lt;table1.field&gt; = &lt;table2.field&gt;;\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#working-with-dates","title":"Working with dates","text":"<p>Dates and times can be complicated. PostreSQL has a number of utilities that help store dates and times correctly and calculate time intervals or age.</p> <pre><code>CREATE TABLE dates (\n    date DATE,\n    time TIMESTAMP\n);\n\nINSERT INTO dates (\n    date,\n    time\n) VALUES (DATE '2022-11-02', 1999-01-08 04:05:06 );\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#time-intervals","title":"Time intervals","text":"<pre><code>NOW() + INTERVAL '1 YEAR';\n</code></pre> <p>See the docs for more detail.</p>"},{"location":"pages/knowledge/base/SQL/#age","title":"Age","text":"<pre><code>AGE(&lt;birthday&gt;)\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#extract","title":"Extract","text":"<p>Use <code>EXTRACT</code> to get a day, week, month, year, decade or century from a date.</p> <pre><code>EXTRACT(DAY FROM NOW());\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#casting","title":"Casting","text":"<p>Casting converts a value to another format, often used for datetimes. Use <code>::</code> (double colon) to cast.</p> <pre><code>NOW()::DATE;\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#data-types","title":"Data Types","text":"Name Description <code>integer</code> typical choice for integer <code>real</code> inexact floating point up to 6 decimal places (see also <code>double precision</code> and <code>numeric</code>) <code>text</code> variable length character string (see also <code>varchar(n)</code>) <code>boolean</code> true/false <code>date</code> date <code>timestamp</code> date and time (see options for time zones) <code>bigserial</code> auto-incrementing integer up to 9,223,372,036,854,775,807 <code>uuid</code> universally unique identifier <code>money</code> currency amount <code>point</code> geometric point <code>polygon</code> geometric polygon <code>jsonb</code> textual json data (see also <code>json</code>) <code>varchar(n)</code> string not to exceed length n <code>serial</code> auto-incrementing integer up to 2,147,483,647 <code>double precision</code> inexact floating point up to 15 decimal places <code>numeric</code> high precision floats (slow calculations) <code>time</code> time only <code>time interval</code> interval of time <p>All available data types are described here.</p>"},{"location":"pages/knowledge/base/SQL/#postgresql-json-data-type","title":"PostgreSQL JSON data type","text":"<p>JSON and JSONB both store JSON data. JSONB is generally preferred because, while it is slower to load, it is faster to query. The \"B\" in JSONB stands for binary and is sometimes referred to as \"BSON\". JSONB will remove trailing white spaces and any duplicate keys. This shouldn't be a problem for well-constructed JSON, but it's important to know. Read the docs before using this data type to understand it's limitations.</p> <p>To insert JSON data, wrap the dictionary in single quotes and use double quotes for any string-like keys or values.</p> <pre><code>CREATE TABLE my_json(\n  id SERIAL PRIMARY KEY,\n  data JSONB\n);\n\nINSERT INTO my_json (data)\nVALUES ('{\"name\": \"Coffee Value Chains\", \"year\": 2022, \"results\": {\"indicator1\": 100, \"indicator2\": 150}, \"contacts\":[\"Sergio\", \"Monica\"]}'});\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#basic-queries","title":"Basic queries","text":"<p>Use the <code>-&gt;</code> operator to query within a JSON object. </p> <pre><code>-- Select a property\nSELECT data -&gt; 'name' as name FROM my_json;\n\n-- Select a property based on a condition\nSELECT data -&gt; 'results' FROM my_json WHERE data -&gt; 'name' = '\"Coffee Value Chains\"';\n</code></pre> <p>You can chain multiple dash-arrow ( <code>-&gt;</code> ) operators to query within nested JSON.  Alternatively, use the hash-arrow ( <code>#&gt;</code> ) operator to query by passing a path of properties (wrapped in curly braces <code>{}</code> ). Queries for JSON and JSONB are the same.</p> <pre><code>-- Select nested JSON object field by property\nSELECT data -&gt; 'results' -&gt; 'indicator1' as indicator1 FROM my_json;\n\n-- Alternative to select nested JSON object field by property\nSELECT data #&gt; '{results, indicator1}' as indicator1 FROM my_json;\n</code></pre> <p>The previous queries all return JSON objects so that you can continue to query them. The dash-double-arrow ( <code>-&gt;&gt;</code> ) operator will return text from the query instead of JSON.</p> <pre><code>-- Select a JSON object field as text\nSELECT data-&gt;&gt;'name' as name FROM users; -- you cannot chain after returning text\n</code></pre> <p>If your JSON contains arrays, you can index into the array using the query techniques described above combined with the element's index to get a specific element from an array.</p> <pre><code>-- Select a JSON array element\nSELECT data -&gt; 'contacts'-&gt; 0 as first_contact FROM my_json;\n\n-- Alternative to select a JSON array element\nSELECT data #&gt; '{contacts, 0}' FROM my_json;\n</code></pre> <p>You can also unpack an array using the <code>jsonb_array_elements_text()</code> function.</p> <pre><code>SELECT id, jsonb_array_elements_text(data -&gt; 'contacts') FROM my_json;\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#contains-queries","title":"Contains queries","text":"<p>Use the contains operator ( <code>@&gt;</code> ) to return records that include a specific property or property value.</p> <pre><code>-- Select all records that contain a specific property\nSELECT * FROM my_json WHERE data @&gt; '{\"results\": \"indicator1\"}';\n\n-- Select all records that contain a specific property value\nSELECT * FROM my_json WHERE data @&gt; '{\"results\": \"indicator1: 100\"}';\n\n-- Select all records that contain a specific value(s) in an array\nSELECT * FROM my_json WHERE data -&gt; 'contacts' @&gt; '[\"Monica\"]';\n</code></pre> <p>Use the property-test ( <code>?</code> ) operator to query records based on the presence of one or more top-level properties.</p> <pre><code>-- Select all records that contain a specific key\nSELECT * FROM my_json WHERE data ? 'results';\n\n-- Select all records that contain any of these keys\nSELECT * FROM my_json WHERE data ?| ['results', 'contacts'];\n\n-- Select all records that contain all of these keys\nSELECT * FROM my_json WHERE data ?&amp; ['results', 'contacts']; \n</code></pre>"},{"location":"pages/knowledge/base/SQL/#updating-json-objects","title":"Updating JSON objects","text":"<p>Use the <code>jsonb_set()</code> function to update JSONB objects.</p> <pre><code>UPDATE my_json SET data = jsonb_set(data, '{year}', 2023);\n</code></pre> <p>If any records do not have the property you want to update, you must first create it. To avoid overwriting those records that do have the property, use the <code>COALESCE</code> command with an empty object ( <code>'{}'</code> ).</p> <pre><code>UPDATE my_json SET data = jsonb_set(data, '{\"targets\"}', COALESCE(data-&gt;'targets', '{}'));\nSELECT jsonb_set(data, '{targets, indictaor1}', (SELECT data-&gt;'indicator1' FROM my_json j1 WHERE j1.id=j2.id)) FROM my_json j2;\n</code></pre>"},{"location":"pages/knowledge/base/SQL/#indexing-json-objects","title":"Indexing JSON objects","text":"<p>JSONB supports GIN indexes by means of <code>json_ops</code> and <code>jsonb_path_ops</code>. <code>json_ops</code> creates an index item for every property and value in the JSON whereas <code>json_path_ops</code> is more compact and faster to process but supports fewer operations.</p> Additional Resources <ul> <li>DevHints.io/postgresql-json</li> <li>PostgreSQL JSON Functions &amp; Operators</li> <li>PostgreSQL Tutorial JSON</li> </ul>"},{"location":"pages/knowledge/base/activity-level-data/","title":"Activity-level data","text":"<p>This tutorial provides an introduction to activity-level data, the data implementing partners collect and report.</p>"},{"location":"pages/knowledge/base/activity-level-data/#key-concepts","title":"Key Concepts","text":""},{"location":"pages/knowledge/base/activity-level-data/#data-collected-by-implementing-partners","title":"Data collected by implementing partners","text":"<p>Implementing partners collect the following types of data</p> <ul> <li>Activity Location Data: depict an activity\u2019s location of implementation and intended beneficiaries</li> <li>Monitoring Data: performance and context indicator data</li> <li>Thematic Data: all other data; examples include scientific research, population-based surveys, and census data</li> </ul>"},{"location":"pages/knowledge/base/activity-level-data/#data-collection-and-reporting-requirements","title":"Data collection and reporting requirements","text":"<p>ADS 579 requires implementing partners </p> <ol> <li>develop Data Management Plans for any Activity that also requires an AMELP</li> <li>collect Activity Location Data at the appropriate geographic level</li> <li>submit data collected or acquired to the Development Data Library (DDL)</li> </ol>"},{"location":"pages/knowledge/base/activity-level-data/#introduction","title":"Introduction","text":"<p>Implementing partners (IPs) capture substantial data through activity implementation that might be used to inform future activities, collaborate with partners, and coordinate across activities. ADS 579 requires that certain data be collected by IPs and that collected data be contributed to the Development Data Library (DDL). Establishing clear guidance and an efficient method for IPs to submit data they collect or acquire will support reporting, increase data quality, and result in more informed decision-making by the Mission and development community.</p>"},{"location":"pages/knowledge/base/activity-level-data/#what-types-of-data-do-ips-collect","title":"What types of data do IPs collect?","text":"<p>We categorize the types of data collected by IPs into three types: </p> <ul> <li>Activity Location Data: depict an activity\u2019s location of implementation and intended beneficiaries</li> <li>Monitoring Data: performance and context indicator data</li> <li>Thematic Data: all other data; examples include scientific research, population-based surveys, and census data</li> </ul> <p>The data collected by IPs conducting Evaluations is not covered in this tutorial, refer instead to USAID\u2019s Evaluation Policy.</p>"},{"location":"pages/knowledge/base/activity-level-data/#activity-location-data","title":"Activity Location Data","text":"<p>Activity Location Data depict an activity\u2019s location of implementation and intended beneficiaries to support operational awareness and programmatic decision making. Activity Location Data allows USAID to ask and answer questions, such as: </p> <ul> <li>Where do we work and what do we do there?</li> <li>Where are the people and places that benefit from USAID programming?</li> <li>Where are opportunities to strategically coordinate USAID investments to maximize impact?</li> </ul> <p>ADS 579 defines minimum requirements for IPs to report Activity Location Data. ADS 201/579 Mandatory Reference: Activity Location Data requires IPs collect data that indicate </p> <ol> <li>the location where an activity is implemented and </li> <li>the location of the activity\u2019s intended beneficiaries.</li> </ol>"},{"location":"pages/knowledge/base/activity-level-data/#monitoring-data","title":"Monitoring Data","text":"<p>Monitoring data refers to performance and context indicator data and is governed for the IP by the Activity Monitoring, Evaluation and Learning Plan (AMELP). ADS 201 recommends disaggregating indicator data by geographic location and ADS 201 Additional Help: Monitoring Data Disaggregation by Geographic Location provides guidance on doing so.</p>"},{"location":"pages/knowledge/base/activity-level-data/#thematic-data","title":"Thematic Data","text":"<p>Thematic data refers to all other data collected or used by IPs. Examples include but are not limited to scientific research, population-based surveys, and census data. ADS 579 USAID Development Data requires that all data collected or acquired by IPs is submitted to the Development Data Library (DDL). </p> <p>ADS 579 Additional Help: Geographic Data Collection and Submission Standards provides guidance on fulfilling this requirement in regard to geographic data assets. </p>"},{"location":"pages/knowledge/base/activity-level-data/#what-is-required-under-ads-579","title":"What is required under ADS 579?","text":"<p>ADS 579 provides specific requirements for the collection of Activity Location Data and the reporting of data collected to the DDL. </p>"},{"location":"pages/knowledge/base/activity-level-data/#data-management-planning","title":"Data management planning","text":"<p>Data Management Plans (DMPs) enable USAID staff and implementing partners (IPs) to identify data assets that will be created and used in a USAID-funded activity. DMPs complement Activity Monitoring, Evaluation, and Learning Plans (AMELPs). Activity DMPs also describe important data management tasks to ensure the use of data management best practices at each stage of a lifecycle. They ensure that data assets contribute to the Agency\u2019s evidence base and that implementing partners submit data assets designated as deliverables in accordance with conditions and guidelines in awards.</p> <p>Activity DMPs are required for: </p> <ul> <li>any Activity that requires an AMELP (see ADS 201.3.4.10A Activity MEL Plan). The DMP may be developed and approved as a section of the MEL Plan or as a separate plan. Similar to the Activity MEL Plan, OUs and Missions must put an Activity DMP in place before major implementation actions begin and update it as necessary. </li> <li>any USAID efforts that meet the criteria for Federally Funded Scientific Research (see the USAID Scientific Research Policy). The DMP must describe best practices for managing data assets during the research effort and describe protocols for ensuring public access to related publications and underlying research data.</li> </ul>"},{"location":"pages/knowledge/base/activity-level-data/#collecting-activity-location-data","title":"Collecting Activity Location Data","text":"<p>Activity Location Data include both an activity\u2019s location of implementation and the location of intended beneficiaries. Both components of Activity Location Data must be collected by the IP at the Exact Site Location level. IPs must select the most applicable type of Exact Site Location from this list of options: </p> <ul> <li>Administrative Unit Area: the Administrative Unit in which the Activity takes place. In most cases, the IP would indicate the Municipality (identifying it by its official INE code) in which the activity occurred.</li> <li>Populated Place Location: the name of a populated place in which the Activity took place. USAID/Guatemala maintains an official list of populated places for IPs to reference to prevent ambiguous references.</li> <li>Exact Line or Area Feature: a recognizable feature such as a protected area boundary. </li> <li>Exact Site Location: may be either a recognizable feature such as a health site or school or geographic coordinates. USAID/Guatemala maintains an official list of schools and health sites for IPs to reference to prevent ambiguous references.</li> </ul> <p></p>"},{"location":"pages/knowledge/base/activity-level-data/#location-of-implementation","title":"Location of implementation","text":"<p>If the activity is implemented in a location that is directly related to the development results generated by the activity, then collect the Exact Site Location or Exact Line or Area Feature. For example, if an Activity is located in a school to benefit the students or faculty of the school, collect the school name (Exact Site Location). If the Activity is located in a protected area and intended to benefit the ecosystem of that protected area, collect the name of the protected area (Exact Area Feature).</p> <p>If the activity is implemented in a location that is not directly related to the development results generated by the activity, then collect the Populated Place Location. For example, if an Activity focused on policy reform to improve agricultural policy in the Western Highlands takes place in Guatemala City, record Guatemala City (Populated Place Location) since the development results are not related to the location of implementation.</p> <p>We also recommend that IPs collect the latitude(s) and longitude(s) of the activity location or activity boundary whenever possible, except when doing so might reveal Personally Identifiable Information (e.g., do not collect the lat/lon of a household in a survey). Geographic coordinates can be used to help with disambiguation and confirm data quality. </p>"},{"location":"pages/knowledge/base/activity-level-data/#location-of-intended-beneficiaries","title":"Location of intended beneficiaries","text":"<p>The Location of Intended Beneficiaries must be collected, at a minimum, at the Department level (Administrative Unit 1). However, whenever possible we recommend collecting beneficiary location data at the Municipal level (Administrative Unit 2). Ideally, the Location of Intended Beneficiaries should be collected at the level of granularity necessary to support operational awareness and programmatic decision-making. When the activity benefits all residents of Guatemala, the Location of Intended Beneficiaries should be recorded as the country of Guatemala.</p> <p>For example, consider an activity intended to build capacity for Municipality managers in the Department of Huehuetenango. The training is conducted in the city of Huehuetenango but benefits managers of Municipalities across the Department. In this case, the Department of Huehuetenango will represent the activity\u2019s Location of Intended Beneficiaries. However, if only a few managers of the 31 Municipalities in Huehuetenango attend, the Municipalities of the attending managers will better represent the activity\u2019s Location of Intended Beneficiaries. </p>"},{"location":"pages/knowledge/base/activity-level-data/#exemptions-and-waivers","title":"Exemptions and waivers","text":"<p>Activity Location Data does not need to be collected under the following circumstances</p> <ol> <li>If the Activity was implemented prior to issuance of ADS579mab, some requirements are relaxed. Location of implementation must be reported at the Department (Admin1) level, but higher precision is preferred. Location of beneficiaries is not required.</li> <li>For worldwide or regional partnerships with Public International Organizations (PIOs) that qualify as Project Contributions or General Contributions pursuant to ADS 308.3.9, Types of PIO Agreements</li> <li>For Inter-Agency Agreements (IAAs) that support broad development objectives that are not tied to specific countries</li> <li>For Activities funded by multiple donors in which USAID does not have direct control over the countries of implementation</li> </ol> <p>Waivers may be granted under extenuating circumstances if approved by the Mission Director and documented under an Action Memorandum for up to two years, which can be extended.</p>"},{"location":"pages/knowledge/base/activity-level-data/#submitting-data-to-the-ddl","title":"Submitting data to the DDL","text":"<p>The Development Data Library (DDL) is the Agency\u2019s repository of USAID-funded, machine readable data created or collected by the Agency and its implementing partners. Datasets and supporting documentation created or collected directly by USAID/Guatemala or under USAID-funded awards must be submitted for inclusion in the DDL. </p> <p>USAID staff, as well as contractors and recipients of USAID assistance awards (e.g., grants and cooperative agreements), must submit any dataset created or collected with USAID funding to the DDL in accordance with the terms and conditions of their awards.</p> <p>Requirements for which datasets must be submitted to the DDL are defined broadly (see ADS 579.3.3.2) and include:</p> <ul> <li>Data that inform performance monitoring (excluding the indicator data itself, which will be reported through alternative means) </li> <li>Surveys, economic assessments, and organization capacity assessments</li> <li>Research data</li> <li>USAID Information System data (e.g., financial data)</li> </ul> <p>Supporting documentation including codebooks, data dictionaries, data gathering tools (e.g., forms, survey instruments), explanations, data quality reviews, data collection methodologies, and other metadata must also be reported alongside the dataset.</p> <p>USAID will not necessarily release to the public all data submitted to the DDL in its entirety, however data submitters are responsible for redacting any sensitive information, including personally identifiable information (PII). Redactions must fall under one of the \u201cprincipled exceptions to the presumption in favor of openness\u201d established in OMB Bulletin 12-01, \u201cGuidance on Collection of U.S. Foreign Assistance Data.\u201d</p> <p>If the redaction process will remove information that would prove useful during secondary data analysis, USAID/Guatemala must work within prevailing best practices to identify alternative redaction methods or consider, with their GC or RLO point of contact, assigning an access level of \u201cRestricted Public\u201d to the Dataset. </p> <p>Should USAID staff or implementing partners submit a Dataset to a publicly accessible research database, they are not required to submit the data to the DDL. However, they must submit a notice to the DDL, providing details on where and how to access the data.</p> <p>The A/COR is responsible for reviewing Datasets prior to submission, which includes identifying principled exceptions to release, redacting (or ensuring the redaction of) sensitive information, and obtaining required clearances (see 579.3.2.5).</p>"},{"location":"pages/knowledge/base/activity-level-data/#managing-data-risk","title":"Managing data risk","text":"<p>USAID development data are collected, managed, and shared for an intended benefit, such as improved development decision making and accountability and transparency. However, USAID development data may also present some level of data risk in addition to benefit. Therefore, balancing benefit and risk is a key component of responsible data collection and management.</p> <p>As mentioned above, data submitted to the DDL will not always be publicly available. Do not unnecessarily limit data collection or reporting due to potential data risk or overly redact or aggregated data before it is submitted to the DDL. Instead, consider restricting data as \u201cRestricted Public\u201d or \u201cNon-Public\u201d. IPs may propose the appropriate data access level and recommend how the data should be redacted or aggregated before it is shared when Activity Location Data or other Datasets present risk.</p> <p>When data include PII, safeguarding procedures should be applied to protect PII from unauthorized access (see ADS 508, Privacy Program and Data Security Guidance: Protecting Beneficiaries). In addition, certain country contexts may present insecure environments that result in safety concerns regarding geographic data collection in the field. When applicable, these data collection constraints should also be considered when planning for data collection. </p> <p>Collecting data at a high level of geographic detail often increases the benefit of collecting the data yet it may also increase the data risk in certain scenarios due to the detailed location information included in the data. </p> <p>Data risk for geographic data should be evaluated at two stages:</p> <ol> <li>Planning for geographic data collection: consider if collecting data at the intended level of geographic detail will pose a substantial risk to beneficiaries or data collectors.</li> <li>Managing geographic data submitted to USAID: consider whether publicly releasing data at the current level of geographic detail will pose substantial risk.</li> </ol> <p>ADS 579saa Geographic Data Collection and Submission Standards provides guidance for considering risk related specifically to geographic data, including strategies for mitigating data collection risk. The Monitoring Toolkit\u2019s Data Security Guidance and the Considerations for Using Data Responsibly at USAID are also helpful resources.</p>"},{"location":"pages/knowledge/base/activity-level-data/#sharing-data","title":"Sharing data","text":"<p>The Agency encourages its partners, the academic and scientific communities, and the public at large to make broad use of USAID data for innovative scientific, technological, analytical, and other applications. The DDL is the primary source of shareable data, however Guatemala may also make available internal data management systems to IPs. This will increase the use of data by IPs, ensure data used by IPs is appropriate, and reduce duplication of efforts in data acquisition and cleaning.</p>"},{"location":"pages/knowledge/base/activity-level-data/#relevant-policies-and-initiatives","title":"Relevant policies and initiatives","text":"<p>The Foundations for Evidence-Based Policymaking Act of 2018 (Evidence Act) and subsequent Office of Management and Budget (OMB) Memoranda M-19-18 and M-19-23 lay the foundation for improving data collection and use in the federal government.</p> <p>The draft USAID Geospatial Strategy (in draft) will guide the Agency\u2019s strategy for collection and use of geospatial data specifically.</p> <p>ADS 579 USAID Development Data requires that all data collected or acquired by implementing partners is submitted to USAID. This is included as mandatory provisions in all awards, for example, see the language in ADS 302mas (see page 7).</p> <p>ADS 579mab Activity Location Data requires that all USAID OUs collect activity location data and this is most commonly done by including relevant requirements in solicitation/award language.</p> <p>ADS 579saa Geographic Data Collection and Submission Standards provides data collection and submission standards for geographic data to be used in implementing the geographic data collection and submission requirements outlined above.</p> <p>ADS 201 Operational Policy for the Program Cycle recommends disaggregating performance and context indicators by geographic location and there is an Additional Help document that provides implementation guidance.</p> <p>Proposed revisions to the AIDAR will increase the importance of and requirements for Data Management Plans (DMPs) and Activity Monitoring, Evaluation and Learning Plans (AMELPs).</p> <p>The rollout of the Development Information System (DIS) and proposed combination of the Development Experience Clearinghouse (DEC) and Development Data Library (DDL) into a single Digital Front Door (DFD) for the Agency will also affect this proposal.</p>"},{"location":"pages/knowledge/base/add-psql-to-path/","title":"Add psql to your PATH variable","text":"<p>Using Windows Explorer, locate the executable for <code>psql</code>. This file should be located in the <code>bin/</code> folder within the directory you specified when installing PostgreSQL. Copy the path to the file <code>psql.exe</code>.</p> <p>Next, add this path to the PATH variable.</p> <ol> <li>In the Control Panel, select System &gt; About and click Advanced system settings (or search \"edit the system environment variables\" in the Windows Menu).</li> <li>Click Environment Variables.</li> <li>Under System Variables, double-click the Variable \"Path\" (or select and click Edit).</li> <li>In the Edit environment variable dialog box, click New.</li> <li>Paste the path into the dialog box.</li> <li>Click Ok and then Ok again.</li> </ol>"},{"location":"pages/knowledge/base/crs/","title":"Coordinate Reference Systems","text":""},{"location":"pages/knowledge/base/crs/#coordinate-reference-systems","title":"Coordinate Reference Systems","text":"<p>The Coordinate Reference System (CRS) tells the software reading the spatial data where the data are located on the globe. For an introduction to this topic, see this article.</p> <p>For most web applications use WGS 84 (EPSG code 4326), a geographic coordinate system.</p> <p>Guatemala has an official projection that is used by IDEG and other government data sources in some instances. It is not registered with an EPSG code, so you must use the proj4 string to set the CRS when reading or immediately thereafter. Then you can convert to WGS84.</p> <p>The proj4 string is </p> <pre><code>\"+proj=tmerc +lat_0=0 +lon_0=-90.5 +k=0.9998 +x_0=500000 +y_0=0 +ellps=WGS84 +units=m +no_defs\"\n</code></pre> <p>To set projection using GeoPandas, a Python library, use </p> <pre><code>import geopandas as gpd\ngdf = gpd.read_file('/path/to/file/filename.ext')\ngdf.crs = \"+proj=tmerc +lat_0=0 +lon_0=-90.5 +k=0.9998 +x_0=500000 +y_0=0 +ellps=WGS84 +units=m +no_defs\"\n</code></pre> <p>For any spatial calculations, you will instead want a projected coordinate system. See here for more on the difference between geographic and projected coordinate systems.</p> <p>The Universal Transverse Mercator (UTM) CRS is a common a projected coordinate system. It's units are meters, so any area calculations are in square meters, distance is in meters, etc. To find the UTM zone of your geography, use this application. When a geography spans more than one zone, this CRS will still work but it will be skewed as you get further outside the selected zone. There are also a number of projected coordinate systems devised for different geographies, so you can generally find one that is tailored to the bounds of your geography.</p> <p>To find the EPSG code of the UTM Zone, search this database.</p> <p>Alternatively, use World Equidistant Cylindrical (EPSG code 4087) for imprecise calculations without worrying about which UTM zone to use.</p>"},{"location":"pages/knowledge/base/data-design/","title":"Principles of good data design","text":"<p>Tidy Data</p> <p>Normal-form Data</p> <p>Data Standards</p> <p>Interoperability, Standards</p> <p>Interoperability is the ability of multiple applications to communicate with one another by accessing, exchanging and making use of data in a coordinated manner.</p> <p>\u201cIt is often said that 80% of data analysis is spent on the process of cleaning and preparing the data.\u201d1</p> <p>1 Dasu T, Johnson T (2003). Exploratory Data Mining and Data Cleaning. Wiley-IEEE as cited in:</p> <p>2Wickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1 - 23. </p>"},{"location":"pages/knowledge/base/data-pipeline-rationale/","title":"Data Pipeline Options","text":"<p>This article describes the rationale for developing the semi-automated data pipelines.</p>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#piping-data-to-the-database","title":"Piping data to the database","text":"<p>The Activity Database, Thematic Database, and most Activity data will be stored in Google Drive. Each needs to be on Google Drive to either allow users to interact with it (Activity DB, Activity data) or serve to the Data Hub (Thematic Database). </p> <p>For the Activity Database especially, the data most be converted from a flat file to a set of related tables that can be stored in a relational database. In addition, the Activity Database includes some procurement sensitive information that we may want to exclude from the Mission Database or partition to a sensitive layer.</p> <p>ETL of these data into the Mission Database therefore requires a strategy for (1) reading data from Google Drive, (2) manipulating data into desired formats, and (3) reading the data in to the Mission Database. A layer of data validation may also be useful. </p> <p>The Mission has immediate access to tools such as Python (Google Collab) and JavaScript (Google Apps Script) for these tasks, or the Data Steward could request access to additional technology like a cloud database. Below we discuss each option for ETL of Mission data into the Mission Database.</p> <ol> <li>Get permission to use a cloud database. If approved, a cloud database would  be able to serve data for applications that require it (e.g, Data Hub) and allow programmatic access to use a scripting language like Python for ETL tasks. This is likely the best option for functionality but the least likely option given M/CIO attitudes towards cloud databases.</li> <li>Use Python (Google Collab): Python libraries like SQLAlchemy can support loading data into the Mission Database. The fatal flaw of this option is M/CIO's restriction on programmatic access to Google Drive by Google Collab. Without this access, you must download data to csv and upload each to Google Collab before processing and passing to the database. </li> <li>Use JavaScript (Google Apps Script): Google Apps Script allows easy access to data/docs on Google Drive. However, Apps Script does not have a clean interface to download data locally (it stores it back in Google Drive instead). As with Python, JavaScript could be used to create <code>.sql</code> files for defining and manipulating the database based on Google Sheets.</li> <li>Use PL/pgSQL: Apparently PostgreSQL can interact with JavaScript through it's PL/pgSQL language. I don't have experience with this but it could be combined with Google Apps Script to avoid an interim step of storing data locally.</li> <li>Use PostgreSQL <code>PROGRAM</code> and <code>curl</code>: for public data, PostgreSQL can read directly from a url. Because PostgreSQL cannot authenticate with  Google Drive, this option will not work for sensitive data. However, it will save considerable lines of SQL for data that can be served publicly. </li> <li>Use Google Sheets: one or more tabs could be added to the sheet or IMPORTRANGE could be used to import data that is further manipulated in that sheet. A download link can be created to give access to any program that can read from the web (including PostgreSQL).</li> </ol>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#recommendation","title":"Recommendation","text":"<p>Of the options explored above, the best option in my opinion is to use Google Apps Script given the ease with which it interacts with Google Drive. Google Sheets can also be used for some intermediate processing, especially when intermediate processing is necessary for other downstream applications (e.g. Tableau needs well-formatted data from the Activity Database). Google Apps Script provides a few options for implementation.</p> <ol> <li>Custom menu: A custom menu allows the Data Steward to very easily run the script, but slows loading time for the sheet considerably and exposes the menu to external users, which may not be desired. When those drawbacks are acceptable, this is a good choice.</li> <li>Containerized non-menu script: A script can be containerized within a Google Sheet but not exposed through a custom menu. Instead, the Data Steward would open the Apps Script editor and run the script manually. One advantage is that the script is copied anytime the Sheet is copied, which could be useful for often-replicated sheets like the Activity Data Tracker.</li> <li>Non-containerized: A non-containerized script would only be accessible to the Data Steward. It could be deployed as a web app with additional administrative functionality. This option requires that each sheet is registered individually (which could be done in a Sheet like the Data Inventory) so the script knows where data resides. Long term, a web app could provide really useful functionality for the Data Steward.</li> <li>Scheduled: A trigger can be set to run the script (or a function) at some regular interval. This can save the Data Steward from needing to interact with the script file to update the Mission Database, however if the solution cannot both interact with Google Drive and the database, the Data Steward will need to manually close the loop. </li> </ol>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#converting-tables-to-sql","title":"Converting tables to SQL","text":"<ol> <li>Google Apps Script Content Service can serve the <code>.sql</code> to the web, which can be accessed with <code>curl</code> to programmatically read the <code>.sql</code> to a local file before uploading, rather than requiring the Data Steward to do this manually, however, each <code>.sql</code> file would require it's own project.</li> <li>Would an API service on Google Apps Script work here?</li> <li>Could the Data Steward Admin dashboard have a download button rather than save the file to Google Drive?</li> </ol> <p>Always use prepared statements to sanitize inputs! See example in Activity Database below.</p>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#specific-designs","title":"Specific Designs","text":""},{"location":"pages/knowledge/base/data-pipeline-rationale/#activity-database","title":"Activity Database","text":"<p>This process automates the update of the Mission Database with changes to the Activity Database on a daily basis. </p> <ol> <li> <p>In the Activity Database sheet, add a tab to format data for export to another Google Sheet (<code>export2gdb</code>). Copy each column individually (using named ranges when available) to avoid changing data ranges when columns are added. </p> </li> <li> <p>In a new Google Sheet within the MECLA Toolkit (<code>GIS/Thematic Data/internal/activity-data.ghseet</code>) use IMPORTRANGE to import the <code>export2gdb</code> sheet, wrapping with a QUERY to exclude procurement-sensitive data. Because this sheet has read permissions on the Activity Database, you must NEVER SHARE edit access outside of USAID.</p> </li> <li> <p>In the <code>activity-data</code> sheet, process data from the flat file to multiple tabs to third-normal form</p> </li> <li> <p>Use a Google Apps Script to create a <code>.sql</code> script to <code>INSERT</code> the tabs in the <code>activity-data.gsheet</code> as a <code>.sql</code> file on Google Drive. Use <code>PREPARE</code> and <code>EXECUTE</code> to sanitize inputs. Note that prepared statement names (<code>fooplan</code>) below, must be unique across the session (docs).</p> </li> </ol> <pre><code>PREPARE fooplan (int, text, bool, numeric) AS\n    INSERT INTO foo VALUES($1, $2, $3, $4);\nEXECUTE fooplan(1, 'Hunter Valley', 't', 200.00);\n</code></pre> <ol> <li>Use the saved <code>.sql</code> file to read the data into the Mission Database using <code>psql</code>. Drop the original table and add the new table in its place. Curl will not work with these data unless they are published with global view access since Postgres doesn't have access to my Google Account. </li> </ol>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#thematic-data","title":"Thematic Data","text":"<p>This process supports as-needed updates of thematic data in the Mission Database. We use a non-containerized script to prevent slow loading with the Data Hub and avoid exposing the script to the public (<code>thematic-data.gsheet</code> is shared publicly).</p> <p>The process described below describes how to add a new dataset to the thematic dataset</p> <ol> <li>Describe the new dataset in the data tab of the <code>Data Inventory-internal</code> worksheet. If the data provider is new, update the providers tab.</li> <li>Add dataset to a new tab in the the thematic database. Ensure that geocodes (<code>Codigo</code> and <code>Codigo1</code>) are correct and that municipalities and departments are spelled correctly. Populate all blank fields with #N/A (use find/replace if needed). </li> <li>Translate to English if needed and record translations in the <code>TRANSLATIONS</code> tab.</li> <li>Describe codes in the <code>CODES</code> tab.</li> <li>Add definitions to the <code>DEFINITIONS</code> tab. To allow access to the data in Map Viewer, ensure the data type attribute is set to one of the Map Viewer supported data types.</li> <li>Use a Google Apps Script to create a <code>.sql</code> script to <code>COPY</code> the new dataset along with the <code>DEFINITIONS</code>, <code>CODES</code>, and <code>TRANSLATIONS</code> tables on Google Drive. </li> <li>Use the saved <code>.sql</code> file to read the data into the Mission Database using <code>psql</code>. Drop the original table and add the new table in its place.</li> </ol> <p>To update a dataset</p> <ol> <li>Make required changes in the <code>thematic-data</code> worksheet. Append rows to end of a sheet to add new data or edit records directly if needed.</li> <li>Proceed from step 3 above. The entire table will be dropped and replaced with the new or edited data.</li> </ol> <p>To add new data from the DDL</p> <ol> <li>Use the process for adding new data described above, adding the access link to the DDL in the <code>Data Inventory-internal</code> sheet.</li> </ol> <p>The <code>.sql</code> file should look like this, using <code>curl</code> to download the data. The data type (and any constraints or relationships) can be read programmatically from the DEFINITIONS tab of each spreadsheet. Additional relationships may need to be defined in another script with an ALTER TABLE command.</p> <pre><code>DROP TABLE IF EXISTS tablename;\n\nCREATE TABLE tablename (\nid PRIMARY KEY NOT NULL,\nheader1 header1type,\nheader2 header2type\n);\n\nCOPY tablename\n(header1, header2)\nFROM PROGRAM 'curl -L \"download-link\"'\nHEADERS CSV DELIMITER ',';\n</code></pre> <p>The download link for google sheets takes the form </p> <pre><code>https://docs.google.com/spreadsheets/d/&lt;spreadsheet_id&gt;/export?format=csv&amp;gid=&lt;sheet_id&gt;\n</code></pre> <p>Writing to a text file in Google Drive</p> <p>https://stackoverflow.com/questions/35865273/how-to-update-google-drive-text-file-via-google-script</p> <p>Important components</p> <ul> <li>Activity Location Data (internal)</li> <li>Activity Location Data (public)</li> <li>Thematic Data</li> <li>activity-data</li> <li>Data Steward Admin Dashboard</li> <li>SQL folder (or just one file that is overwritten each time? maybe archive past versions?)</li> <li>Folder for performance monitoring reports (as json?)</li> </ul>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#activity-location-data","title":"Activity Location Data","text":"<ol> <li>Copy the Activity Location Data template to the Activity's Google Drive folder.</li> <li>Register the download link for the template in a Google Sheet (don't publish the template to the web, just use the link described above, it should only be accessible to the Data Steward)</li> <li>Create a scheduled Google Apps Script to compile all data from each Activity Location Data template (using the links registered in step 2) into a single Google Sheet. </li> <li>Create another scheduled Google Apps Script to create a <code>.sql</code> file to <code>COPY</code> new Activity Location data to the Mission Database. </li> <li>Use the Google Colab script to convert Activity Location Data to hex locations and transfer to Activity Location Data (Public).</li> </ol>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#activity-specific-data","title":"Activity-specific data","text":"<p>Activity-specific data refers to the raw performance monitoring data and other thematic datasets collected by an Implementing Partner. Not every dataset collected is worth including in the Mission Database. Many datasets will not be updated (they will only be produced once), but some may be updated quarterly (especially performance monitoring data). It is critical that the Data Steward meet with the Implementing Partner early in the award to inventory datasets, agree on data formats, and plan to ingest valuable data into the Mission Database. </p>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#periodically-updated-data","title":"Periodically updated data","text":"<p>Activities commonly collect two core datasets:</p> <ul> <li>List of beneficiaries and their characteristics</li> <li>List of implementation activity locations and details</li> </ul> <p>These are clearly related to Activity Location Data, but they are often not in the format of the Activity Location Data template, so they must be re-formatted for upload to the DIS. They often drive most if not all of the performance monitoring data (number of women trained, etc.). However they are not the rolled up performance monitoring data that is also reported to the DIS. Therefore they are both but neither. </p> <p>Datasets like these are of interest for quarterly updating into the Mission Database. Follow this process</p> <ol> <li>Register the dataset in the Data Inventory sheet within the Activity Drive Folder. The Data Inventory sheet must include a DEFINITIONS tab that specifies</li> <li>short header</li> <li>data type</li> <li>constraints</li> <li> <p>Register the Data Inventory sheet in the Data Steward Admin Dashboard (or you could register the entire folder and programmatically access it)</p> </li> <li> <p>Use a Google Apps Script to access each Data Inventory sheet and the datasets registered within to create a <code>.sql</code> file to update the Mission database. </p> </li> </ol> <p>Encourage the Implementing Partner to append new records to their dataset, rather than creating multiple datasets by reporting period. Not only will this make updating the Mission Database easier, it will also force them to de-dup data rather than require you to figure out how to do that.</p>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#from-the-ddl","title":"From the DDL","text":"<p>Many datasets will be submitted to the DDL (or should be). After submission, they can be added to the Thematic Database and the process described for those datasets should be followed. The DDL has an API that could be used for this purpose.</p>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#performance-monitoring-reports","title":"Performance Monitoring Reports","text":"<p>Performance Monitoring Reports contain the targets, baselines, and actuals for performance measures by Activity. Given the unstructured nature of these data, they may be best saved as <code>json</code> documents. PostgreSQL can handle <code>json</code> documents.</p>"},{"location":"pages/knowledge/base/data-pipeline-rationale/#spatial-data","title":"Spatial Data","text":"<p>Spatial data are stored in ArcGIS Online and the <code>Spatial/</code> directory of the Thematic Database. Spatial data can be uploaded to PostgreSQL with PostGIS and to ArcGIS Online. Given the sporadic nature of acquiring spatial data, this can be handled manually by the Data Steward and GIS Specialist. </p>"},{"location":"pages/knowledge/base/data-steward-job-sim/","title":"Data Steward Job Simulation","text":"<p>The Data Steward should be skilled in data cleaning, analysis and management. This job simulation is designed to test the skills of applicants to the Data Steward position. The full exercise can be found here. Full instructions are included with the exercise. The following is a brief summary of the tasks required of the applicant.</p>"},{"location":"pages/knowledge/base/data-steward-job-sim/#task-overview","title":"Task Overview","text":""},{"location":"pages/knowledge/base/data-steward-job-sim/#task-1","title":"Task 1","text":"<p>This task reflects a core job requirement for the Data Steward to collaborate with partners to improve data collection, storage, and management. The applicant will review a realistic but manufactured dataset that was compiled from two different partners. The data reflects attendance at each partner's event. In this task, the Data Steward will review the data for data quality issues and recommend best practices to help the partners collect data in a more systematic way. </p>"},{"location":"pages/knowledge/base/data-steward-job-sim/#task-2","title":"Task 2","text":"<p>This task is intended to evaluate proficiency with SQL and knowledge of database normalization. The applicant will normalize the data to prepare it for efficient storage in a relational database management system. The applicant will use SQL to write the Data Definition Language (DDL) to create one or more tables necessary to store the data as efficiently as possible. The applicant is expected to normalize data to Boyce and Codd Normal Form.</p>"},{"location":"pages/knowledge/base/data-steward-job-sim/#task-3","title":"Task 3","text":"<p>This task is intended to evaluate basic proficiency with JavaScript. Example code is provided that converts a simplified dataset to JSON format with JavaScript with some coding errors. The applicant is expected to find and fix errors to produce the desired output. </p>"},{"location":"pages/knowledge/base/data-steward-job-sim/#rubric","title":"Rubric","text":"<p>The rubric for scoring is available here (please request access to view).</p>"},{"location":"pages/knowledge/base/database-rationale/","title":"Database Options","text":"<p>The Data Steward/GIS Specialist are facing increasing data storage requirements and increasingly complex analysis requests. </p> <p>Databases and similar concepts (e.g., data lakes and lake houses) offer a solution for both of these requirements. Data can be stored efficiently and more easily queried in a relational database or No-SQL database. A database or similar technology seems necessary to meet the long term needs of the Mission for Mission-wide analysis and maintenance of data over time.</p>"},{"location":"pages/knowledge/base/database-rationale/#relational-or-no-sql","title":"Relational or No-SQL?","text":"<p>The first question is whether to use a relational database, No-SQL database, or some combination of the two. Data lakes and other less-structured approaches are beyond the scope of the current program, but may be of interest to a more sophisticated implementing partner (through the MECLA Platform, for instance).</p> <p>Relational databases are best for well-structured data and rapid analysis across tables. No-SQL databases are best for less-structured data. Combinations of the two are also possible.</p> <p>For example, activity data could all be stored in a relational database. These data are well structured because all activities share attributes  (e.g., name, geographic focus, ceiling, PM alignments).</p> <p>Performance monitoring data could be stored in a file-based No-SQL database. This would allow each activity to have different disaggregates for each PM (which represents less-structured data). </p> <p>Links between the two would be maintained in the relational database.</p>"},{"location":"pages/knowledge/base/database-rationale/#options-for-relational-databases","title":"Options for Relational Databases","text":""},{"location":"pages/knowledge/base/database-rationale/#sqlite","title":"SQLite","text":"<p>SQLite is a lightweight, file-based database solution that is distributed with Python. This means that anyone with ArcGIS Pro (which includes Python) has access. SQLite can also be downloaded as a standalone application (you may also want SQLite Studio as a database client), however it is not in the Approved Product Catalog.</p> <p>SQLite can be used from the command line with the <code>sqlite</code> command line tool. However, SQLite is best when coupled with Python using the library <code>sqlite3</code> and analysis library like <code>pandas</code> in a Jupyter Notebook or Python application.  </p> <p>SQLite does not have a geospatial extension (like PostGIS for PostgreSQL). Instead, most analysis will be conducted in Python using <code>geopandas</code>. The Python interpreter of ArcGIS Pro can also provide access to the database for analysis with the <code>arcpy</code> library. </p> <p>This solution is best for an advanced Python user with access to ArcPro.</p>"},{"location":"pages/knowledge/base/database-rationale/#postgresql","title":"PostgreSQL","text":"<p>PostgreSQL is a robust, client/server style relational database management system. PostgreSQL has been approved by M/CIO for restricted use and must be requested for new uses. pgAdmin is the database client and psql is the command line tool. </p> <p>PostgreSQL includes PostGIS, an extension for geospatial data, bindings for Uber's H3, and JSON support, replicating some of the benefits of a file-based No-SQL database.</p> <p>This solution is best for an a user with database management and SQL experience. It is more scalable and deployable than SQLite. Also, it does not require a unique M/CIO review. </p>"},{"location":"pages/knowledge/base/digital-data-collection/","title":"Digital Data Collection Options","text":"<p>This review of digital data collection options describes a subset of available digital data collection options potentially available to the Mission. There are many options for digital data collection and options are changing all the time.  </p> <p>See also the report Technologies for Data Collection, Processing and Communication in Education in Emergencies.</p>"},{"location":"pages/knowledge/base/digital-data-collection/#digital-data-collection-tools","title":"Digital Data Collection Tools","text":"Tool Description Pros &amp; Cons Google Docs Easy to collect narrative information. Use tables, dropdowns and checkboxes to create limited structure. Combine with Google Apps Script to automate extraction and transfer of data. + Easy-to-use and quick to set up; best for longer narratives.- Best for use informally or with trusted partners; not suitable for lots of quantitative data. Google Sheets Familiar tool for structured data collection. Best for quantitative and brief narrative data. Extend with Google Apps Script for custom UI elements. + Familiar tool for quantitative data; supports batch upload via template.- Can quickly become difficult to navigate; bad for qualitative data. Google Forms A simple and intuitive survey tool integrated within Google Workspace. + Easy-to-use and quick to set up; data connects directly to Google Sheets.- Features are limited for all but the most basic surveys and data collection tasks. Kobo Toolbox A full-feature survey tool built specifically for humanitarian organizations. Now includes native support for Google Drive and Google Sheets. + Lots of question types; supports more complex surveys.- Data must be manually downloaded or connected to a Google Sheet via the API. Cloud storage not permitted by Agency. Survey123 A full-feature survey tool with geospatial data capture support integrated with ArcGIS software. + Designed for mobile collection of geospatial data; integrates well with other ArcGIS applications like Dashboards.- Less easy to integrate with Google Sheets. Fillable PDF The Agency's preferred form option. + Preferred by the Agency; good for record-keeping.- Limited functionality; requires duplication of form for each data provider. Google Apps Script Allows for extension of Google Sheets (e.g., custom UI) and deployment of stand-alone web applications. + Very customizable; full-featured; powerful.- Requires specialized expertise to set up and manage. Does not support offline data collection. Google AppSheet No-code app builder from Google. + Easier to learn than code-based solutions. Highly flexible.- Requires training to set up and maintain. Esri Web Application No-code app builder from Esri. See this example. + Customizable no-code framework for web app development.- Requires specialized expertise to set up and manage. MAX Survey Powerful survey platform that allows token-based access control (enables tracking of responses and personalized follow up emails). + Full-featured and themable.- Requires manual download of data; poor survey development experience. MAGE The Mobile Awareness GEOINT Environment is a platform for collecting mobile geospatial data in which custom forms can be created. + Easy to add new forms, good for geospatial data.- Requires support from the MAGE team for most customization or data export."},{"location":"pages/knowledge/base/digital-data-collection/#evaluating-options","title":"Evaluating options","text":"<p>The right solution may in fact be a combination of multiple softwares\u2013-when considered together, the suite of software representing the right solution must include these components:</p> <ol> <li>Development Environment: how will you build the data collection templates or forms? Is it easy to use and commonly understood or does it require special expertise?</li> <li>User Interface: how will the intended users interact with the form or template? Does it require a license or is it freely available? How user-friendly is the interface?</li> <li>Data Store: where will data be stored? Is the data store reliable, internally consistent, and accessible to those who need it?</li> <li>Dashboard/Visualization: how will data be aggregated, visualized, and otherwise made useful to those who need it?</li> </ol>"},{"location":"pages/knowledge/base/digital-data-collection/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>What constitutes a good software solution? We suggest the following are the most important criteria for evaluating potential solutions:</p> <ul> <li>Is it accessible to the Mission? The solution should be accessible to the Mission based on cost relative to funding available and considering Agency software approval requirements. </li> <li>Does it support the necessary features? The right solution will have all of the features required while limiting unnecessary bloat. See Recommended Features below. </li> <li>Is it easy to set up and adapt? Frequent changes to reporting requirements necessitate a solution that can be set up and adapted easily.</li> <li>How well does it integrate with existing workflows? The solution should integrate with existing workflows, including data collection, quality assurance and clearance, rather than require new workflows to the extent possible.</li> <li>Is it compatible with existing partner data collection methods? Ideally, the selected data collection method would work well with existing systems and avoid any duplication of efforts for partners. (see Box 1 below). Further, it must be within the bounds of what can be reasonably required of partners. </li> <li>Consistent with the long-term vision for Atlas? Will this solution be \u201cone more tool\u201d for staff and partners to learn or is it worth investing in because it can be reused for multiple purposes?</li> </ul> <p>Note</p> <p>A draft report by DDI/ITR reviewed the use of Digital Data Collection tools by partners and found \"the prevalence of organizational use of digital tools for M&amp;E data collection on projects varies significantly among partners: several report that all or \"close to all\" projects use digital data collection, while others note that less than half do.\" Further, the education and health sectors were found to commonly use the Digital Data Collection tools Tangerine and DHIS2. Others, including Feed the Future (FTF), are already utilizing DIS.</p>"},{"location":"pages/knowledge/base/digital-data-collection/#recommended-features","title":"Recommended Features","text":"<p>Software with these recommended features will provide a better user experience, increase flexibility, ensure data quality, and reduce data risk. While the preferred solution may not include all of these features, the more features available\u2013all other things being equal\u2013the better.</p>"},{"location":"pages/knowledge/base/digital-data-collection/#usability","title":"Usability","text":"<ul> <li>Conditional flow: can questions be skipped (or added) based on previous responses?</li> <li>Cascading dropdowns: can options within one select dropdown be informed by the selection from another (e.g., only show municipalities within the department selected)?</li> <li>Repeatable question groups: can respondents repeat a section multiple times?</li> <li>Editing: can the user edit (update/delete) previous entries?</li> <li>In-form validation (client-side): does the form validate user responses before submission?</li> <li>Post-submission validation (server-side): does the form validate user responses after submission (allows for more complex validation logic)?</li> <li>Maps: while not necessary for all applications, a tool with geographic data support might be preferred to one without.</li> </ul>"},{"location":"pages/knowledge/base/digital-data-collection/#data-formats-accepted","title":"Data formats accepted","text":"<ul> <li>Supports unstructured data: can both qualitative and quantitative data be entered?</li> <li>File upload: allows users to upload files (e.g., supporting documentation)?</li> <li>Batch upload: allows users to batch upload data rather than input one-by-one?</li> </ul>"},{"location":"pages/knowledge/base/digital-data-collection/#data-risk","title":"Data risk","text":"<ul> <li>Access control: can restrict access or require login?</li> <li>Partitioning: data are only available to stakeholders who should see it?</li> <li>Secure storage: data are stored securely with no risk of disclosing PII or SBU?</li> </ul>"},{"location":"pages/knowledge/base/digital-data-collection/#accessibility","title":"Accessibility","text":"<ul> <li>Language support: questions can be asked in English and local languages?</li> <li>ADA: Application is accessible to all in accordance with accessibility policy?</li> </ul> <p>A review of each digital data collection option relative to these features is available here.</p>"},{"location":"pages/knowledge/base/encoding/","title":"Encoding","text":"<p>UTF-8 or LATIN1 are common encodings, where LATIN1 is often used when working with Latin languages (e.g., Spanish, Italian). Google Sheets uses UTF-8, so go with that unless you specifically need LATIN1. </p> <p>Google Sheets and Tableau expect encoding UTF-8, data may not display correctly if encoded in another encoding.</p> <p>When importing data to a Google Sheet from another source (e.g., Excel), use the Import function (File &gt; Import) to add data rather than copy/paste. </p>"},{"location":"pages/knowledge/base/encoding/#set-encoding-for-mission-database","title":"Set encoding for Mission Database","text":"<p>To set UTF-8 as the database encoding with psql use the following command. This must be set for each session, as the default is the encoding of your operating system.</p> <pre><code>\\encoding UTF8\n</code></pre>"},{"location":"pages/knowledge/base/encoding/#determining-encoding","title":"Determining encoding","text":"<p>Encoding cannot be reliably determined just by looking at a file, unless the encoding is stored in the metadata. Instead, you may need to guess and check. You will often get errors refering to invalid byte sequences if you have the wrong encoding. </p>"},{"location":"pages/knowledge/base/encoding/#white-listing-text","title":"White listing text","text":"<p>White list text characters before importing if you continue to have encoding errors. In some rare cases, a string might contain a byte sequence that cannot be encoded with your chosen coding scheme and the character does not appear when you review the data in the data source.  </p> <p>To sanitize an <code>array_of_strings</code> in JavaScript, use this regular expression.</p> <pre><code>array_of_strings.map(ele =&gt; String(ele).replace(/[^\\p{Letter}\\p{Mark}\\p{Number}.,&lt;&gt;\"'\\[\\]\\|+@\\/#!\u00a1$%\\^&amp;\\*;:{}=\\-_`~()?\u00bf\\s]/gui, ''))\n</code></pre> <p><code>\\p{Letter}</code> is a character class representing all letters and <code>\\p{Mark}</code> represents the accents and other special characters (this is best practice as opposed to only selecting ASCII characters <code>[a-zA-Z]</code>).</p> <p><code>p{Number}</code> captures all numeric values. </p> <p>You can try <code>p{Punctuation}</code> for all punctuation, but it includes some punctuation that is non-standard (e.g., left and right single and double quotes). I've listed all punctuation on my keyboard instead, including upside down exclamation and question marks common in Spanish, and escaping characters that have special meaning in regex ( <code>\\[\\]\\|\\^\\*\\-*\\/</code> ). </p> <p><code>g</code> replaces all instances, rather than just the first; <code>u</code> allows for use of the Unicode character classes and <code>i</code> makes the regex case-insensitive.</p>"},{"location":"pages/knowledge/base/geospatial-data-processing/","title":"Geospatial Data Processing","text":"<p>Google Colab is used to process spatial data before use in Atlas. </p> <p>This notebook contains the Python code used to process the spatial data included in Atlas to date. </p>"},{"location":"pages/knowledge/base/google-apps-script/","title":"Learning Google Apps Script","text":"<p>To learn Google Apps Script, go to the Beginner's Guide, the New Apps Script Editor guide, the Fundamentals of Apps Script with Google Sheets codelab, the Extending Google Sheets page, javascript.info, Mozilla Developer Network and Clean Code JavaScript.</p>"},{"location":"pages/knowledge/base/h3/","title":"H3","text":"<p>H3 is a hexagonal hierarchical geospatial indexing system, developed by Uber.</p> <p>H3 has become popular in the geospatial community for indexing and aggregating spatial data at multiple spatial scales. H3 partitions the world into overlapping hexagonal grids. </p> <p>Atlas uses the h3-js library to generalize Activity Location Data at hexagon resolution six. Bindings are also available for PostgreSQL (H3-pg) and Python with pandas (h3pandas).</p> <p>This notebook extracts the h3 hexagons clipped to a country boundary for mapping in the Activity Location Data Portal (Public).</p>"},{"location":"pages/knowledge/base/iFrame/","title":"iFrames","text":"<p>iFrames can be used to share Google Apps Script web applications in various platforms including Google Sites and MyUSAID.</p> <p>To add an iFrame to MyUSAID, use the HTML code editor and input the following code:</p> <pre><code>&lt;iframe src=\"URL\" style=\"width:100%; height:600px\" title=\"Search\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"pages/knowledge/base/igce/","title":"Incorporating Atlas in the IGCE","text":"<p>Partners should set aside sufficient budget to meet requirements for data collection and sharing. The Mission should ensure that the requirements are clearly defined in solicitation and award language. When issuing the request for proposals, the Mission will develop an Independent Government Cost Estimate (IGCE) to estimate these costs.</p> <p>This guide supports the Mission in developing the IGCE for the functions of Monitoring, Evaluation Collaborating, Learning and Adapting (MECLA) to better evaluate proposals.</p> <p>Info</p> <p>To learn more about the types of data that partners collect, see here.</p>"},{"location":"pages/knowledge/base/igce/#introduction","title":"Introduction","text":"<p>There is no one-size fits all approach to Monitoring, Evaluation, Collaboration, Learning &amp; Adapting (MECLA). The resources required for MECLA will depend on the size of the award, the anticipated MECLA needs, and the desired approach to MECLA. USAID recommends that funding for MECLA should total four to thirteen percent, on average, of an OU\u2019s program funds, including one to three percent for evaluation and another three to ten percent for monitoring and CLA activities<sup>1</sup>. In budgeting for MECLA within Activities, each OU may compare current planned spending to these benchmarks to determine the adequacy of MECLA within and across Activities.</p>"},{"location":"pages/knowledge/base/igce/#process","title":"Process","text":"<ol> <li>Obtain a copy of the IGCE Template: The estimate for MECLA activities should be developed in a dedicated copy of the USAID IGCE Template (always use the latest template available on GLAAS). </li> <li>Compile the necessary supporting documentation: Supporting documentation may include (a) Activity Statement of Work Draft, (b) Requirements for the type of instrument for the Award, and (c) Estimates for Labor Costs (level of effort and labor rates), Other Direct Costs (number of trips, per diem days, workshops and their respective unit costs), and Other Costs (Indirects and Fee) for similar Activities completed in the past.</li> <li>Update draft Scope of Work: review the MECLA requirements and common tasks described in the section MECLA Tasks, below, to ensure MECLA is appropriately reflected in the draft Scope of Work.</li> <li>Estimate costs: considering the anticipated MECLA activities and cost considerations, estimate costs for each of the cost categories in the IGCE. Additional guidance is provided below.</li> </ol>"},{"location":"pages/knowledge/base/igce/#mecla-tasks","title":"MECLA Tasks","text":"<p>This section describes typical MECLA requirements and tasks to consider when estimating costs. Each Activity will be unique. Ensure that sufficient resources are budgeted to allow the Implementing Partner to comply with all USAID regulations as well as fulfill the important functions of MECLA during activity implementation.</p>"},{"location":"pages/knowledge/base/igce/#requirements","title":"Requirements","text":"<p>Table 1 describes the primary requirements for Activities related to MECLA. These requirements are defined in the Automated Directive System (ADS) and Mission Performance Management Plan (PMP). Ensure that these requirements are included in the award agreement, described in the draft Scope of Work, and budgeted for in the IGCE.</p> <p>Note that some MECLA tasks will be led by Mission staff or independent third-parties, including conducting Data Quality Assessments (DQAs; ADS 201.3.5.7) and collecting and utilizing beneficiary feedback (ADS 201.3.4.10, B). If using a third-party for these requirements, include budget in the IGCE under the Contractual/Consultants category.</p> <p>Table 1 | MECLA-related Requirements</p> Requirement Authority Develop Activity MECLA Plan &amp; Data Management Plan within 90 days of award. ADS 201.3.4.10, A Develop a monitoring approach and include one indicator per Activity outcome and integrate Mission performance indicators within new Activity designs where relevant. ADS 201.3.4.10, APMP Include indicator disaggregates for youth, sex, linguistic community (recommended for geography also). ADS 201.3.5.6, GPMP Develop a beneficiary feedback plan Monitor performance related to closing gaps and effects on Indigenous Peoples Establish targets and annual baselines for each Performance Indicator. ADS 201.3.4.10, A Submit PIRS within 3 months of data collection. ADS 201.3.5.6, D Collect Activity Location Data (see Box 1, below, for additional detail). ADS 579mab Analyze and use performance, risk, and context monitoring to learn, inform management and adapt interventions. ADS 201.3.4.10, B Report results in the annual Performance Plan and Report (PPR) and other reporting processes. ADS 201.3.5PMP Submit datasets to the Development Data Library (DDL) within 90 days of use for an Intellectual Work. ADS 579.3.3.2ADS 302mas Submit reports to the Development Experience Clearinghouse (DEC). ADS 540.3.2.3"},{"location":"pages/knowledge/base/igce/#common-tasks","title":"Common Tasks","text":"<p>This section describes, in general terms, common MECLA-related tasks. These tasks should be reflected in the draft Scope of Work as well as the IGCE. Consider how the Implementing Partner will complete these tasks when estimating costs for each cost category.</p> <p>Set Up &amp; Systems Maintenance: the Implementing Partner may need to hire or contract for MECLA roles; evaluate, purchase and set up hardware and software to facilitate MECLA, and maintain those teams and systems over the life of the contract. In addition, during the inception phase the activity\u2019s MECLA team will develop the Activity MECLA Plan and Data Management Plan (DMP), draft Performance Indicator Reference Sheets (PIRS), establish baselines and targets, and adapt the MECLA systems as needed. </p> <p>Data Collection &amp; Management: data collection and management is an important consideration for MECLA. Consider how the Implementing Partner will collect data given the geographic focus of the Activity, the location of implementation activities and intended beneficiaries, and the location of Implementing Partner offices. Note that data collection may be incidental to other implementation activities and thus the costs will be accounted for elsewhere. The Implementing Partner may need to develop data collection instruments and design a data management system. If the Activity includes research on human subjects, additional time should be allocated for review by an Institutional Review Board and develop informed consent forms. Collected data, especially data containing sensitive information and PII must also be protected and shared responsibly. Finally, internal data quality assessments should be completed regularly. </p> <p>Performance Management: Implementing Partners are required to analyze and use performance, risk, and context monitoring to learn, inform management and adapt interventions. Often this is done in concert with the A/COR and others on the Technical Team. Include tasks to analyze data, prepare reports, and participate in regular performance management meetings with the Technical Team. </p> <p>Reporting: USAID requires that collected data and intellectual works, such as reports, are submitted to the appropriate repositories. Performance data must be submitted to the Mission at the required reporting frequency. The minimum frequency is annual reporting, but often implementers are reporting more frequently. Thematic data[^2] must also be submitted to DDL within 90 days of its use (or fitness for use) in an intellectual work. Reports and other intellectual works must be submitted to the Development Experience Clearinghouse. Allow 1-2 days per dataset for submission to the DDL and DEC. </p> <p>Evaluation: Internal evaluations (conducted/commissioned by USAID or by the implementing partner) may be included to assess various aspects of the implementation of or outcomes from the Activity. Note that internal evaluations do not count towards the requirements for evaluations described in ADS 201.3.6. If an external evaluation (led by an expert external to USAID who has no fiduciary relationship with the implementing partner) is planned, also consider additional tasks or costs to the Implementing Partner to enable the independent evaluation. See Guidance Note: Developing an Independent Government Cost Estimate (IGCE) for a Performance Evaluation for assistance estimating costs for evaluations.</p> <p>Learning: Learning is an important component of MECLA however no specific requirements for Activities are declared in the ADS. Consider how the Activity may support learning relative to important learning questions for the Activity or for the Mission\u2019s CDCS. Also consider any participation by the Implementing Partner in learning events sponsored by the Mission or others.</p>"},{"location":"pages/knowledge/base/igce/#cost-considerations","title":"Cost Considerations","text":"<p>How much should MECLA-related activities cost? Ten percent of the total award is a reasonable benchmark for estimating MECLA costs for most Activities. Spending much more on MECLA-related tasks will unnecessarily reduce funds for implementation activities whereas spending less puts the Activity at risk of missing opportunities to optimize performance or creating unintended consequences.</p> <p>The following considerations may result in more or less than this benchmark for MECLA costs.</p> <ul> <li>Size of award: larger awards will spend more overall on MECLA than smaller awards.</li> <li>Type of award: acquisition awards may include more MECLA requirements than assistance awards.</li> <li>Type and complexity of data collection: Activities which need to collate hand-written data (such as from health clinics) or implement large surveys will require more resources than an activity that relies on data collection through participation lists, for instance.</li> <li>Number of subgrants: awards with many subgrants may require more resources for standardizing systems across multiple grantees.</li> <li>Number of field offices: more field offices may reduce the travel time to collect data.</li> <li>Proportion US-based staff: more resources are often required for US-based staff for salaries and travel.</li> <li>Use of contractors and consultants: using experienced contractors and consultants for MECLA and/or enumeration can reduce costs depending on the type of Activity.</li> <li>Travel required: travel to collect data and meet with Technical Teams must be considered.</li> <li>Technology used: digital technologies require more cost to set up and maintain but can be more efficient and accurate than transcribing data.</li> <li>Centralization of MECLA roles: the structure of the MECLA team, whether using dedicated staff or existing program staff for MECLA roles, may affect how costs are accounted for in the overall IGCE.</li> <li>Implementing Partner sophistication: Implementing Partners with experience working with USAID may require fewer resources to comply with MECLA requirements given their experience and existing staff.</li> <li>Number of indicators: the more indicators employed and disaggregates required will increase overall MECLA costs.</li> </ul>"},{"location":"pages/knowledge/base/igce/#cost-categories","title":"Cost Categories","text":"<p>The IGCE Template includes many cost categories. For MECLA, the most common cost categories to budget for include personnel, equipment, supplies, contractual, travel, and other direct costs.</p>"},{"location":"pages/knowledge/base/igce/#personnel","title":"Personnel","text":"<p>Labor is typically the largest expense to budget for with respect to MECLA. There are many options for composing the MECLA team. For large awards, a dedicated and centralized team of MECLA specialists may be best. For smaller awards, MECLA responsibilities may be taken on by other program staff. Typically, at least one MECLA specialist should be dedicated to managing and performing MECLA work. Review the draft Scope of Work to understand the needs for MECLA to help estimate the time required to complete MECLA tasks. Include sufficient time for non-MECLA specialists to support and participate in MECLA-related activities. Labor costs for personnel are specified as the number of units (months, days or hours) and the unit costs (rate). </p>"},{"location":"pages/knowledge/base/igce/#equipment","title":"Equipment","text":"<p>Vehicles are an expensive cost in the equipment category. If fulfilling MECLA requirements will require a dedicated vehicle or vehicles such as for consistent and remote data collection or monitoring,, include it here. See IGCE guidance for other costs to consider in this category.</p>"},{"location":"pages/knowledge/base/igce/#supplies","title":"Supplies","text":"<p>Supplies include computers, servers, office furniture, and other equipment required by the Implementing Partner. Include any costs associated directly with MECLA roles, such as laptops, here. See the Other Direct Costs section below for additional hardware considerations. </p>"},{"location":"pages/knowledge/base/igce/#contractual","title":"Contractual","text":"<p>Review the draft Scope of Work, which should now reflect MECLA requirements and common MECLA tasks, to estimate costs associated with contractual obligations for program activities, consultants, and subgrants. </p>"},{"location":"pages/knowledge/base/igce/#program-activities","title":"Program Activities","text":"<p>The draft Scope of Work should clearly describe expectations for program activities related to MECLA requirements and common tasks. For additional detail, see the MECLA Tasks section, above.</p>"},{"location":"pages/knowledge/base/igce/#consultants","title":"Consultants","text":"<p>When consultants are used to fulfill MECLA roles, include expected costs here. </p>"},{"location":"pages/knowledge/base/igce/#subgrants","title":"Subgrants","text":"<p>Subgrantees may also have roles and responsibilities related to MECLA. Include those costs here.</p>"},{"location":"pages/knowledge/base/igce/#travel","title":"Travel","text":"<p>Consider international travel for US-based staff, local travel for performance management meetings, and any travel for data collection not accounted for elsewhere.</p>"},{"location":"pages/knowledge/base/igce/#other-direct-costs","title":"Other Direct Costs","text":"<p>Software is the most common other direct cost to consider. Be sure to include hardware that meets software requirements in the Equipment category. Also consider costs associated with vehicles, such as fuel, insurance, repairs and maintenance, etc.</p> <p>Table 2 describes common software and hardware employed by MECLA teams for digital data collection, data management, analysis, and reporting. Include sufficient time in the personnel budget for setting up the system, training users, maintaining the technology and potentially upgrading technology for longer awards. Consider the entire package of software and hardware together. Some software providers package multiple functionalities which can be less expensive than purchasing each functionality separately. Importantly, lower-cost or free options often require more labor to set up and maintain.</p> <p>Table 2 | Common software &amp; hardware</p> Software/Hardware Cost Range Range for Estimates Geographic Information System (GIS)Supports collection and management of geospatial data. Examples include ArcGIS and QGIS. $0 - $500 /user /year.$2,000 - $2,500 per laptop. Budget 1-2 licenses per year. Use high end of range for more complex GIS needs. GIS software requires more powerful laptops than typical standard-issue laptops. Include one laptop per license. Data Visualization SoftwareSupports data visualization and reporting. Examples include Tableau and Google Data Studio. $0 - $900 /user /year. Budget 1-2 licenses per year. Performance Management SystemSupport data management, reporting and performance management. Examples include DevTech and AmpImact. $30,000 - $100,000 to set up plus $25,000 - $100,000 /year. Scale based on award size (low end for &lt;$5MM award, high end for &gt;$100MM award). See Database Management System for alternatives. Database Management SystemSupports data management. Examples include Microsoft Access, Postgres, and Oracle Database. $0 - $12,000 /year. Scale based on award size (low end for &lt;$5MM award, high end for &gt;$100MM award).Do not include if budgeting for a Performance Management System. Field Data Collection Platform Supports field staff to digitally collect data; includes software and mobile device. Software examples include Survey123 and KoboToolbox. Mobile devices range from smartphones to dedicated units (e.g., Trimble devices). $0 - $350 /user /year.$250 - $1,000 / mobile device. Budget 1 license per field enumerator and at least 1 license for the office-based MECLA team. Budget for 1 mobile data collection device per license. Consider whether the device requires satellite or cellular data collection for real-time data uploads. <ol> <li> <p>ADS 201 3.5.2 &amp; ADS 201 3.2.15\u00a0\u21a9</p> </li> <li> <p>Thematic data refers to all other data collected or used by IPs. Examples include but are not limited to scientific research, population-based surveys, and census data. ADS 579 USAID Development Data requires that all data collected or acquired by IPs is submitted to the Development Data Library (DDL).\u00a0\u21a9</p> </li> </ol>"},{"location":"pages/knowledge/base/importrange/","title":"IMPORTRANGE","text":"<p>Google Sheet's <code>IMPORTRANGE</code> function allows for easy transfer of data between Google Sheets.</p> <p>Warning</p> <p>Anyone with edit access to a Google Sheet with an <code>IMPORTRANGE</code> function can view the Google Sheet from which it is importing data. To avoid accidental leaks of sensitive data, best practice is to stage data in an intermediate spreadsheet.</p> <p>Tip</p> <p>You cannot use <code>IMPORTRANGE</code> on an Excel (<code>.xlsx</code>) file stored in Google Drive.  </p>"},{"location":"pages/knowledge/base/importrange/#set-up-the-export","title":"Set up the export","text":"<p>Before you import data, it is best practice to set up the export. This will protect the import from breaking due to changes in the source Sheet.</p> <ol> <li>Create a new sheet in the Google Sheet named <code>export</code> (or similar).</li> <li> <p>For each column you want to export, use an array formula to copy the column to the <code>export</code> sheet. For example, in cell A1 copy/paste the below formula to copy column <code>A</code> from <code>data</code> to column <code>A</code> in the <code>export</code> sheet. </p> <pre><code> ={data!A:A}\n</code></pre> </li> <li> <p>Repeat for all columns from the source data sheet that you want to export.</p> </li> </ol>"},{"location":"pages/knowledge/base/importrange/#import-data","title":"Import data","text":"<p>In the new sheet, use the <code>IMPORTRANGE</code> function to import the data.</p> <ol> <li> <p>Copy the sheet ID from the source data sheet. You can find the sheet ID in the url between <code>/d/</code> and <code>/edit</code> :</p> <pre><code>https://docs.google.com/spreadsheets/d/{spreadsheet_ID}/edit\n</code></pre> </li> <li> <p>In cell A1 of the sheet you want to import data into, use the <code>IMPORTRANGE</code> function, updating the spreadsheet ID, sheet name and range.</p> <pre><code>=IMPORTRANGE(\"spreadsheet ID\", \"export!A:A\")\n</code></pre> </li> <li> <p>Authorize access to the data source. You only need to do this on the first time. Click the cell A1 and select \"Allow Access\".</p> </li> </ol> <p> </p>"},{"location":"pages/knowledge/base/importrange/#using-a-query","title":"Using a Query","text":"<p>You can combine <code>IMPORTRANGE</code> with <code>QUERY</code> to filter the imported data upon import. This can be useful for filtering out sensitive or irrelevant data.</p> <pre><code>=QUERY(IMPORTRANGE(\"sheet ID\", \"export!A:A\"), \"SELECT * WHERE Col1 = 'foobar'\")\n</code></pre> <p>Note that you reference columns by their number (starting with 1) and that strings should be enclosed in single quotes <code>''</code>. See here for further details on the query language.</p>"},{"location":"pages/knowledge/base/naming-conventions/","title":"Naming Conventions","text":"<p>Use these naming conventions to promote consistency and reduce the opportunity for errors.</p> <ul> <li>Use Title Case for filenames, tab names and column names.</li> <li>Do not store units in column names, use the DEFINITIONS table.</li> <li>Do not start a column name with a number.</li> </ul>"},{"location":"pages/knowledge/base/naming-conventions/#considerations-for-optional-postgresql-users","title":"Considerations for optional PostgreSQL users","text":"<ul> <li>Tab names in Google Sheets are best if formatted in a valid way to be a Postgres table. Any spaces or hyphens will be converted to an underscore. Any other punctuation will be removed (although rare characters can still result in an error).</li> <li>Column names in Google Sheets are best if formatted in a valid way to be a Postgres column. Any spaces, hyphens or forward and backward slashes will be converted to underscores. Parentheses will be removed.</li> <li>Column names longer than 63 characters will be truncated.</li> <li>Do not use a numeric format that includes a comma in the data.</li> </ul>"},{"location":"pages/knowledge/base/null-values/","title":"Null Values","text":"<p>When data are missing or invalid, a null value is used to represent the data. This is preferable to a blank cell, which is ambiguous. </p> <p>To read data from Google Sheets into the Mission Database, all null values should be represented by the signifier <code>#N/A</code>. Blank cells in non-text columns will result in a failure to load the data into the table. For text cells, blanks are allowed but not preferred.</p>"},{"location":"pages/knowledge/base/null-values/#how-to-convert-blanks-in-google-sheets-to-null-values","title":"How to convert blanks in Google Sheets to null values","text":"<p>To convert all blank cells in a range within a Google Sheet:</p> <ol> <li>Select the range and then use <code>Ctrl + F</code> to open the Find box.</li> <li>Click the three dot menu to open the Find/Replace dialog box,</li> <li>Use <code>^\\s*$</code> to represent blank values in the Find box and check Search using regular expressions.</li> <li>Input <code>#N/A</code> in the Replace with box.</li> <li>Click Replace all.</li> </ol> <p></p>"},{"location":"pages/knowledge/base/null-values/#how-null-values-are-read-into-the-mission-database","title":"How null values are read into the Mission Database","text":"<p>The Mission Database uses <code>psql</code> to upload data from Google Sheets using the <code>COPY FROM PROGRAM</code> command. In the Data Steward Admin Tool, the null values are specified in <code>utils.gs</code>: <code>write_sql_copy_table</code> function.  <code>COPY FROM PROGRAM</code> can only accept a single null value. Advanced users may alter the data pipelines if requiring more than one null value.</p>"},{"location":"pages/knowledge/base/pgAdmin/","title":"pgAdmin","text":"<p>pgAdmin is a client application for PostgreSQL, providing a GUI to create and manage databases.</p>"},{"location":"pages/knowledge/base/pgAdmin/#installation","title":"Installation","text":"<p>pgAdmin is installed by default when installing PosgreSQL. If you elected not to install pgAdmin when installing PostgreSQL, follow the instructions below.</p> <p>Download the pgAdmin installer for your operating system. Click the <code>.exe</code> file to install.</p> Installation Screens <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"pages/knowledge/base/pgAdmin/#getting-started","title":"Getting Started","text":"<ol> <li>Open pgAdmin. </li> <li>When pgAdmin first opens, you may be prompted to set a password. Write this down and don't lose it!</li> <li>At first, you should see one server in the Browser tree (left sidebar). This server was created when you installed Postgres along with a database named <code>postgres</code>. Click the server to select. You will be prompted for the password. Provide the superuser password you created when you installed Postgres.</li> <li>To create a new database, drill down into the Browser tree and select the Databases (1) database cluster. In the Object menu, select Create &gt; Database. A dialog box will appear. Provide a name and optionally, comment, and click Save.</li> </ol> <p>You can now use pgAdmin to manipulate the database through its graphical user interface. You can also use either the Query Tool to write SQL statements or the PSQL Tool to use psql. If you receive the error Please configure the PostgreSQL Binary Path in the Preferences dialog, you must you must register the binary path for psql. </p> <p>Tip</p> <p>At first, pgAdmin can seem overwhelmingly complex. It's useful to understand the Postgres client/server model. pgAdmin is the client application; a server process, named <code>postgres</code>, manages operations on the database.  A single server can manage multiple databases, referred to as a database cluster.</p> <p>Click the help button for help with any dialog box. </p>"},{"location":"pages/knowledge/base/pgAdmin/#common-workflows","title":"Common Workflows","text":""},{"location":"pages/knowledge/base/pgAdmin/#create-a-table","title":"Create a table","text":"<p>To create a new table, expand the Browser tree for the database to select Schemas &gt; public. In the Object menu, select Create &gt; Table (or right-click the Table item in the Browser tree). A dialog box will appear. Provide a name for the table under the General tab and the desired columns in the column tab. </p>"},{"location":"pages/knowledge/base/pgAdmin/#import-csv","title":"Import CSV","text":"<p>pgAdmin provides a dialog utility for importing data from CSV. First create a table with the correct columns. Then, in the Browser tree, navigate to the table, right-click, and select Import/Export. If you receive the error Please configure the PostgreSQL Binary Path in the Preferences dialog, you must register the binary path for psql. </p>"},{"location":"pages/knowledge/base/pgAdmin/#import-csv-from-web","title":"Import CSV from web","text":"<p>You can read a file directly from the web, avoiding the need to download the file locally. See instructions in psql.</p>"},{"location":"pages/knowledge/base/pgAdmin/#auto-incrementing-id-field","title":"Auto-incrementing ID field","text":"<ol> <li>If the table already exists, open the table properties dialog box. If not, create a table.</li> <li>Edit the column by selecting the pencil icon.</li> <li>In the edit field dialog box, select the Constraints tab (not the Constraints tab at the top of the dialog box).</li> <li>Change the \"Type\" to \"Identity\" and change the \"Identity\" to \"ALWAYS\". The remaining fields will be filled automatically after you save, but you can edit them now if desired.</li> <li>Click Save.</li> </ol>"},{"location":"pages/knowledge/base/pgAdmin/#writing-and-saving-sql-queries","title":"Writing and saving SQL queries","text":"<p>A common workflow when developing SQL queries is to write the query in the Query Tool (<code>Alt</code> + <code>Shift</code> + <code>Q</code>) to draft the query and, once satisfied with the result, save the query to a <code>.sql</code> file. Store these files in a <code>sql/</code> folder in the project repository. You can load the <code>.sql</code> file in the Query Tool for future use. For very common queries, you should instead create a view.</p>"},{"location":"pages/knowledge/base/pgAdmin/#register-the-binary-path-for-psql","title":"Register the binary path for psql","text":"<p>If you receive the error Please configure the PostgreSQL Binary Path in the Preferences dialog when using pgAdmin, you need to register the binary path for <code>psql</code> for the version of Postgres you are using.</p> <ol> <li>Open File &gt; Preferences. </li> <li>Select Binary paths in the left sidebar. </li> <li>In the PostreSQL Binary Path table, identify the Database Server associated with your version of PostgreSQL and enter the directory in which the <code>psql</code>, <code>pg_dump</code>, <code>pg_dumpall</code>, and <code>pg_restore</code> utilities can be found for the corresponding database server version. If you have already registered [[psql]] to your PATH variable, you can use <code>which psql</code> in a [[Bash]] session. For me, the correct path is</li> </ol> <pre><code>C:\\Progarm Files\\PostgreSQL\\15\\bin\n</code></pre> <p>Select this version of PostgreSQL as the default.</p> <p></p>"},{"location":"pages/knowledge/base/postgres/","title":"PostgreSQL","text":"<p>PostgreSQL is an open source relational database management system (RDBMS), often referred to as Postgres.</p> <p>See Installation for guidance on installing PostgreSQL.</p> Additional Resources <ul> <li>Amigoscode YT series on PostgreSQL</li> <li>PostgresSQL Tutorials</li> </ul>"},{"location":"pages/knowledge/base/psql/","title":"psql","text":"<p>psql is a command line tool for Postgres databases. <code>psql</code> is installed by default with PostgreSQL (unless specifically excluded during the installation process). <code>psql</code> can also be accessed from pgAdmin using the PSQL Tool.</p>"},{"location":"pages/knowledge/base/psql/#getting-started","title":"Getting Started","text":"<p>These instructions describe how to use <code>psql</code> from the command line for advanced users. You can also access <code>psql</code> using the PSQL Tool in pgAdmin, however instructions will differ.</p> <ol> <li>Add psql to your PATH variable, follow the instructions here.</li> <li>Open Bash from the project directory or the SQL Shell (psql), installed with Postgres.</li> <li> <p>Log in to the <code>postgres</code> account. </p> <pre><code>psql -U postgres\n</code></pre> </li> <li> <p>You will be prompted for the password. Note that when typing in the password you may not see any characters appear. Input the password you set when installing Postgres.</p> <pre><code>&gt;&gt;&gt;Password for user postgres:\n</code></pre> </li> <li> <p>Create a new database. If successful, the prompt will print <code>CREATE DATABASE</code>. You can also use pgAdmin to view the database.</p> <pre><code>CREATE DATABASE &lt;databasename&gt;;\n</code></pre> </li> <li> <p>Connect to the new database ( <code>\\c &lt;databasename&gt;</code> is also valid).  </p> <pre><code>\\connect &lt;databasename&gt;\n</code></pre> </li> </ol> <p>See SQL for details on how to use <code>psql</code> to add tables, insert data, create views, and read from and write to CSV. </p> <p>Quit the interactive SQL session. (This is not required if using the PSQL Tool in pgAdmin).</p> <pre><code>\\q\n</code></pre>"},{"location":"pages/knowledge/base/psql/#common-workflows","title":"Common workflows","text":""},{"location":"pages/knowledge/base/psql/#connect-to-a-database","title":"Connect to a database","text":"<p>When coming back to a database project, login and specify the database at the same time. You will be prompted for the password each time.</p> <pre><code>psql -U postgres -d &lt;databasename&gt;\n</code></pre>"},{"location":"pages/knowledge/base/psql/#read-from-csv","title":"Read from CSV","text":"<p>Often, your data will initially be stored in a CSV. Before reading from a CSV, you must [create a table](SQL.md#create a table] with the corresponding rows. PostgreSQL provides a helper COPY command for reading from  <code>.csv</code> files.  The  will append the data to whatever is in the table already.  </p> <p>Specify the fields if your CSV does not contain every column in your database table; those fields not specified will receive their default value. Most commonly, specify fields when  your database has an auto-incrementing <code>id</code> field and your CSV does not have a unique identifier. </p> <pre><code>COPY &lt;tablename&gt;\n(&lt;field_1&gt;, &lt;field_2&gt;)\nFROM &lt;full/path/to/file&gt; \nDELIMITER \u2018,\u2019\nCSV Header;\n</code></pre> <p>Alternatively, use a scripting language to create a <code>.sql</code> file that creates the table and inserts each record one at a time. The file should look like this:</p> <pre><code>CREATE TABLE &lt;tablename&gt; (\n    &lt;field_1&gt; &lt;field_1_type&gt;,\n    &lt;field_2&gt; &lt;field_2_type&gt;\n);\n\nINSERT INTO &lt;tablename&gt; (&lt;field_1&gt;, &lt;field_2&gt;) VALUES (&lt;field_1_val&gt;, &lt;field_2_val&gt;);\nINSERT INTO &lt;tablename&gt; (&lt;field_1&gt;, &lt;field_2&gt;) VALUES (&lt;field_1_val&gt;, &lt;field_2_val&gt;);\nINSERT INTO &lt;tablename&gt; (&lt;field_1&gt;, &lt;field_2&gt;) VALUES (&lt;field_1_val&gt;, &lt;field_2_val&gt;);\n...\n</code></pre> <p>Save the file and copy the full path to the file. Open a <code>psql</code> session and use <code>\\i</code> which executes commands from a file. <code>\\ir</code> allows you to use the relative path.</p> <pre><code>\\i &lt;full/path/to/file&gt;\n</code></pre>"},{"location":"pages/knowledge/base/psql/#import-csv-from-web","title":"Import CSV from web","text":"<p>You can read a file directly from the web using <code>curl</code>, avoiding the need to download the file locally. Use quotes around the URL. </p> <pre><code>COPY &lt;tablename&gt;\nFROM PROGRAM 'curl -L \"&lt;url&gt;\"'\nHEADER CSV DELIMITER ',';\n</code></pre> <p>To test the download link, open Bash and use the same command <code>curl -L \"&lt;url&gt;\"</code>, replacing <code>&lt;url&gt;</code> with the URL to your data; you should see the contents printed in the terminal in comma-separated rows. </p> <p>You can use Google Sheets to serve data in this way. In fact, you can read any data that can be accessed programmatically from standard output of your terminal. See this blog post for more detail.</p>"},{"location":"pages/knowledge/base/psql/#export-to-csv","title":"Export to CSV","text":"<p>To export to query, you can provide a table, view, or query to the COPY command.</p> <pre><code>COPY (&lt;tablename/view/query&gt;) \nTO 'path/to/file.csv' \nDELIMTER ',' \nCSV HEADER;\n</code></pre>"},{"location":"pages/knowledge/base/psql/#tips","title":"Tips","text":"<p>Use <code>\\x</code> to toggle expand display and make results easier to read.</p>"},{"location":"pages/knowledge/base/psql/#common-psql-commands","title":"Common psql commands","text":"psql command result <code>\\?</code> get help <code>\\q</code> quit <code>\\l</code> list databases <code>\\d</code> list tables <code>\\d &lt;table&gt;</code> list fields in table <code>\\i</code> execute commands from a file <code>\\ir</code> execute commands from file, relative path <code>\\x</code> toggle expand display <code>\\copy</code>, <code>COPY</code> copy from/to CSV <code>\\! clear</code> clear prompt <code>DROP TABLE &lt;table&gt;</code> drop a table"},{"location":"pages/knowledge/base/raster2pgsql/","title":"raster2pgsql","text":"<p>Advanced users may choose to load rasters using the <code>raster2pgsql</code> command line tool. </p> <p>As with shapefiles, raster files should be processed and stored in the Thematic Database before being read into the Mission Database (see here for details). Download the <code>.tif</code> file(s) from the Spatial directory in the Thematic Database. </p>"},{"location":"pages/knowledge/base/raster2pgsql/#usage","title":"Usage","text":"<p>In this example, a raster located at <code>data/spatial/population.tif</code> is saved to a new <code>population_grid</code> table in the database.</p> <pre><code>raster2pgsql -s 4326 -I -M -C data/spatial/population.tif -F public.population_grid &gt; sql/load_population_grid.sql\n</code></pre> <p>Load the file into the Postgres database with psql.</p> <pre><code>psql -U postgres -d missiondb -f sql/load_population_grid.sql\n</code></pre>"},{"location":"pages/knowledge/base/set-up-google-account/","title":"How to Set Up a Google Account","text":"<p>These instructions will help partners set up a Google Account for their work email. If the partner cannot set up a Google Account due to IT restrictions, they will need to work with the Data Steward to set up an alternative process for sharing data.</p> <ul> <li> <p>Open Google.com and click the Sign In button at the top right of your screen. If you are already signed in, log out first.</p> </li> <li> <p>Click the Use another account link to begin setting up a new account.</p> </li> <li> <p>Click the Create account link.</p> </li> <li> <p>Fill in your first and last name, birthdate and any other fields until you arrive at the username screen. The screen may look like this:</p> </li> </ul> <p></p> <ul> <li> <p>Under the \"Username\" box is a link that says \"Use your existing email\" which, when clicked, will allow you to use your work-issued email address. Click the link.</p> </li> <li> <p>Enter your work-issued email address in the box. Make sure you have appropriate permissions from your workplace before continuing.</p> </li> <li> <p>Click Next and finish filling the rest of the form.</p> </li> </ul>"},{"location":"pages/knowledge/base/shp2pgsql/","title":"shp2pgsql","text":"<p><code>shp2pgsql</code> is a command line tool for importing shapefiles into a PostgreSQL database. See the cheatsheet here.</p> <p>Shapefiles should be processed and stored in the Thematic Database before being read into the Mission Database (see here for details).</p>"},{"location":"pages/knowledge/base/shp2pgsql/#usage","title":"Usage","text":"<p>In this example, a shapefile located at <code>data/spatial/departments.shp</code> is saved to a new <code>departments</code> table in the database.</p> <pre><code>shp2pgsql -c -D -s 4326 -I -W UTF-8 data/spatial/departments.shp public.departments &gt; sql/load_shp_departments.sql\n</code></pre> <p>Load the file into the Postgres database with psql.</p> <pre><code>psql -U postgres -d missiondb -f sql/load_shp_departments.sql\n</code></pre>"},{"location":"pages/knowledge/base/staging-data/","title":"Staging Data","text":"<p>Staging data in an intermediate Google Sheet is best practice when piping data from a source with sensitive data to a source that will have public view access. This prevents bad actors from accessing data by manipulating an <code>IMPORTRANGE</code> function should they get edit access to the sheet.</p> <ol> <li>Create a new Google Sheet to stage the data</li> <li>Use a combination of <code>QUERY</code> and <code>IMPORTRANGE</code> to import only non-sensitive data from the source sheet (see guidance here).</li> <li>Use <code>IMPORTRANGE</code> from the new Google Sheet to the public destination sheet.</li> </ol> <p>For added protection, remove all share permissions from the staged data except for the Data Steward.</p>"},{"location":"pages/knowledge/base/stakeholder-engagement/","title":"Stakeholder Engagement Plan","text":"<p>Developing a solution does not guarantee that intended users will use the solution. Both primary users and stakeholders (secondary users) must be engaged to promote awareness of the product, learn how to use the product, and tailor the product to meet their needs.</p> <p>This template Stakeholder Engagement Plan will help you plan for the roll out of a new tool or product.</p>"},{"location":"pages/knowledge/base/stakeholder-engagement/#overview","title":"Overview","text":"<p>Provide a high level description of the plan to keep users engaged.</p>"},{"location":"pages/knowledge/base/stakeholder-engagement/#communication-plan","title":"Communication Plan","text":"<p>Describe how you will make users aware of the tool. Consider using multiple communication channels and audience-specific tactics. The table below provides a framework for quickly building a communication strategy.</p>"},{"location":"pages/knowledge/base/stakeholder-engagement/#communication-plan-table","title":"COMMUNICATION PLAN TABLE","text":"Objective Audience Tactic Lead Timeframe Promote awareness of the product Technical Office Directors Email blast Erik Anderson Week of final production"},{"location":"pages/knowledge/base/stakeholder-engagement/#dissemination-plan","title":"Dissemination Plan","text":"<p>Describe where the tool will be made available to users and how users will receive the tool directly. If relevant, describe how users will access updates to the tool.</p>"},{"location":"pages/knowledge/base/stakeholder-engagement/#adoption-plan","title":"Adoption Plan","text":"<p>Adoption occurs after users are onboarded, increase engagement, and realize sufficient benefits from using the tool to fully adopt it. Onboarding is the first step towards adoption. Engagement entails more substantial or more frequent use of the tool. Adoption describes when the user is using the tool for most if not all of the intended use cases (i.e., not using other tools or workarounds).</p> <p>The table below will help you walk through this process and test where users might drop off from the adoption curve. Guidance for each section is provided below.</p> <ul> <li>Desired behavior: Describe what you want your user to do at each phase of adoption. Make sure it is an observable behavior.</li> <li>Timeframe: Describe how long the user will be in each adoption phase. For example, behaviors associated with engagement might happen after one month of using the tool.</li> <li>Test: Describe how you can test whether users are or are not exhibiting the desired behavior.</li> <li>Metrics: Describe the indicators that will be measured to provide evidence of whether, and to what extent, users are exhibiting the desired behavior. For example, a web app may measure number of site visits.</li> <li>Red flags: Describe what to watch out for that might indicate users are falling off of the adoption curve at each phase.</li> <li>Incentives: Describe any strategies for increasing adoption or mitigating red flags.</li> </ul>"},{"location":"pages/knowledge/base/stakeholder-engagement/#adoption-table","title":"ADOPTION TABLE","text":"Onboarding Engagement Adoption Desired behavior Timeframe Tests Metrics Red flags Incentives"},{"location":"pages/knowledge/base/trackers/","title":"Trackers","text":"<p>Trackers are a necessary part of how we work. When a new information need arises, we often develop a new tracker to meet this need. This guidance will help you develop a tracker that meets your information needs using best practices that will save time and create a trustworthy data source.</p> <p>Not all trackers require the same level of complexity. This guide provides best practices for basic, intermediate and advanced trackers.</p> <ul> <li>Basic: if you are developing a simple tracker for short-term use within a small team, use best practices for basic trackers.</li> <li>Intermediate: intermediate level trackers are intended for long-term use and/or to request data from other teams.</li> <li>Advanced: an advanced tracker will be used by many users for many years because it fulfills a central information need for the Mission. </li> </ul>"},{"location":"pages/knowledge/base/trackers/#basic","title":"Basic","text":"<p>Basic trackers are intended for short-term use and rarely for external data collection. These should be quick to build once you know your information needs. A few best practices include:</p> <p>Use one data type per column: each column should contain only one data type. For example, if you are collecting a date, only allow dates in this column. Don\u2019t allow a date range or a note about how dates don\u2019t make sense for this record. Use data validation rules to reject incorrect data types.</p> <p>Tip</p> <p>Use dropdown validation whenever possible.</p> <p>Use separators that can be split: In some cases, you may need to put multiple values in a single cell (e.g., a list of relevant geographies). You should be able to use a <code>SPLIT</code> function to split the cell into multiple cells, each with one entry, by using a separator such as a pipe (\u201c|\u201d). </p> <p>Use existing standards for common data types: if you\u2019re collecting data on a common data type, use appropriate Mission Data Standards for that field. </p> <p>Use consistent #N/A values: use a consistent #N/A value. The default choice for Google Sheets is \u201c#N/A\u201d. It\u2019s best practice to not leave any fields blank.</p> <p>Name all tabs: provide a clear concise name for each tab. Never leave a tab with the default tab names. Follow the Atlas naming conventions.</p> <p>Delete unused tabs: delete any tabs that are not used.</p> <p>Delete unused columns: delete any blank columns to the right of your dataset.</p>"},{"location":"pages/knowledge/base/trackers/#intermediate","title":"Intermediate","text":"<p>Intermediate trackers should follow all of the best practices for Basic trackers and these additional best practices which will promote interoperability with other Mission systems and reduce human automation.</p> <p>Import from related systems: instead of manually copying and pasting a column from another tracker, use the <code>IMPORTRANGE</code> function to automatically pull data. This will ensure that your data does not become stale. Be careful, the imported data will change when it changes in the data source, and your tracker must be able to adjust. Use a dropdown field to select from the imported data.</p> <p>Create a push query: to share data with other trackers, create one or more <code>export</code> sheets that others can use to import data from your tracker. Data should meet the criteria for Tidy data at least, and ideally for Normal data as well.</p> <p>Use Google Sheet naming conventions: use these naming conventions for sheet names and column names so that the tracker can be read into the Mission Database.</p> <p>Note</p> <p>Human automation is when humans spend time doing work that a computer could do, such as manually transferring data from one source to the next. We\u2019re busy enough already! Let\u2019s let computers do the boring stuff. Did you know that the cost of human automation increases exponentially with each new tracker that stores the same information as another tracker?</p>"},{"location":"pages/knowledge/base/trackers/#advanced","title":"Advanced","text":"<p>Trackers intended for long-term use or use by many stakeholders should be well designed, have user friendly interfaces, and be integrated in well-defined business processes. </p> <p>Split entry, storage and visualization: advanced trackers often need to support easy data entry, efficient storage, and provide tables and charts to visualize data in different ways. It\u2019s very difficult to do all of these things well in one place. For advanced trackers, consider using a Google form for data input or developing custom user interfaces using Google Apps Script.</p> <p>Keep it focused: advanced trackers can grow quickly. Do your best to keep the number of tabs limited and keep the tracker focused on its intended purpose and audience.</p> <p>Back it up: regularly back up the data by downloading a copy of the file or using a scheduled Google Apps Script. Don\u2019t back the data up into separate tabs in the same Worksheet.</p> <p>Support users: with many users, you should provide regular training, listen for feedback, and continuously improve the tracker.</p>"},{"location":"pages/knowledge/base/versioning/","title":"Versioning","text":"<p>All templates that are distributed by copy are versioned. This will help you keep track of who is using which version and maintain compatibility with any versions still in use. Update version numbers and capture changes in the CHANGELOG using the best practices as discussed at Keep a Changelog.</p>"},{"location":"pages/knowledge/playbook/","title":"Playbook","text":"<p>The Playbook describes approaches to develop the culture and capacity of the Mission required to empower teams with a data-informed, geographic approach to development. It serves as both a record of steps taken and ideas for rolling out new initiatives.</p> <p>Note that for some links access may be restricted to the Mission or Agency workforce.</p>"},{"location":"pages/knowledge/playbook/additional-engagement/","title":"Additional Engagement","text":"<p>To support the successful implementation of Atlas, consider these additional engagement options.</p>"},{"location":"pages/knowledge/playbook/additional-engagement/#myusaid-page","title":"MyUSAID page","text":"<p>Create a page on MyUSAID to describe Atlas and share links to key components, such as the Activity Database.</p>"},{"location":"pages/knowledge/playbook/additional-engagement/#data-hub","title":"Data Hub","text":"<p>Develop a Google Site to share products and partner resources like the Guatemala Data Hub. Be sure to secure approvals from the Web Governance Board first (internal link).</p>"},{"location":"pages/knowledge/playbook/additional-engagement/#customer-service-approach","title":"Customer service approach","text":"<p>Finally, expect interest in a data-informed, geographic approach to development to grow over time. Use a customer service approach to meet the needs of Mission staff. By responding to requests, developing high-quality examples, and supporting existing champions within the Mission, support for the program will grow organically. Use the Stakeholder Engagement Plan template to help roll out new products.</p>"},{"location":"pages/knowledge/playbook/continuous-improvement/","title":"Continuous Improvement","text":"<p>A continual improvement cycle ensures that Atlas is constantly improving and integrating user feedback. Follow this simple process to set up a continual improvement cycle.</p> <ol> <li> <p>During the year, track feedback on Atlas. </p> </li> <li> <p>Once a year, review all feedback and synthesize findings. </p> </li> <li>From the findings, identify recommendations. </li> <li>Work with the appropriate stakeholders to prioritize recommended improvements.</li> <li>Implement improvements.</li> </ol> <p>Be sure to follow versioning practices and capture changes in the CHANGELOG.</p>"},{"location":"pages/knowledge/playbook/enabling-conditions/","title":"Enabling Conditions","text":"<p>The level of effort to create the enabling conditions for a data-informed, geographic approach to development will depend on many factors. If the Mission is in compliance with all requirements of ADS 579 and using existing Agency systems like the DIS, many of the enabling conditions are likely already in place. </p>"},{"location":"pages/knowledge/playbook/enabling-conditions/#update-mission-orders","title":"Update mission orders","text":"<p>Mission orders set the policy for the Mission and provide essential guidance for Mission staff to do their jobs. Mission orders should include language that reflects best practices and requires the engagement of the Data Steward at key points. For example, the Activity Design Mission order was updated to require consultation with the Data Steward early during activity design to ensure existing datasets are utilized and that partners budget sufficient resources to report and use data during activity implementation.</p>"},{"location":"pages/knowledge/playbook/enabling-conditions/#draft-solicitation-language","title":"Draft solicitation language","text":"<p>When activity design teams prepare solicitations, the solicitation language must sufficiently describe the requirements for data collection and sharing that respondents can plan and budget for the tasks required. The respondent\u2019s plan and capacity for meeting data-related requirements may also be considered as criteria during technical evaluation of proposals. Standard language was drafted to cover all data sharing requirements, including for Activity Location Data, Performance Monitoring, and Thematic Data. This language is an adaptation of the standard language provided by the GeoCenter. Solicitation language should be reviewed by OAA.</p> <p>The sections of the solicitation language template that were updated include:</p> <ul> <li>Section C - Scope of Work: Clearly describe the requirements for including MECLA and other data collection/submission requirements</li> <li>Section L - Instructions: provide instructions on how respondents should indicate their plan to meet the requirements of Section C.</li> <li>Section M - Evaluation Criteria: describe how the MECLA plan and other data management responses will be evaluated.</li> <li>Section F - Reports: requirements for reporting and submitting data.</li> </ul> <p>The solicitation language must also be carried forward into award language.</p>"},{"location":"pages/knowledge/playbook/enabling-conditions/#define-data-standards","title":"Define data standards","text":"<p>Data Standards are used to standardize data collection methods, the formats of submitted data, the data attributes collected, the coding of specific attributes and other aspects. Clarifying data standards up front will reduce the effort of partners during data submission and ensure that data can be aggregated for Mission-wide analysis. ADS 579saa describes the data collection standards for geographic data and the DDL User Guide provides more general guidance. </p> <p>In a Mission, data standards should be developed with input from all offices because they must work for everyone. To ensure proper governance, an owner is assigned to each data standard. The data standard owner must approve any changes to their data standard.</p> <p>Data standards often include both schema and vocabulary.</p> <ul> <li>Schema: describes the attributes that must be collected for each type of data collection.</li> <li>Vocabulary: describes the standard codes to be used for each attribute. </li> </ul> <p>For example, \u201cBeneficiary Type\u201d is a standard attribute and \u201cWomen\u201d is a standard code. The attributes of the schema and each code in the vocabulary should be defined clearly. </p> <p>These principles can be used to inform development of the data standards: </p> <ol> <li>Link schema and vocabulary to teams so there is accountability and ownership</li> <li>Vocabulary for any attribute should be mutually exclusive</li> <li>Link to existing schemas as much as possible (e.g., the Standardized Program Structure and Definitions)</li> <li>Use nested hierarchies to allow for additional specificity when needed (e.g., exact age, 5-year age bands, and youth/adult would be hierarchical and aggregable)</li> <li>Don\u2019t have so many categories that it is overwhelming or impossible to find the right one.</li> </ol> <p>Data assets submitted should include data documentation (e.g., metadata) that define the schema and vocabulary of each dataset. </p>"},{"location":"pages/knowledge/playbook/enabling-conditions/#create-partner-resources","title":"Create partner resources","text":"<p>Partner resources, including guidance and standard templates, can help partners create, use, and contribute data to support evidence-based decision-making. Partner resources for Guatemala are distributed in the Data Hub's Partner Resources page. A few key resources that were developed for Guatemala are described below.</p>"},{"location":"pages/knowledge/playbook/enabling-conditions/#activity-monitoring-evaluation-learning-plan-amelp","title":"Activity Monitoring, Evaluation &amp; Learning Plan (AMELP)","text":"<p>The AMELP describes the expected monitoring, evaluation, collaborating, learning, and adapting (MECLA) efforts of an activity over a specified period of time, and focuses on whether an activity is achieving programmatic results and generating learning to inform its adaptation based on evidence. </p>"},{"location":"pages/knowledge/playbook/enabling-conditions/#data-management-planning-guide","title":"Data Management Planning Guide","text":"<p>A Data Management Plan (DMP) is a tool to guide the identification of anticipated data assets and outline tasks to manage these assets across a full data lifecycle. The DMP Planning Guide ensures that all DMPs contain these elements:</p> <ul> <li>a data inventory</li> <li>protocols for data collection, management and storage</li> <li>protocols for maintaining adequate safeguards that may include the privacy and security of digital information collected under the award</li> <li>documentation that ensures other users can understand and use the data</li> <li>protocols for preserving digital information and facilitating access by other stakeholders.</li> </ul>"},{"location":"pages/knowledge/playbook/enabling-conditions/#indicator-menu","title":"Indicator menu","text":"<p>A menu of indicators ensures that new activities utilize standard indicators and existing custom indicators whenever they are sufficient to inform adaptive management of the activity. This helps the Mission better report results from programming and communicate with stakeholders.</p>"},{"location":"pages/knowledge/playbook/implementation/","title":"Implementation","text":""},{"location":"pages/knowledge/playbook/implementation/#launch-the-activity-database","title":"Launch the Activity Database","text":"<p>One of the first steps in implementing Atlas with the Mission is launching the Activity Database. The Activity Database serves as the single source of truth for Mission operational data and the backbone of Atlas more generally. Introducing the Mission to the Activity Database will provide them an introduction to this new paradigm of data management.</p>"},{"location":"pages/knowledge/playbook/implementation/#onboard-existing-activities","title":"Onboard existing activities","text":"<p>Follow the usage guidance to onboard an activity to Atlas.</p> <p>When first implementing Atlas, it's important to design a strategy for onboarding existing activities. In Guatemala, we used the roll out of Activity Location Data as a catalyst for onboarding activities. A workshop was held to introduce the data collection process and templates to partners. Partners that volunteered to collect Activity Location Data were then onboarded to Atlas so that their data would be aggregated through the automated data pipelines. We also prioritized outreach to activities near closing to harvest as much thematic data and other activity-level data as possible before it was lost.</p>"},{"location":"pages/knowledge/playbook/implementation/#onboard-new-activities","title":"Onboard new activities","text":"<p>Onboarding new activities to Atlas requires engagement from the initial design through solicitation and activity start-up.</p>"},{"location":"pages/knowledge/playbook/implementation/#activity-design","title":"Activity design","text":"<p>The Data Steward should participate in the design of the Activity to support the Technical Team to develop a data strategy for the Activity; understand requirements for data collection, management and reporting; and provide input on the Independent Government Cost Estimate (IGCE). The Activity Design team must consider the requirements for data collection and submission during activity design so that requirements are reflected in the solicitation, that applicants can plan and budget for data collection, and that awards can require data collection as envisioned by the Activity Design team. </p> <p>Following the Mission Order on Activity Design, the activity design team should provide regular design updates to the Data Steward through backstops. The Data Steward should utilize these opportunities to share data standards and requirements such that the activity design team may consider these when preparing the Activity Summary and that any costs associated with data collection and submission are reflected in the estimated budget.</p> <p>The activity design team should review datasets of the Thematic Database for relevant data that can be utilized or updated by the Activity. For example, could this activity use or update remittance data from the Bank of Guatemala? </p> <p>Finally, the activity design team should review existing activities that share the same geographic targeting of the proposed activity. Once the Activity Approval Memorandum is approved, the design team lead (usually the COR) should input the planned geographic targeting of the activity in the Activity Database. The planned geographic targeting may differ from the actual geographic targeting, which will be ascertained through the collection of Activity Location Data by partners.</p>"},{"location":"pages/knowledge/playbook/implementation/#new-activity-mecla-system-workshop","title":"New activity MECLA system workshop","text":"<p>All new activities must participate in a 1-day New Activity MECLA System Workshop during the first 90 days of the award. These workshops include the A/COR, MECLA team, office MECLA POC, partner leadership and partner MECLA team members. This meeting will allow the partner to address questions such as </p> <ol> <li>Which new datasets will be collected (pursuant to requirements of the award)?</li> <li>Which existing datasets will be utilized (either from the Thematic Database or the DEC)?</li> <li>Which existing datasets will be updated through the activity (again, considering both data in the Thematic Datasets and the DEC)</li> <li>What is the timeline for submitting data (consider frequency, deadlines, etc.)</li> <li>How will data be used for learning and managing adaptively?</li> <li>How can data be used to collaborate with others?</li> <li>What risks are presented by the data collection plan?</li> </ol> <p>This meeting can also be an opportunity to train the partner on the standard data collection and management tools of Atlas and Agency systems; however a separate training with primarily technical staff may be more appropriate.</p>"},{"location":"pages/knowledge/playbook/implementation/#work-planning","title":"Work planning","text":"<p>Activity work planning is another important opportunity to ensure that Atlas and MECLA requirements are integrated. Atlas and other requirements should be incorporated directly into the main work plan rather than described in a separate work plan.</p>"},{"location":"pages/knowledge/playbook/implementation/#initial-data-quality-assurance","title":"Initial Data Quality Assurance","text":"<p>Conducting an initial data quality assurance (DQA) will ensure that data are being collected according to the data standards, check for issues with data management systems, ensure data risk mitigation measures are in place, and confirm other elements of the DMP and AMELP are being followed. The initial DQA is a great opportunity to make course corrections before it's too late.</p>"},{"location":"pages/knowledge/playbook/implementation/#mecla-community-of-practice","title":"MECLA Community of Practice","text":"<p>Establishing a community of practice (CoP) to support MECLA specialists, data stewards, GIS specialists, and other technical staff at partners is an excellent way to help partners learn together, co-design best practices, and enforce norms.</p>"},{"location":"pages/knowledge/playbook/institutional-support/","title":"Institutional Support","text":""},{"location":"pages/knowledge/playbook/institutional-support/#clarify-roles-and-responsibilities-of-the-program","title":"Clarify roles and responsibilities of the program","text":"<p>A critical first step is to establish roles and responsibilities for progressing the roadmap. The Guatemala Mission established the Digital, Data and GIS Program to implement Atlas. The table below descries the recommended roles.</p> Role Responsibilities Leadership Sponsor - Defines the vision for how to use data around the Program Cycle- Supports and oversees the Program Owner- Secures resources for continued support of the program Program Owner - Manages and maintains the program- Secures external support when needed- Works with offices and implementing partners in the application of Atlas Data Steward, GIS Specialist, and other technicians - Day-to-day operations of the program- Technical support for users- Analysis and other tasks"},{"location":"pages/knowledge/playbook/institutional-support/#analyze-capacity-gaps","title":"Analyze capacity gaps","text":"<p>A capacity gap analysis will help identify the types of skills and level of effort required to implement Atlas. The approach taken for this exercise was to list out all expected tasks and ongoing responsibilities for the period of interest, define the skills required, and identify whether those skills were available on the team and if that individual had sufficient time to carry out the task. Where gaps were identified, we evaluated all options and determined the most preferable path forward. Find the gap analysis here.</p>"},{"location":"pages/knowledge/playbook/institutional-support/#plan-for-data-governance","title":"Plan for data governance","text":"<p>As the Mission collects more data it will need to make collective decisions the data its collected and the data collection efforts. The Front Office, the Resource Legal Officer (RLO), and individual office directors will need to be able to coordinate before sharing data externally or when considering new data collection efforts. </p>"},{"location":"pages/knowledge/playbook/institutional-support/#acquire-software","title":"Acquire software","text":"<p>Acquiring software for data visualization and GIS will help discover insights and create products that meet the needs of Mission staff. The DataSeed Blanket Purchase Agreement (BPA) is the fastest way to acquire Esri ArcGIS and Tableau. Create onboarding materials for users to help new users adopt these technologies (see for example our onboarding guide for Tableau here).</p>"},{"location":"pages/knowledge/playbook/responsibilities/","title":"Who is Responsible?","text":"<p>ADS 579.2 and ADS579mab section 6 outline the primary responsibilities of entities involved in data collection, management and use within the Agency. The responsibilities of relevant entities are described below.</p> <p>Operating Units are responsible for </p> <ul> <li>including Datasets as deliverables in awards, when writing statements of work or program descriptions. </li> <li>appointing and maintaining a Data Steward and their alternate, ensuring their participation in meetings, taskings, and trainings coordinated by the InfoGov Permanent Working Group (PWG);</li> <li>utilizing Datasets and Activity Location Data during strategic planning and project design, to adapt interventions when appropriate, and as part of their Collaborating, Learning and Adapting (CLA) approach to engage and coordinate across the OU, with partners and among key stakeholders;</li> <li>ensure that Activity Planners or Design Teams, CORs/AORs/GATRs, and/or other subject matter experts receive training on geographic data collection and analysis practices.</li> </ul> <p>Activity Planners or Design Teams, in close coordination with a GIS Specialist or other subject matter expert, must </p> <ul> <li>identify and include Datasets as deliverables that would benefit the development community;</li> <li>determine the appropriate Level of Geographic Detail to collect the Location of Implementation and the Location of Intended Beneficiaries for Activity Location Data;</li> <li>assess data risk and identify mitigation measures, if required; </li> <li>include the estimated cost of collecting Datasets and Activity Location Data in the Activity\u2019s cost estimate; </li> <li>ensure the solicitation indicates sufficient information for respondents to submit cost proposals that reflect the data collection requirements (see ADS 579mab Section (6)(a)(4) for more information); and</li> <li>plan to use Datasets and Activity Location Data for improved monitoring, evaluation and learning.</li> </ul> <p>Contracting Officer Representatives (CORs) and Agreement Officer Representatives (AORs), in close coordination with implementing partners, activity managers, or other appropriate subject matter experts for each Dataset:</p> <ul> <li>identify and include Datasets as deliverables that would benefit the development community during the formulation of the annual work plan; </li> <li>consult with IPs to ensure the understand the requirements for collection and submission of Datasets including Activity Location Data;</li> <li>ensure that USAID-funded Datasets are collected and submitted in accordance with the terms of the award under which they were created;</li> <li>review Datasets prior to submission, which includes identifying principled exceptions to release, redacting (or ensuring the redaction of) sensitive information, and obtaining required clearances (see 579.3.3.5);</li> <li>utilize Datasets including Activity Location Data with other monitoring data to inform efforts to manage adaptively and promote accountability.</li> </ul> <p>Contracting Officers (COs) and Agreement Officers (AOs), including those delegated contract and grant-making authority in ADS 103, Delegations of Authority are responsible for </p> <ul> <li>incorporating clauses and provisions into contracts, grants, cooperative agreements (CAs), interagency agreements, and other implementing instruments to instruct implementing partners that they must submit Datasets created or collected with USAID funding to the DDL.</li> </ul> <p>Data Stewards and GIS Specialists will</p> <ul> <li>serve as subject matter experts for their Operating Unit; </li> <li>provide guidance to CORs/AORs and others within their operating unit on identifying, cataloging, and clearing data for public release, among other responsibilities;</li> <li>establish methods to standardize Activity Location Data and other Datasets such as defining standard names and locations for administrative units, populated places, facility locations, etc.</li> </ul>"},{"location":"pages/knowledge/playbook/stocktaking/","title":"Stocktaking","text":""},{"location":"pages/knowledge/playbook/stocktaking/#conduct-a-stocktaking","title":"Conduct a stocktaking","text":"<p>The first step is understanding where you are now. A stocktaking was conducted in the first two months of this effort. Key informant interviews and a review of existing data assets and business practices helped discover the existing capacity and culture of the Mission. Findings and opportunities were identified related to data infrastructure, access and use. A list of recommendations was produced to guide future work. See the Stocktaking Report here.</p>"},{"location":"pages/knowledge/playbook/stocktaking/#inventory-existing-and-available-data","title":"Inventory existing and available data","text":"<p>A natural next step is to inventory all existing data and understand which data are potentially available. Data assets available within the Mission were inventoried and a review of government and other relevant data sources was conducted. When evaluating external data sources, it's important to consider the relevance, reliability, timeliness and consistency with the Mission's point of view. The data inventory for the Mission is described as an annex in the Stocktaking Report. Over time, the data inventory became the foundation of the Thematic Database.</p>"},{"location":"pages/knowledge/playbook/stocktaking/#create-a-roadmap","title":"Create a roadmap","text":"<p>Once you understand where you are, and which data are available, it's time to set a goal and create a plan to get there. A roadmap describes the key milestones you intend to achieve in the next 3-5 years. For Guatemala, we developed a results framework to inform the roadmap. </p> <p></p> <p>Four pillars comprise the results framework:</p> <ol> <li>Data Infrastructure, Access &amp; Use: managing data as an asset will ensure that stakeholders have timely access to trustworthy and up-to-date information.</li> <li>Technology &amp; Human Resources: securing the capacity required to deliver data-driven products and services will empower stakeholders to use data across the Program Cycle.</li> <li>Institutional Support: sufficiently resourcing and maintaining the program will ensure it\u2019s long-term sustainability.</li> <li>Demand from Stakeholders: cultivating an inquisitive culture will build demand for data products and services that support data-driven decision making.</li> </ol> <p>The recommendations from the Stocktaking Report served as the first roadmap that led to the develop of Atlas.  </p> <p>To identify intervention points for the roadmap, a theory of change was developed for each pillar.</p>"},{"location":"pages/knowledge/playbook/stocktaking/#data-infrastructure-access-use","title":"Data Infrastructure, Access &amp; Use","text":"<pre><code>graph LR\n  A(Sufficient data \\n available for \\n basic analyses) --&gt; B(Data are \\n joinable, \\n shareable, \\n queryable) --&gt; C(Major data \\n gaps are \\n filled) --&gt; D(Data are \\n trustworthy, \\n stakeholders \\n trust data) --&gt; E(Data are \\n updated \\n regularly) --&gt; F{Data quality \\n and frequency \\n meet stakeholder \\n needs}</code></pre>"},{"location":"pages/knowledge/playbook/stocktaking/#technology-human-resources","title":"Technology &amp; Human Resources","text":"<pre><code>graph LR\n  A(Access to \\n expertise &amp; \\n techology) --&gt; B(Stakeholder \\n needs are \\n understood) --&gt; C(Products &amp; \\n services \\n are defined) --&gt; D(Long-term \\n strategy is \\n defined)\n  D --&gt; E(Continuous \\n learning and \\n exploring \\n new tools)\n  D --&gt; F(Manage &amp; \\n maintain \\n capacity)\n  E --&gt; G{Expertise &amp; \\n tech meet \\n stakeholder \\n needs}\n  F --&gt; G</code></pre>"},{"location":"pages/knowledge/playbook/stocktaking/#institutional-support","title":"Institutional Support","text":"<pre><code>graph LR\n  A(Identify internal \\n champions &amp; \\n data leaders) --&gt; B(Small wins \\n are created) --&gt; C(Stakeholder \\n requests \\n are met in a \\n timely way ) --&gt; D(Platforms are \\n developed to \\n make data \\n and analyses \\n accessible) --&gt; E(Data and \\n analyses are \\n integrated \\n across the \\n Program Cycle) --&gt; F{Timely \\n products &amp; \\n services meet \\n stakeholder \\n needs}</code></pre>"},{"location":"pages/knowledge/playbook/stocktaking/#demand-from-stakeholders","title":"Demand from Stakeholders","text":"<pre><code>graph LR\n  A(Stakeholders are \\n aware of \\n products &amp; \\n services, know \\n when to \\n use them) --&gt; B(Stakeholders \\n ask insightful \\n questions)\n  B --&gt; C(Stakeholders \\n access \\n shared data \\n for insights)\n  B --&gt; D(Stakeholders \\n request ad-hoc \\n analyses)\n  C --&gt; E(Stakeholders \\n develop data \\n literacy)\n  D --&gt; E\n  E --&gt; F(Stakeholders \\n are accountable \\n for decisions)\n  F --&gt; G(Stakeholders \\n rigorously test \\n assumptions and \\n tactics)\n  G --&gt; H{Stakeholders \\n share culture \\n of curiosity \\n &amp; continuous \\n improvement}</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/","title":"Google Apps Script Tutorial","text":"<p>Develop a web application with Google Apps Script and Bootstrap. After completing this tutorial, you will know how to set up and deploy a standalone web application with Google Apps Script, use Bootstrap for styling, read data from a Google Sheet, and dynamically populate the web app using JavaScript.</p> <p></p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#1-google-apps-script-set-up","title":"1. Google Apps Script set up","text":""},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#11-set-up-apps-script-project","title":"1.1 Set up Apps Script Project","text":"<ul> <li>Navigate to Google Apps Script.</li> <li>Click New Project to create a new project. This will create a stand-alone application. </li> </ul> <p>Note</p> <p>Google Apps Script projects may also be 'containerized' within a Google Sheet, Doc, Slides or other Google Workplace product. To create a containerized project, select Apps Script under the Extensions menu in a Sheet, Doc, Slides, or Form.</p> <ul> <li>Name the project.</li> </ul>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#12-create-the-doget-function","title":"1.2 Create the doGet Function","text":"<ul> <li>Replace all existing content in the <code>Code.gs</code> file with the code shown below:</li> </ul> Code.gs<pre><code>function doGet(e) {\n  return HtmlService.createTemplateFromFile('index').evaluate();\n};\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#13-create-html-file","title":"1.3 Create HTML File","text":"<ul> <li>Click the \"+\" next to Files in the left sidebar and select 'HTML'. Name the file <code>index</code> (note the file extension <code>.html</code> will automatically be added). The name of this file must be the same as the string passed to the <code>doGet</code> function, but you may name it however you want.</li> </ul>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#14-include-bootstrap","title":"1.4 Include Bootstrap","text":"<ul> <li>Navigate to the Bootstrap Quick Start Guide.</li> <li>Follow the instructions in the Quick Start Guide to populate your <code>index.html</code> file and include Bootstrap's JavaScript and CSS. See below for how your <code>index.html</code> file should look. </li> </ul> index.html<pre><code>&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n    &lt;title&gt;Bootstrap demo&lt;/title&gt;\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65\" crossorigin=\"anonymous\"&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Hello, world!&lt;/h1&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#16-deploy-the-web-application","title":"1.6 Deploy the web application","text":"<p>We'll deploy the basic version to confirm that everything is working correctly.</p> <ul> <li>Save your project by clicking the Save Project icon or <code>Ctrl+s</code>.</li> <li>At the top right of the script project, click Deploy &gt; New deployment.</li> <li>Next to \"Select type,\" click the Settings icon and select Web app.</li> <li>Input \"Initial Deployment\" in the Description field. </li> <li>Select 'Me' in the Execute as field.</li> <li>Select 'Only myself' in the Who has access field.</li> <li>Click Deploy.</li> <li>Click the URL of the Web app shown in the dialog box that was created to visit the app. You should see a webpage that simply says \"Hello, world!\".</li> <li>Close the web application and return to the script files; close the dialog box if necessary.</li> </ul> <p>You now have a functioning Google Apps Script web application with Bootstrap. Next we'll begin using Bootstrap \"containers\" for easy styling of our web app. </p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#2-bootstrap-grid-system","title":"2. Bootstrap grid system","text":"<p>Tip</p> <p>While developing your web application, you'll want to be able to easily edit the code, review the draft site, and access documentation. I like to split my screen such that the left half displays the code editor and the right half has either the draft application or the documentation I need (in two or more browser tabs within the same window). You may also want to create a README file to capture notes as you go. Figure out what works best for you. Multiple screens make coding even easier.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#21-create-a-container","title":"2.1 Create a container","text":"<p>Bootstrap organizes its grid system within containers. You'll notice when you create a container, the content within the container has more padding from the edges of the screen. After completing this step, notice how the 'Hello, World!' header moves inward once we place it within a container. </p> <ul> <li>Create a simple container within the body tag of <code>index.html</code> by replacing the existing code <code>&lt;h1&gt;Hello, world!&lt;/h1&gt;</code>:</li> </ul> index.html<pre><code>&lt;div class=\"container\"&gt;&lt;/div&gt;\n</code></pre> <p>Tip</p> <p>Note how I've opened and closed the <code>div</code> tag on the same line. It's best practice to always close the tag first and then insert content so that you don't accumulate multiple unclosed tags, which can be difficult to sort out later.</p> <ul> <li>Add a row within the container by creating a return (insert your cursor and press Enter) just before the closing div tag. The container div will now look like this.</li> </ul> index.html<pre><code>&lt;div class=\"container\"&gt;\n    &lt;div class=\"row\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <ul> <li>Add a single column within the row. This column will span the entire row. While not necessary, we'll get in the habit of specifying both the number of columns (up to 12) to span and the screen size for responsive design.</li> </ul> index.html<pre><code>&lt;div class=\"container\"&gt;\n    &lt;div class=\"row\"&gt;\n        &lt;div class=\"col-sm-12\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#23-populate-container-content","title":"2.3 Populate container content","text":"<ul> <li>For this example, we'll add back the header <code>Hello, world!</code> and add what's known as a lead paragraph as a subtitle for the page.</li> </ul> index.html<pre><code>&lt;div class=\"container\"&gt;\n    &lt;div class=\"row\"&gt;\n        &lt;div class=\"col-sm-12\"&gt;\n            &lt;h1&gt;Hello, world!&lt;/h1&gt;\n            &lt;p class=\"lead\"&gt;\n                This is a lead paragraph. It stands out from regular paragraphs.\n            &lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <ul> <li>Save your work by clicking the Save icon in the toolbar above the code or with <code>Ctrl + S</code>.</li> </ul>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#22-deploy-as-a-test-deployment","title":"2.2 Deploy as a Test Deployment","text":"<p>To see changes to our app as we make them, we need to deploy as a Test Deployment.</p> <ul> <li>At the top right of the script project, click Deploy &gt; Test deployment.</li> <li>Click the URL in the dialog box to open the test deployment.   Refresh the test deployment to see changes after you save changes to the project's code files.</li> </ul> <p>Note</p> <p>Learn more about the Grid System on Bootstrap's official documentation. Play around with different combinations of containers, rows and columns to get the feel for the grid system. Also, try responsive design by changing the number of columns associated with different screen sizes. You can resize the screen or use the Developer Tools in Chrome (<code>Ctrl+Shift+I</code>) to see how the application would look on a tablet or smart phone.</p> <p>Next, we'll create a Card to practice using a Bootstrap Component in Google Apps Script Bootstrap Components.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#3-bootstrap-card-components","title":"3. Bootstrap card components","text":""},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#31-review-card-component-documentation","title":"3.1 Review card component documentation","text":"<p>The first step when using a component is always to review the documentation and understand how to use it and what its capabilities are. </p> <ul> <li>Navigate to Bootstrap's Card documentation.</li> <li>Review the documentation.</li> </ul>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#32-build-a-simple-card","title":"3.2 Build a simple card","text":"<ul> <li>Copy the relevant code for the card style you want and paste it into your <code>index.html</code> file directly below the lead paragraph. For this tutorial, let's use the Titles, text, and links card example.</li> </ul> index.html<pre><code>&lt;h1&gt;Hello, world!&lt;/h1&gt;\n&lt;p class=\"lead\"&gt;\n    This is a lead paragraph. It stands out from regular paragraphs.\n&lt;/p&gt;\n&lt;div class=\"card\"&gt;\n  &lt;div class=\"card-body\"&gt;\n    &lt;h5 class=\"card-title\"&gt;Card title&lt;/h5&gt;\n    &lt;h6 class=\"card-subtitle mb-2 text-muted\"&gt;Card subtitle&lt;/h6&gt;\n    &lt;p class=\"card-text\"&gt;Some quick example text to build on the card title and make up the bulk of the card's content.&lt;/p&gt;\n    &lt;a href=\"#\" class=\"card-link\"&gt;Card link&lt;/a&gt;\n    &lt;a href=\"#\" class=\"card-link\"&gt;Another link&lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <ul> <li>Save the file and reload the test deployment to see the card displayed on the page. It should look like this:</li> </ul> <p>Next we will use the data read from a Google Sheet to populate the content of the Bootstrap card components.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#4-populating-components","title":"4. Populating components","text":""},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#41-create-a-container-to-store-the-cards","title":"4.1 Create a container to store the cards","text":"<p>Replace the code that creates the individual card with a container that will store all cards once created from the Google Sheet. Provide this component with an <code>id</code> equal to <code>\"datasets\"</code>. The <code>id</code> will allow us to find it later when we want to add cards to it.</p> index.html<pre><code>&lt;h1&gt;Hello, world!&lt;/h1&gt;\n&lt;p class=\"lead\"&gt;\n    This is a lead paragraph. It stands out from regular paragraphs.\n&lt;/p&gt;\n&lt;div class=\"container\" id=\"datasets\"&gt;&lt;/div&gt;\n</code></pre> <p>Tip</p> <p>To find the right closing div tag to delete, select any div tag and note that the corresponding opening div tag is highlighted in your code. You want to delete the entire div whose <code>class</code> property is equal to <code>\"card\"</code>.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#42-create-a-script-tag-in-indexhtml","title":"4.2 Create a script tag in <code>index.html</code>","text":"<p>We're going to use JavaScript to create and populate cards. To include JavaScript in our <code>index.html</code> file, we need to create a new script tag. Create a new line just above the closing body tag ( <code>&lt;/body&gt;</code> ) and add an opening and closing script tag.</p> index.html<pre><code>&lt;script&gt;\n\n&lt;/script&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#43-insert-dummy-data","title":"4.3 Insert dummy data","text":"<p>For now, we'll use dummy data to make sure the JavaScript is working as expected.</p> <ul> <li>In between the opening and closing script tag, paste the following dummy data.</li> </ul> index.html<pre><code>&lt;script&gt;\n    var data = [\n      ['Dataset1', 'Source1', 'Abstract1', 'http://www.example.com/'],\n      ['Dataset2', 'Source2', 'Abstract2', 'http://www.example.com/']\n    ];\n&lt;/script&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#44-write-code-to-print-the-data-to-the-console","title":"4.4 Write code to print the data to the console","text":"<p>To get started, let's simply write code that will print each line of the data to the console. The Developer Tools console is a part of your browser that provides additional information to developers. To access it using Chrome, navigate to the test deployment of your web app and use <code>Ctrl+Shift+I</code>. </p> <ul> <li>Write a JavaScript arrow function to print data to the console. </li> </ul> index.html<pre><code>&lt;script&gt;\n    var data = [\n      ['Dataset1', 'Source1', 'Abstract1', 'http://www.example.com/'],\n      ['Dataset2', 'Source2', 'Abstract2', 'http://www.example.com/']\n    ];\n\n    data.forEach(row =&gt; {\n      console.log(row);\n    });\n&lt;/script&gt;\n</code></pre> <ul> <li>Navigate to the test deployment of your web app and use <code>Ctrl+Shift+I</code> to open the Developer Tools console. Reload the app. You should see each row of data printed out (you may also see warnings (yellow) and errors (red); ignore those for now).</li> </ul> <p></p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#45-write-code-to-create-cards","title":"4.5 Write code to create cards","text":"<p>Let's expand our code so that not only does the data print to the console, it is also added to our web application. It's not important to understand how this code works right now, however you will need to understand how the code works as you progress in your learning.</p> <ul> <li>Expand the code in your script tags such that it looks like this.</li> </ul> index.html<pre><code>&lt;script&gt;\n    var data = [\n      ['Dataset1', 'Source1', 'Abstract1', 'http://www.example.com/'],\n      ['Dataset2', 'Source2', 'Abstract2', 'http://www.example.com/']\n    ];\n\n    var cardContainer = document.getElementById('datasets');\n\n    data.forEach(row =&gt; {\n        console.log(row);\n\n        var card = document.createElement('div');\n        card.className = 'card';\n\n        var cardBody = document.createElement('div');\n        cardBody.className = 'card-body'\n\n        var cardTitle = document.createElement('h5');\n        cardTitle.innerText = row[0];\n        cardTitle.className = 'card-title';\n\n        var cardSubtitle = document.createElement('h6');\n        cardSubtitle.innerText = row[1];\n        cardSubtitle.className = 'card-subtitle mb-2 text-muted';\n\n        var cardText = document.createElement('p');\n        cardText.innerText = row[2];\n        cardText.className = 'card-text';\n\n        var cardLink = document.createElement('a');\n        cardLink.innerText = 'Get Data';\n        cardLink.href = row[3];\n        cardLink.target = \"_blank\";\n        cardLink.className = 'card-link';\n\n        cardBody.appendChild(cardTitle);\n        cardBody.appendChild(cardSubtitle);\n        cardBody.appendChild(cardText);\n        cardBody.appendChild(cardLink);\n        card.appendChild(cardBody);\n        cardContainer.appendChild(card);\n      });\n&lt;/script&gt;\n</code></pre> <ul> <li>Save the project and reload the web application to see the changes.</li> </ul>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#46-wrap-the-code-in-a-function","title":"4.6 Wrap the code in a function","text":"<ul> <li>Finally, we'll wrap all of this code in a function so we can more easily run the code with our Google Sheets data. Call the function at the end so the code still runs. Refresh the web browser to confirm everything is still working.</li> </ul> index.html<pre><code>&lt;script&gt;\n    var data = [\n      ['Dataset1', 'Source1', 'Abstract1', 'http://www.example.com/'],\n      ['Dataset2', 'Source2', 'Abstract2', 'http://www.example.com/']\n    ];\n\n    var cardContainer = document.getElementById('datasets');\n\n    function populateCards(data) {\n      var cardContainer = document.getElementById('datasets');\n\n      data.forEach(row =&gt; {\n        console.log(row);\n\n        var card = document.createElement('div');\n        card.className = 'card';\n\n        var cardBody = document.createElement('div');\n        cardBody.className = 'card-body'\n\n        var cardTitle = document.createElement('h5');\n        cardTitle.innerText = row[0];\n        cardTitle.className = 'card-title';\n\n        var cardSubtitle = document.createElement('h6');\n        cardSubtitle.innerText = row[1];\n        cardSubtitle.className = 'card-subtitle mb-2 text-muted';\n\n        var cardText = document.createElement('p');\n        cardText.innerText = row[2];\n        cardText.className = 'card-text';\n\n        var cardLink = document.createElement('a');\n        cardLink.innerText = 'Get Data';\n        cardLink.href = row[3];\n        cardLink.target = \"_blank\";\n        cardLink.className = 'card-link';\n\n        cardBody.appendChild(cardTitle);\n        cardBody.appendChild(cardSubtitle);\n        cardBody.appendChild(cardText);\n        cardBody.appendChild(cardLink);\n        card.appendChild(cardBody);\n        cardContainer.appendChild(card);\n      });\n    };\n\n    populateCards(data);\n&lt;/script&gt;\n</code></pre> <p>Next we'll replace our dummy data by reading data directly from a Google Sheet.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#5-reading-data-from-google-sheets","title":"5. Reading data from Google Sheets","text":""},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#51-create-a-google-sheet","title":"5.1 Create a Google Sheet","text":"<ul> <li>Create a new Google Sheet and populate it with the dummy data (or data of your choosing). Include headers in your Google Sheet. The data should look something like the below; you can include as many rows as you like.</li> <li>Rename the sheet to <code>data</code>.</li> </ul>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#52-copy-the-google-spreadsheet-id","title":"5.2 Copy the Google Spreadsheet ID","text":"<ul> <li>Copy the Google Spreadsheet ID from the URL bar for your newly created Google Sheet. It will be located immediately after the <code>/d/</code> in the URL, as shown by <code>{google-sheets-id}</code> below.</li> </ul> <pre><code>https://docs.google.com/spreadsheets/d/{google-sheet-id}/edit\n</code></pre> <ul> <li>Within the file <code>Code.gs</code> in your Apps Script project, create a variable to store the Google Sheet spreadsheet ID and paste the ID.</li> </ul> index.html<pre><code>const ssid = '15QXeVfDXwESIHVzj9YDn7KarN93qPmwl6g-BWdt5QUw';\n</code></pre> <p>Tip</p> <p>I like to include these variables at the top of my files so they are easy to change if needed.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#53-write-a-function-to-read-the-data","title":"5.3 Write a function to read the data","text":"<ul> <li>Write the following function to read all data from your spreadsheet in <code>Code.gs</code>. Note I use <code>Logger.log(data)</code> to write the data to the Google Apps script console; this will help us test our code.</li> </ul> index.html<pre><code>function getData(){\n  var ss = SpreadsheetApp.openById(ssid);\n  var ws = ss.getSheetByName('data');\n  var data = ws.getRange(1, 1, ws.getLastRow(), ws.getLastColumn()).getValues();\n  var headers = data.shift();\n\n  Logger.log(data);\n\n  return data\n}\n</code></pre> <ul> <li>Save the project.</li> </ul> <p>The full Code.gs file should now look like this (your google sheet ID will be different).</p> index.html<pre><code>const ssid = '15QXeVfDXwESIHVzj9YDn7KarN93qPmwl6g-BWdt5QUw';\n\nfunction doGet(e) {\n  return HtmlService.createHtmlOutputFromFile('index');\n}\n\nfunction getData(){\n  var ss = SpreadsheetApp.openById(ss_id);\n  var ws = ss.getSheetByName('data');\n  var data = ws.getRange(1, 1, ws.getLastRow(), ws.getLastColumn()).getValues();\n  var headers = data.shift();\n\n  Logger.log(data);\n\n  return data\n}\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#54-authorize-access-to-google-drive","title":"5.4 Authorize access to Google Drive","text":"<ul> <li>On the toolbar above the code, select the getData function.</li> </ul> <ul> <li>Click Run.</li> <li>A dialog box prompting you to grant authorization will pop up. Click Review Permissions.</li> </ul> <ul> <li>Grant authorization by following the prompts.</li> <li>Click Run again to run the function with permissions granted. You should see the data printed to the Log in Google Apps Script.</li> <li>The data from the Google Sheet should print out to the Execution Log.</li> </ul>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#55-pass-data-from-sheets-to-web-application","title":"5.5 Pass data from Sheets to web application","text":"<ul> <li>Open the <code>index.html</code> file.</li> <li>Replace the function <code>populateCards(data)</code> with the following statement.</li> </ul> index.html<pre><code>google.script.run.withSuccessHandler(populateCards).getData();\n</code></pre> <p>This statement runs the <code>getData()</code> function in the <code>Code.gs</code> file and passes the data, which was returned by that function, to your function <code>populateCards</code>. Save the project and reload the web application to test it.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#56-load-cards-on-document-load","title":"5.6 Load cards on document load","text":"<p>Finally, wrap the above statement in an event listener so that the function only runs once the webpage content is loaded. This ensures that the card container is available to the <code>populateCards</code> function before the cards are loaded.</p> index.html<pre><code>document.addEventListener('DOMContentLoaded', function(){\n    google.script.run.withSuccessHandler(populateCards).getData();\n});\n</code></pre> <ul> <li>Save the project and reload the web application to test it.</li> </ul> <p>Next, we'll re-organize our project to a more modular structure that will enable better long-term maintenance.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#7-project-organization","title":"7. Project Organization","text":""},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#71-write-an-include-function","title":"7.1 Write an <code>include</code> function","text":"<ul> <li>Navigate to your <code>Code.gs</code> file and write this function below the existing code.</li> </ul> Code.gs<pre><code>function include(filename){\n    return HtmlService.createHtmlOutputFromFile(filename).getContent();\n}\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#72-create-a-file-to-store-javascript","title":"7.2 Create a file to store JavaScript","text":"<ul> <li>Click the \"+\" next to Files and select 'HTML'. Name the file <code>JavaScript</code> (note the file extension <code>.html</code> will automatically be added). Delete any existing code in the file and cut and paste everything in the script tags created instep 4 into this file. </li> </ul> <p>Note</p> <p>Everything must be between <code>script</code> tags in this file.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#73-include-the-script-file-in-your-file-indexhtml","title":"7.3 Include the script file in your file <code>index.html</code>","text":"<ul> <li>Add this code after the last script tag in the <code>index.html</code> file, but before the closing body tag.</li> </ul> index.html<pre><code>&lt;!-- Include custom JavaScript --&gt;\n&lt;?!= include(\"JavaScript\"); ?&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#74-optional-create-a-file-to-store-styles","title":"7.4 (Optional) Create a file to store styles","text":"<p>In web development, styles (which change how your app looks) are controlled using cascading style sheets (CSS). Create a file</p> <ul> <li> <p>Click the \"+\" next to Files and select 'HTML'. Name the file <code>StyleSheet</code> (note the file extension <code>.html</code> will automatically be added).</p> </li> <li> <p>Add an opening and closing style tag. All content must be included within the style tags.</p> </li> </ul> StyleSheet.html<pre><code>&lt;style&gt;\n\n&lt;/style&gt;\n</code></pre> <ul> <li>Include the style sheet file in your file <code>index.html</code>. Add this code after the last style tag within the headers tags.</li> </ul> <pre><code>&lt;!-- Include custom Styles --&gt;\n&lt;?!= include(\"StyleSheet\"); ?&gt;\n</code></pre> <p>You can use this file to adjust styles and change how the web app looks. </p> <p>Next we'll deploy this version from the test environment to the web.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#8-deploy-web-app","title":"8. Deploy web app","text":"<p>It is best practice to update the Active deployment rather than deploy multiple versions of the same application. This will keep the link to your project the same, so your users can find it again after you update it. The Apps Script editor maintains a version history that will allow you to roll back changes if you need to.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#81-manage-existing-deployments","title":"8.1 Manage existing deployments","text":"<ul> <li>Save your project by clicking the Save Project icon or <code>Ctrl+s</code>.</li> <li>At the top right of the script project, click Deploy &gt; Manage deployments.</li> <li>Click the pencil icon to Edit the active deployment.</li> </ul> <ul> <li>Under Version, select \"New version\".</li> <li>Update the description to a meaningful but concise statement about what you changed (this will help you roll back changes if needed to the correct version in the future).</li> <li>You can also change the Execute as and Who has access settings. Learn more about these settings in the Google Apps Script Developer Guide.</li> <li>Click Deploy.</li> </ul> <p>Congratulations! You've created a standalone web application that reads data from a Google Sheet and populates Bootstrap components. See the Additional Resource below to continue learning.</p>"},{"location":"pages/knowledge/tutorials/google-apps-script-tutorial/#full-code","title":"Full code","text":"<p>See below for full code.</p> Code.gs<pre><code>const ssid = '15QXeVfDXwESIHVzj9YDn7KarN93qPmwl6g-BWdt5QUw';\n\nfunction doGet(e) {\n  return HtmlService.createTemplateFromFile('index').evaluate();\n}\n\nfunction include(filename){\n    return HtmlService.createHtmlOutputFromFile(filename).getContent();\n}\n\nfunction getData(){\n  var ss = SpreadsheetApp.openById(ssid);\n  var ws = ss.getSheetByName('data');\n  var data = ws.getRange(1, 1, ws.getLastRow(), ws.getLastColumn()).getValues();\n  var headers = data.shift();\n\n  Logger.log(data);\n\n  return data\n}\n\nfunction include(filename){\n    return HtmlService.createHtmlOutputFromFile(filename).getContent();\n}\n</code></pre> index.html<pre><code>&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n    &lt;title&gt;Bootstrap demo&lt;/title&gt;\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65\" crossorigin=\"anonymous\"&gt;\n    &lt;?!= include(\"StyleSheet\"); ?&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;div class=\"row\"&gt;\n        &lt;div class=\"col-sm-12\"&gt;\n          &lt;h1&gt;Hello, world!&lt;/h1&gt;\n          &lt;p class=\"lead\"&gt;\n              This is a lead paragraph. It stands out from regular paragraphs.\n          &lt;/p&gt;\n          &lt;div class=\"container\" id=\"datasets\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;?!= include(\"JavaScript\"); ?&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> JavaScript.html<pre><code>&lt;script&gt;\n  var data = [\n    ['Dataset1', 'Source1', 'Abstract1', 'http://www.example.com/'],\n    ['Dataset2', 'Source2', 'Abstract2', 'http://www.example.com/']\n  ];\n\n  var cardContainer = document.getElementById('datasets');\n\n  function populateCards(data) {\n    var cardContainer = document.getElementById('datasets');\n\n    data.forEach(row =&gt; {\n      console.log(row);\n\n      var card = document.createElement('div');\n      card.className = 'card';\n\n      var cardBody = document.createElement('div');\n      cardBody.className = 'card-body'\n\n      var cardTitle = document.createElement('h5');\n      cardTitle.innerText = row[0];\n      cardTitle.className = 'card-title';\n\n      var cardSubtitle = document.createElement('h6');\n      cardSubtitle.innerText = row[1];\n      cardSubtitle.className = 'card-subtitle mb-2 text-muted';\n\n      var cardText = document.createElement('p');\n      cardText.innerText = row[2];\n      cardText.className = 'card-text';\n\n      var cardLink = document.createElement('a');\n      cardLink.innerText = 'Get Data';\n      cardLink.href = row[3];\n      cardLink.target = \"_blank\";\n      cardLink.className = 'card-link';\n\n      cardBody.appendChild(cardTitle);\n      cardBody.appendChild(cardSubtitle);\n      cardBody.appendChild(cardText);\n      cardBody.appendChild(cardLink);\n      card.appendChild(cardBody);\n      cardContainer.appendChild(card);\n    });\n  };\n  document.addEventListener('DOMContentLoaded', function(){\n    google.script.run.withSuccessHandler(populateCards).getData();\n  });\n&lt;/script&gt;\n</code></pre> StyleSheet.html<pre><code>&lt;style&gt;\n\n&lt;/style&gt;\n</code></pre> Additional Resources <ul> <li>Google AppScript Web Apps guide<ul> <li>YouTube - Google Sheet Bootstrap form tutorial</li> <li>YouTube - Learn Google Spreadsheets - Web Apps Playlists</li> </ul> </li> </ul>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/","title":"Leaflet Tutorial","text":"<p>Develop a geospatial web application using Google Apps Script and Leaflet. After completing this tutorial, you will know how to set up a web application that creates map-based data visualization using the library Leaflet and deployed with Google Apps Script. Consider completing the Google Apps Script Tutorial first to familiarize yourself with the Google Apps Script environment.</p>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#1-leaflet-project-set-up","title":"1. Leaflet project set up","text":""},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#11-create-the-project","title":"1.1 Create the project","text":"<ul> <li>Navigate to Google Apps Script.</li> <li>Click New Project to create a new project. This will create a stand-alone application.</li> <li>Name the project.</li> </ul>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#12-set-up-codegs","title":"1.2 Set up Code.gs","text":"<ul> <li>Replace all existing content in the <code>Code.gs</code> file with the code shown below:</li> </ul> code.gs<pre><code>function doGet(e) {\n  return HtmlService.createTemplateFromFile('map').evaluate();\n};\n\nfunction include(filename) {\n  return HtmlService.createHtmlOutputFromFile(filename)\n      .getContent();\n}\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#13-set-up-maphtml","title":"1.3 Set up map.html","text":"<ul> <li>Create a new HTML file called <code>map.html</code>.</li> <li>Include imports for both Bootstrap and Leaflet.</li> <li>Create a <code>div</code> element with the <code>id</code> \"map\".</li> </ul> map.html<pre><code>&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n    &lt;title&gt;Leaflet Demo&lt;/title&gt;\n    &lt;!-- Import Bootstrap --&gt;\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx\" crossorigin=\"anonymous\"&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap-icons@1.9.1/font/bootstrap-icons.css\"&gt;\n    &lt;!-- Import Leaflet --&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://unpkg.com/leaflet@1.8.0/dist/leaflet.css\"\n   integrity=\"sha512-hoalWLoI8r4UszCkZ5kL8vayOGVae1oxXe/2A4AO6J9+580uKHDO3JdHb7NzwwzK5xr/Fs0W40kiNHxM9vyTtQ==\"\n   crossorigin=\"\"/&gt;\n    &lt;!-- Include custom CSS --&gt;\n    &lt;?!= include('css'); ?&gt; \n  &lt;/head&gt;\n\n  &lt;body&gt;\n      &lt;div id=\"map\"&gt;&lt;/div&gt;\n\n    &lt;!-- Import Popper (for Bootstrap) --&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.5/dist/umd/popper.min.js\" integrity=\"sha384-Xe+8cL9oJa6tN/veChSP7q+mnSPaj5Bcu9mPX5F5xIGE0DVittaqT5lorf0EI7Vk\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;!-- Import Bootstrap --&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.min.js\" integrity=\"sha384-ODmDIVzN+pFdexxHEHFBQH3/9/vQ9uori45z4JjnFsRydbmQbmL5t1tQ0culUzyK\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;!-- Import Leaflet --&gt;\n    &lt;script src=\"https://unpkg.com/leaflet@1.8.0/dist/leaflet.js\"\n   integrity=\"sha512-BB3hKbKWOc9Ez/TAwyWxNXeoV9c1v6FIeYiBieIWkpLjauysF18NzgR1MBNBXf8/KABdlkX68nAhlwcDFLGPCQ==\"\n   crossorigin=\"\"&gt;&lt;/script&gt;\n    &lt;!--Include custom JavaScript--&gt;\n    &lt;?!= include('JavaScript') ?&gt;   \n  &lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#14-set-up-css-file","title":"1.4 Set up CSS file","text":"<ul> <li>Create a new HTML file called <code>css.html</code>.</li> <li>Style the map to display with 100% of the viewport height.</li> </ul> css.html<pre><code>&lt;style&gt;\n  #map { height: 100vh; }\n&lt;/style&gt;\n</code></pre> <ul> <li>Make sure that the CSS file is included in the file <code>map.html</code> (see code above).</li> </ul>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#15-set-up-javascript-file","title":"1.5 Set up JavaScript file","text":"<ul> <li>Create a new HTML file called <code>JavaScript.html</code>.</li> <li>Include the code to create a map with a basemap (tile layer).</li> </ul> JavaScript.html<pre><code>&lt;script&gt;\n  // create map\n  var map = L.map('map', {\n    center: [16, -90.8],\n    zoom: 7,\n    maxZoom: 20,\n  });\n\n  // set base map\n  L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {\n    attribution: '\u00a9 OpenStreetMap'\n  }).addTo(map);\n\n&lt;/script&gt;\n</code></pre> <ul> <li>Make sure the JavaScript file is included in the file <code>map.html</code> (see code above).</li> </ul>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#16-deploy-the-app","title":"1.6 Deploy the app","text":"<ul> <li>Save your project by clicking the Save Project icon or <code>Ctrl+s</code>.</li> <li>At the top right of the script project, click Deploy &gt; New deployment.</li> <li>Next to \"Select type,\" click the Settings icon and select Web app.</li> <li>Input \"Initial Deployment\" in the Description field. </li> <li>Select 'Me' in the Execute as field.</li> <li>Select 'Only myself' in the Who has access field.</li> <li>Click Deploy.</li> <li>Click the URL of the Web app shown in the dialog box that was created to visit the app. You should see a webpage with an interactive map.</li> <li>Close the web application and return to the script files; close the dialog box if necessary.</li> </ul>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#2-add-leaflet-sidebar-v2","title":"2. Add Leaflet Sidebar v2","text":"<p>Note</p> <p>Leaflet is very lightweight and relies on third party plugins for a lot of the functionality you might want. If you have an ESRI account, you can also access additional basemaps and plugins from esri-leaflet. Some packages can be added easily via CDN. Others (which you often find on GitHub) require copy/pasting code into files within the project. For those distributed via CDN, just use the <code>include</code> syntax to include the CSS and JavaScript files.  When copying code from Github, you can either use the minified (<code>.min.css</code>) files or the full css (<code>.css</code>). Minified css is not human readable but can be smaller file sizes. If you want to see (and potentially modify) how the package works, get the full code; otherwise the minified is better. Packages like Leaflet-Sidebar-v2 and Marker Cluster will be included by copy/pasting the code into your project.</p>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#21-copy-leaflet-sidebar-v2-code-into-your-project","title":"2.1 Copy Leaflet Sidebar v2 code into your project","text":"<ul> <li>Navigate to Leaflet-Sidebar-v2 github page.</li> <li>Copy the contents from <code>css/leaflet-sidebar.min.css</code> and paste into a new file within the GAS project named <code>leaflet-sidebar-v2.css.html</code>.</li> <li>Copy the contents from <code>js/leaflet-sidebar.js</code> and paste into a new file within the GAS project named <code>leaflet-sidebar-v2.js.html</code>.</li> </ul>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#22-include-in-maphtml","title":"2.2 Include in <code>map.html</code>","text":"<ul> <li>Include the CSS within the <code>&lt;head&gt;</code> tag using the <code>&lt;?!= include ?&gt;</code> syntax.</li> </ul> map.html<pre><code>&lt;!-- Include Sidebar v2 --&gt; \n&lt;?!= include('leaflet-sidebar-v2.css') ?&gt;\n</code></pre> <p>Include the JavaScript within the <code>&lt;body&gt;</code> tag using the <code>&lt;?!= include ?&gt;</code> syntax.</p> map.html<pre><code>&lt;!-- Load Sidebar v2 --&gt;\n&lt;?!= include('leaflet-sidebar-v2.js') ?&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#23-define-the-sidebar-in-maphtml","title":"2.3 Define the sidebar in <code>map.html</code>","text":"<ul> <li>Add the example sidebar html from the Github README in <code>map.html</code> below the line <code>&lt;div id=\"map\"&gt;&lt;/div&gt;</code>. We'll edit this later (you'll need to load Font Awesome icons for this code to work, or switch the icons to Bootstrap icons).</li> </ul> map.html<pre><code>&lt;div id=\"sidebar\" class=\"leaflet-sidebar collapsed\"&gt;\n    &lt;!-- Nav tabs --&gt;\n    &lt;div class=\"leaflet-sidebar-tabs\"&gt;\n        &lt;ul role=\"tablist\"&gt; &lt;!-- top aligned tabs --&gt;\n            &lt;li&gt;&lt;a href=\"#home\" role=\"tab\"&gt;&lt;i class=\"fa fa-bars\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n            &lt;li class=\"disabled\"&gt;&lt;a href=\"#messages\" role=\"tab\"&gt;&lt;i class=\"fa fa-envelope\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#profile\" role=\"tab\"&gt;&lt;i class=\"fa fa-user\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n\n        &lt;ul role=\"tablist\"&gt; &lt;!-- bottom aligned tabs --&gt;\n            &lt;li&gt;&lt;a href=\"#settings\" role=\"tab\"&gt;&lt;i class=\"fa fa-gear\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/div&gt;\n\n    &lt;!-- Tab panes --&gt;\n    &lt;div class=\"leaflet-sidebar-content\"&gt;\n        &lt;div class=\"leaflet-sidebar-pane\" id=\"home\"&gt;\n            &lt;h1 class=\"leaflet-sidebar-header\"&gt;\n                sidebar-v2\n                &lt;div class=\"leaflet-sidebar-close\"&gt;&lt;i class=\"fa fa-caret-left\"&gt;&lt;/i&gt;&lt;/div&gt;\n            &lt;/h1&gt;\n            &lt;p&gt;A responsive sidebar for mapping libraries&lt;/p&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"leaflet-sidebar-pane\" id=\"messages\"&gt;\n            &lt;h1 class=\"leaflet-sidebar-header\"&gt;Messages&lt;div class=\"leaflet-sidebar-close\"&gt;&lt;i class=\"fa fa-caret-left\"&gt;&lt;/i&gt;&lt;/div&gt;&lt;/h1&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"leaflet-sidebar-pane\" id=\"profile\"&gt;\n            &lt;h1 class=\"leaflet-sidebar-header\"&gt;Profile&lt;div class=\"leaflet-sidebar-close\"&gt;&lt;i class=\"fa fa-caret-left\"&gt;&lt;/i&gt;&lt;/div&gt;&lt;/h1&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#24-define-the-sidebar-in-javascripthtml","title":"2.4 Define the sidebar in <code>Javascript.html</code>","text":"<ul> <li>Add the sidebar.</li> </ul> JavaScript.html<pre><code>// create sidebar\nvar sidebar = L.control.sidebar({\n    autopan: false,       // whether to maintain the centered map point when opening the sidebar\n    closeButton: true,    // whether t add a close button to the panes\n    container: 'sidebar', // the DOM container or #ID of a predefined sidebar container that should be used\n    position: 'left',     // left or right\n}).addTo(map);\n</code></pre> <ul> <li>Open the sidebar once the page loads</li> </ul> JavaScript.html<pre><code>document.addEventListener('DOMContentLoaded', function(){\n  // open sidebar\n  sidebar.open('home');\n});\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#25-customize-the-css","title":"2.5 Customize the CSS","text":"<p>I want to change the color used in the leaflet sidebar from the bright blue it is to a navy blue and use a specific font (in my case, to use Agency colors). Do do that, I first looked in the original CSS file from Leaflet Sidebar v2 to see how they styled it, and then I copy/pasted the relevant code into my local CSS file. (If you were using the full CSS file, you could simply make the change there, but this way will also work for projects you load from CDN).</p> <ul> <li>Add the following to your file <code>css.html</code>:</li> </ul> css.html<pre><code>.leaflet-sidebar {\n  font-family: Gill Sans MT, Verdana, sans-serif;\n}\n\n.leaflet-sidebar-tabs &gt; li.active, .leaflet-sidebar-tabs &gt; ul &gt; li.active {\n  color: #fff;\n  background-color: #002F6C; \n}\n\n.leaflet-sidebar-header { \n  background-color: #002F6C\n}\n</code></pre> <p>Because the custom CSS file loads last, any changes to the <code>css.html</code> file will override the changes in the original CSS file. </p>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#3-add-and-edit-map-controls","title":"3. Add and Edit Map Controls","text":""},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#31-update-zoom-control-location","title":"3.1 Update zoom control location","text":"<p>You'll notice that the map has a single control, the zoom control. We enabled the zoom control when we initialized the map by setting the option <code>zoomControl</code> to <code>true</code>. </p> <ul> <li>Move the zoom control to the right by setting the option <code>zoomControl</code> to false and adding a zoom control to the top right.</li> </ul> JavasScript.html<pre><code>// create map\nvar map = L.map('map', {\n    center: [16, -90.8],\n    zoom: 7,\n    maxZoom: 20,\n    zoomControl: false,  // update this line\n  });\n\n// add zoom control\nL.control.zoom({position: 'topright'}).addTo(map);\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#32-create-a-default-extent-or-home-button","title":"3.2 Create a default extent (or home) button","text":"<p>A home button for the map that returns the user to the default extent. </p> <ul> <li>Navigate to the project defaultextent. </li> <li>Copy the contents from <code>dist/leaflet.defaultextetnt.js</code> into a file <code>default-extent.js.html</code> in the GAS project. </li> </ul> <p>This package uses a .png file as an icon, and we'd like to instead use a Bootstrap icon. To do this, we'll modify the file <code>default-extent.js.html</code> to load an icon. </p> <ul> <li>Find the code near the top of the file that looks like this:</li> </ul> default-extent.js.html<pre><code>L.Control.DefaultExtent = L.Control.extend({\n    options: {\n      position: 'topleft',\n      text: 'Default Extent',\n      title: 'Zoom to default extent',\n      className: 'leaflet-control-defaultextent'\n    }, ...\n</code></pre> <ul> <li>Change the <code>text</code> property to the html tag for a home icon using Bootstrap icons:</li> </ul> default-extent.js.html<pre><code>L.Control.DefaultExtent = L.Control.extend({\n    options: {\n      position: 'topleft',\n      text: '&lt;i class=\"bi bi-house-fill\"&gt;&lt;/i&gt;',  // update this line\n      title: 'Zoom to default extent',\n      className: 'leaflet-control-defaultextent'\n    }, ...\n</code></pre> <ul> <li>Add the widget either through specifying <code>defaultExtentControl: true</code> when initializing the map or adding it after the map is intialized (which provides access to the position options).</li> </ul> JavaScript.html<pre><code>// add default extent\nL.control.defaultExtent({position: 'topright'}).addTo(map);\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#4-plot-geojson-data","title":"4. Plot GeoJSON data","text":"<p>GeoJSON is a convenient file type for geospatial data that can be read by Leaflet and many other geospatial packages. Here, we'll walk through the process to host GeoJSON online, read it into our project, and plot it on the map.</p> <p>Tip</p> <p>For this tutorial, we will use the GeoJSON file <code>municipalities.json</code> stored in the gtm-apps repository on GitHub under the <code>data/spatial</code> directory. To use your own file, you may either store the GeoJSON data in an HTML file in the project within a <code>&lt;script&gt;</code> tag or create your own repository on GitHub. There are of course other options, use whatever works best for you.</p>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#41-fetch-geojson-data","title":"4.1 Fetch GeoJSON data","text":"<ul> <li>Write a function to fetch GeoJSON data from the web using the Fetch API.</li> </ul> JavaScript.html<pre><code>// fetch GeoJson\nfunction fetchGeoJson(src, func){\n  fetch(src, {\n    method: 'GET'\n  })\n  .then(response =&gt; response.json())\n  .then(json =&gt; {\n    func(json);\n  })\n  .catch(error =&gt; console.log(error.message));  // END FETCH\n};\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#42-map-geojson-data","title":"4.2 Map GeoJSON data","text":"<p>Leaflet has a helpful <code>geoJSON</code> class available for interacting with GeoJSON data. See this Leaflet guidance for a helpful walk through. The following function will plot the GeoJSON in a transparent grey outline. There are many helpful options for styling GeoJSON described in the Leaflet guidance.</p> <ul> <li>Write a function to map the GeoJSON data.</li> </ul> JavaScript.html<pre><code>// map Municipalities\nfunction mapGeoJSON(json){\n  myGeoJson = L.geoJSON(json, {\n    style: function(feature) {\n      return {color: 'grey', weight: 0.5, fillOpacity: 0};\n    }\n  }).addTo(map)\n};\n</code></pre>"},{"location":"pages/knowledge/tutorials/leaflet-tutorial/#43-bring-it-together","title":"4.3 Bring it together","text":"<p>Now we can combine the two functions to fetch the data and, after the data has been fetched, map the data. </p> <ul> <li>Add a variable to store the URL to the GeoJSON data. </li> <li>Call the <code>fetchGeoJson</code> function with <code>mapGeoJSON</code> as a callback function within the <code>DOMContentLoaded</code> event listener. </li> </ul> JavaScript.html<pre><code>// store municipality layer\nvar muniGeoJsonURL = 'https://raw.githubusercontent.com/eanderson-ei/gtm-apps/main/data/spatial/municipalities.json'\n\n// fetch GeoJson\nfunction fetchGeoJson(src, func){\n  fetch(src, {\n    method: 'GET'\n  })\n  .then(response =&gt; response.json())\n  .then(json =&gt; {\n    func(json);\n  })\n  .catch(error =&gt; console.log(error.message));  // END FETCH\n};\n\n\n// map Municipalities\nfunction mapMunicipalities(json){\n  muniGeoJson = L.geoJSON(json, {\n    style: function(feature) {\n      return {color: 'grey', weight: 0.5, fillOpacity: 0};\n    },\n  }).addTo(map)\n};\n\n\n// load DOM\ndocument.addEventListener('DOMContentLoaded', function(){\n\n  // map municipalities\n  fetchGeoJson(muniGeoJsonURL, mapMunicipalities);\n});\n</code></pre> <p>You should now have a map that displays the municipalities layer on load. The next challenge, to recreate the full Map Viewer application, would be to style the municipalities layer based on data that is joined from a Google Sheet. See the Google Apps Script Tutorial to learn how to read data from a Google Sheet. The Map Viewer application has a number of complex challenges that you'll need to overcome, including joining the spatial and tabular data, creating a choropleth map, and adding a legend. Those challenges are beyond the scope of this tutorial, but you can find the full code for the Map Viewer in the gtm-apps repository.</p> <p>Below is the full code for this tutorial, with the exception of the third-party libraries used.</p> code.gs<pre><code>function doGet(e) {\n  return HtmlService.createTemplateFromFile('map').evaluate();\n};\n\nfunction include(filename) {\n  return HtmlService.createHtmlOutputFromFile(filename)\n      .getContent();\n};\n</code></pre> map.html<pre><code>&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n    &lt;title&gt;Leaflet Demo&lt;/title&gt;\n    &lt;!-- Import Bootstrap --&gt;\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx\" crossorigin=\"anonymous\"&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap-icons@1.9.1/font/bootstrap-icons.css\"&gt;\n    &lt;!-- Import Leaflet --&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://unpkg.com/leaflet@1.8.0/dist/leaflet.css\"\n   integrity=\"sha512-hoalWLoI8r4UszCkZ5kL8vayOGVae1oxXe/2A4AO6J9+580uKHDO3JdHb7NzwwzK5xr/Fs0W40kiNHxM9vyTtQ==\"\n   crossorigin=\"\"/&gt;\n    &lt;!-- Include Sidebar v2 --&gt; \n    &lt;?!= include('leaflet-sidebar-v2.css') ?&gt;\n\n    &lt;!-- Include custom CSS --&gt;\n    &lt;?!= include('css'); ?&gt;\n  &lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div id=\"map\"&gt;&lt;/div&gt;\n\n    &lt;div id=\"sidebar\" class=\"leaflet-sidebar collapsed\"&gt;\n      &lt;!-- Nav tabs --&gt;\n      &lt;div class=\"leaflet-sidebar-tabs\"&gt;\n          &lt;ul role=\"tablist\"&gt; &lt;!-- top aligned tabs --&gt;\n              &lt;li&gt;&lt;a href=\"#home\" role=\"tab\"&gt;&lt;i class=\"fa fa-bars\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n              &lt;li class=\"disabled\"&gt;&lt;a href=\"#messages\" role=\"tab\"&gt;&lt;i class=\"fa fa-envelope\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n              &lt;li&gt;&lt;a href=\"#profile\" role=\"tab\"&gt;&lt;i class=\"fa fa-user\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;ul role=\"tablist\"&gt; &lt;!-- bottom aligned tabs --&gt;\n              &lt;li&gt;&lt;a href=\"#settings\" role=\"tab\"&gt;&lt;i class=\"fa fa-gear\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;\n          &lt;/ul&gt;\n      &lt;/div&gt;\n\n      &lt;!-- Tab panes --&gt;\n      &lt;div class=\"leaflet-sidebar-content\"&gt;\n          &lt;div class=\"leaflet-sidebar-pane\" id=\"home\"&gt;\n              &lt;h1 class=\"leaflet-sidebar-header\"&gt;\n                  sidebar-v2\n                  &lt;div class=\"leaflet-sidebar-close\"&gt;&lt;i class=\"fa fa-caret-left\"&gt;&lt;/i&gt;&lt;/div&gt;\n              &lt;/h1&gt;\n              &lt;p&gt;A responsive sidebar for mapping libraries&lt;/p&gt;\n          &lt;/div&gt;\n\n          &lt;div class=\"leaflet-sidebar-pane\" id=\"messages\"&gt;\n              &lt;h1 class=\"leaflet-sidebar-header\"&gt;Messages&lt;div class=\"leaflet-sidebar-close\"&gt;&lt;i class=\"fa fa-caret-left\"&gt;&lt;/i&gt;&lt;/div&gt;&lt;/h1&gt;\n          &lt;/div&gt;\n\n          &lt;div class=\"leaflet-sidebar-pane\" id=\"profile\"&gt;\n              &lt;h1 class=\"leaflet-sidebar-header\"&gt;Profile&lt;div class=\"leaflet-sidebar-close\"&gt;&lt;i class=\"fa fa-caret-left\"&gt;&lt;/i&gt;&lt;/div&gt;&lt;/h1&gt;\n          &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;!-- Import Popper (for Bootstrap) --&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.5/dist/umd/popper.min.js\" integrity=\"sha384-Xe+8cL9oJa6tN/veChSP7q+mnSPaj5Bcu9mPX5F5xIGE0DVittaqT5lorf0EI7Vk\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;!-- Import Bootstrap --&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.min.js\" integrity=\"sha384-ODmDIVzN+pFdexxHEHFBQH3/9/vQ9uori45z4JjnFsRydbmQbmL5t1tQ0culUzyK\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;!-- Import Leaflet --&gt;\n    &lt;script src=\"https://unpkg.com/leaflet@1.8.0/dist/leaflet.js\"\n   integrity=\"sha512-BB3hKbKWOc9Ez/TAwyWxNXeoV9c1v6FIeYiBieIWkpLjauysF18NzgR1MBNBXf8/KABdlkX68nAhlwcDFLGPCQ==\"\n   crossorigin=\"\"&gt;&lt;/script&gt;\n    &lt;!-- Load Sidebar v2 --&gt;\n    &lt;?!= include('leaflet-sidebar-v2.js') ?&gt;\n\n    &lt;!-- Load Default Extent (home button) --&gt;\n    &lt;?!= include('default-extent.js') ?&gt;\n\n    &lt;!--Include custom JavaScript--&gt;\n    &lt;?!= include('JavaScript') ?&gt; \n  &lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre> css.html<pre><code>&lt;style&gt;\n  #map { height: 100vh; }\n\n  .leaflet-sidebar {\n    font-family: Gill Sans MT, Verdana, sans-serif;\n  }\n\n  .leaflet-sidebar-tabs &gt; li.active, .leaflet-sidebar-tabs &gt; ul &gt; li.active {\n    color: #fff;\n    background-color: #002F6C; \n  }\n\n\n  .leaflet-sidebar-header { \n      background-color: #002F6C\n  }\n&lt;/style&gt;\n</code></pre> JavaScript.html<pre><code>&lt;script&gt;\n  // store municipality layer\n  var muniGeoJsonURL = 'https://raw.githubusercontent.com/eanderson-ei/gtm-apps/main/data/spatial/municipalities.json';\n\n  // create map\n  var map = L.map('map', {\n      center: [16, -90.8],\n      zoom: 7,\n      maxZoom: 20,\n      zoomControl: false,\n    });\n\n  // set base map\n  L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {\n    attribution: '\u00a9 OpenStreetMap'\n  }).addTo(map);\n\n  // add zoom control\n  L.control.zoom({position: 'topright'}).addTo(map);\n\n  // add default extent\n  L.control.defaultExtent({position: 'topright'}).addTo(map);\n\n  // create sidebar\n  var sidebar = L.control.sidebar({\n      autopan: false,       // whether to maintain the centered map point when opening the sidebar\n      closeButton: true,    // whether t add a close button to the panes\n      container: 'sidebar', // the DOM container or #ID of a predefined sidebar container that should be used\n      position: 'left',     // left or right\n  }).addTo(map);\n\n\n  // fetch GeoJson\n  function fetchGeoJson(src, func){\n    fetch(src, {\n      method: 'GET'\n    })\n    .then(response =&gt; response.json())\n    .then(json =&gt; {\n      func(json);\n    })\n    .catch(error =&gt; console.log(error.message));  // END FETCH\n  };\n\n\n  // map Municipalities\n  function mapMunicipalities(json){\n    muniGeoJson = L.geoJSON(json, {\n      style: function(feature) {\n        return {color: 'grey', weight: 0.5, fillOpacity: 0};\n      },\n    }).addTo(map)\n  };\n\n\n  // load DOM\n  document.addEventListener('DOMContentLoaded', function(){\n    // open sidebar\n    sidebar.open('home');\n    // map municipalities\n    fetchGeoJson(muniGeoJsonURL, mapMunicipalities);\n  });\n\n&lt;/script&gt;\n</code></pre> Additional Resources <ul> <li>Mapster GeoJSON | Mapping in LeafletJS YT Video</li> </ul>"},{"location":"pages/knowledge/tutorials/other-tutorials/","title":"Additional Tutorials","text":""},{"location":"pages/knowledge/tutorials/other-tutorials/#python-tutorials","title":"Python Tutorials","text":"<ul> <li> <p>SPSS Tutorial: This tutorial will walk you through the process of reading data provided in the SPSS format (.sav), a common data format for government data using Python. </p> </li> <li> <p>Coordinate Reference Systems: This tutorial will introduce you to the use of the Python library <code>geopandas</code> for converting geospatial files to the Coordinate Reference System (CRS) WGS 84, the preferred CRS of the Agency.</p> </li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/","title":"Tableau Tutorial","text":"<p>Make a map using a Google Sheet and shapefile</p> <p>In this tutorial, you will prepare a Tableau Dashboard with a map and chart by connecting to a Google Sheet and a shapefile (in zip format).</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#1-prepare-data","title":"1. Prepare Data","text":"<p>Open Tableau (Desktop or Public) and connect to the tabular and spatial data.</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#11-connect-to-google-sheet","title":"1.1 Connect to Google Sheet","text":"<ul> <li>Copy this Google Sheet into your Google Drive (requires USAID account; if you do not have access to this Sheet, you can follow this tutorial with your own dataset).</li> <li>Under 'To a Server', select \"Google Sheets\". Tableau will attempt to connect to the Server. If you use multiple Google profiles (i.e., you have a separate profile for personal and work), make sure the correct profile is active (open your browser and load a tab under that profile) before selecting \"Google Sheets\". Select the appropriate account and 'Allow' access.</li> <li>Select the Google Sheet you want to connect to from the list shown in the 'Select Your Google Sheet'. You should have access to sheets owned by you and sheets shared with you.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#12-connect-to-shapefile","title":"1.2 Connect to Shapefile","text":"<ul> <li>First, you must prepare or download a shapefile. The shapefile must be joinable to the Google Sheet you selected (i.e., have at least one shared field). This shared field will be a unique identifier for the geometry you want to display. For this tutorial, save the departments.zip zipped folder to your local computer. </li> </ul> <p>Note</p> <p>You can find geometry data in the ArcGIS Online hub under Content &gt; Organization. Filter by Layer Files to find suitable data. Hover over the image (often just a view of the world) and select Download, or open the file and Download as Shapefile.</p> <ul> <li> <p>In the 'Data Source' tab of Tableau (which should be open now), under Connections, select \"Add\". </p> </li> <li> <p>Select \"Spatial File\" and navigate to the zipped file containing the shapefile you want to use. </p> </li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#13-join-shapefile-and-google-sheet","title":"1.3 Join Shapefile and Google Sheet","text":"<p>In this step, we want to attach a geometry to each attribute we want to illustrate.</p> <ul> <li> <p>In the Data Source tab of Tableau (which should be open now), remove the spatial data file from the GUI window. Drag a table that can be joined to the spatial data from the Google Sheet into the GUI window. Double click the resulting box to open the Join window.</p> </li> <li> <p>Drag the shapefile into the Join window. Select the join fields from the left and right data sources. </p> </li> </ul> <p>Tip</p> <p>We have found that a Join, rather than a Relationship, is best for spatial data. For some reason (which may be a bug addressed in future Tableau releases), numeric fields in shapefiles are read as string type fields and cannot be joined. If necessary, use the dropdown on the shapefile's join field to convert to integer using a Join calculation <code>int([Codigo1 (muni geo1)])</code>.</p> <p></p> <p>In the above example, we've added a table of activities and the associated departments and used a full outer join to join the shapefile to the data. In this way, we don't lose any activity data and we show all departments, even if no activities are currently taking place there. </p> <ul> <li>To be extra speedy, start by double-clicking the table in the tabular data that can be joined to the spatial data. The Join GUI should open. Next, Add a Connection and navigate to the file. The file should be joined automatically. Double click the venn diagram to edit the join type.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#2-create-map","title":"2. Create map","text":"<p>Next we will symbolize the data, in our case by showing the number of activities per department.</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#21-add-geometry","title":"2.1 Add Geometry","text":"<ul> <li>Navigate to the Sheet tab. Rename it by clicking the tab handle.</li> <li>Locate the geometry feature in the right sidebar associated with the shapefile you are using. It should be indicated by a globe icon and be named Geometry. </li> <li>Drag that feature into the canvas. A map should appear showing the spatial data.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#22-specify-a-detail-by-which-to-disaggregate","title":"2.2 Specify a Detail by which to Disaggregate","text":"<ul> <li>You must specify as a Detail an attribute within the spatial file to disaggregate measures by. Most often, this will be the name or other identifier of the features you are interested in. In other words, if you are mapping information by country, drag the country name attribute to the Detail pane. In our case, we will drag the Department name field in.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#23-symbolize-data","title":"2.3 Symbolize Data","text":"<ul> <li>Next, drag the measure or attribute by which you would like to symbolize to the Color option of the Mark pane. This will create a choropleth map. You may need to use the dropdown by the attribute pill in the Mark pane to convert it to an attribute or measure, or specify it as continuous or discrete. </li> </ul> <ul> <li>Change the colorscheme by clicking the color icon next to CNT(activities) in the Marks card and create a custom sequential (or diverging) colorscale using USAID colors (#002F6C is the standard blue).</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#3-connect-additional-tables","title":"3. Connect additional tables","text":"<p>Now we will connect additional tables to allow us to summarize additional data available in the Google Sheet.</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#31-add-additional-table-relations","title":"3.1 Add additional table relations","text":"<ul> <li>Navigate to the Data Source tab. Close the Join window if necessary.</li> <li>Select the Google Sheet in the Connections pane.</li> <li>Drag the desired table into the canvas. </li> <li>Click the join spaghetti to select the relationship. Select the shared field for each table.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#32-symbolize-additional-data","title":"3.2 Symbolize additional data","text":"<ul> <li> <p>Navigate back to the sheet with the map.</p> </li> <li> <p>We will add another map layer to symbolize a different attribute. Drag the Geometry attribute over the canvas until a popup in the upper left says ''Add a Marks Layer\". Drop the Geometry attribute in that card.</p> </li> </ul> <p>Tip</p> <p>Instead of adding another layer, you could display these data as a Tooltip or Label. Later in this tutorial, we will look at bivariate choropleths as a way to compare two datasets by spatial feature.</p> <ul> <li> <p>Add the same Detail you did in step 2.</p> </li> <li> <p>Add a measure or attribute to symbolize.</p> </li> <li> <p>Rename both Marks layers so they can be distinguished easily. You can also click on the icon to the left of the name to hide one or the other. When deployed, only one layer should be visible.</p> </li> </ul> <p>Note</p> <p>Notice the layer control icon is now available on the map (it looks like a stack of squares). You (or your users) can now turn layers on and off with the layer control.</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#33-add-a-filter","title":"3.3 Add a Filter","text":"<p>A filter will allow us to filter data by an attribute of our choice.</p> <ul> <li>Drag an attribute to the Filter pane. A dialog box should pop up allowing you to configure the filter. I recommend using all (including null) at least for now.</li> <li>Click the dropdown on the filter pill and select 'Show Filter'. The filter should appear in the right sidebar. You can adjust many options for the filter, explore by clicking the dropdown to the right of the filter's card. Set dependent filters by setting the subdominant filter to show 'Only Relevant Values'. For example, to filter first by Lead Office and next by Activity name, set the Activity name filter as 'Only Relevant Values'. When you select the Lead Office, only the Activities by that Office will be available in the Activities filter.</li> <li>To remove specific values from a Filter, use a Filter Set. Right click the attribute in the data pane and select Create Set. Select all options except the ones to remove. Then remove the original filter (if needed) and add the Filter Set to the Filter shelf.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#4-build-a-dashboard","title":"4. Build a Dashboard","text":""},{"location":"pages/knowledge/tutorials/tableau-tutorial/#41-add-map-to-layout","title":"4.1 Add Map to Layout","text":"<ul> <li>Click the New Dashboard tab at the bottom of the window to create a new dashboard</li> <li>Drag the Activity_Map sheet onto the canvas. The map should display and the filters should be attached to the map area. Ideally, the filters would be added within a Vertical tiled container. If not, you may need to drag a new Vertical object next to the map and redistribute the filter elements.</li> <li>The Item hierarchy (click the Layout tab of the left sidebar) should look like this (excluding the Show/Hide element for now). This will become important when creating the Collapsible Filter Bar. The Activity_Map and Vertical Container storing all filters must be in the same Horizontal (not Tiled) container for the map to auto-expand when the Filter Bar is collapsed.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#42-stylize","title":"4.2 Stylize","text":"<ul> <li>On the top menu, select Format Workbook and change all fonts to Gill Sans MT. You can also edit fonts at the Dashboard level, which provides more options.</li> <li>Under the Dashboard tab in the left sidebar, set the size as 'Range' and increase the max width to a comfortable size that fills the screen without over-expanding. Tableau will use these min and max sizes as media queries to re-arrange content depending on screen size. Nicely, it will stack content for narrow screen sizes like phones. A special 'Phone' view is automatically created. Click it to see how the auto layout looks, and you can customize this if you need to. You can use the Device Preview button to preview the Dashboard on a variety of different standard device types.</li> <li>You can also change padding (Inner Padding), margin (Outer Padding), background color, and borders around the Dashboard as a whole or individual containers.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#42-add-collapsible-filter-bar","title":"4.2 Add Collapsible Filter Bar","text":"<p>Allowing the user to collapse the filter bar provides a nice user experience.</p> <ul> <li>Select the Vertical container holding the filter elements (double click on the handle above an individual filter element, or use the Layout tab of the left sidebar). Click the down arrow  (or right click the element) and select 'Add show/hide button'. This button will be added as a floating element. I decided to add a small vertical container across the top of the Dashboard to provide space for this button, and I can also use the space to title the map while keeping the filters all top aligned. In the button's options, you can change how it looks and add a tool tip like \"Click here to expand all filters\".  </li> </ul> <p></p> <p>Tip</p> <p>To add additional filters to the Dashboard, first add the filter to the Activity_Map worksheet by dragging the field from the left sidebar into the Filter card. Next, select the drop down arrow on the filter pill and select 'Show Filter'. Customize the filter in the Map as needed (e.g., change to multi-value dropdown type and select the option for Only Relevant Values). Next, go to the Dashboard and select the map element. Use the drop down arrow and hover over filter from the menu, a sub menu of available filters will be shown. Click on the newly created filter to add it to the filter pane. You may need to drag it to the appropriate container.</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#5-add-table-of-activities","title":"5. Add Table of Activities","text":"<p>Next we'll add a table showing all activities that will filter based on filters and selections on the map.</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#51-create-table","title":"5.1 Create Table","text":"<ul> <li>Create a new Worksheet and rename it Activity_Table.</li> <li>Drag the columns you would like in the table from the left sidebar to the 'Rows' </li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#52-stylize-the-table","title":"5.2 Stylize the Table","text":"<ul> <li>Add alternating row shading by clicking 'Format' in the top menu and selecting 'Shading'. Select the Rows tab from the newly appeared left sidebar. Select the color for alternating rows from \"Pane\". Ensure band size is set to 1. Level should be 0.</li> <li>Increase padding between rows under Format &gt; Cell Size &gt; Taller.</li> <li>Set the Table to 'Fit Width' so that it will expand with the viewport (top menu bar where it says 'Standard').</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#53-add-a-tooltip","title":"5.3 Add a Tooltip","text":"<p>Tables are set up so that hovering will expand the content of a cell, which is helpful when the text exceeds the column width. However, it means that a tooltip cannot easily be added to a table to show additional information on hover (although it is possible with some trouble). Instead, we'll create a dedicated column for the tooltip called 'More info'. This approach has the added benefit of solving for the 'Abc' placeholder column at the same time</p> <ul> <li>Under Analysis, select Create Calculated Field. Set the Calculated field to be the string \"More info\".</li> <li>Drag the calculated field into the \"Text\" Mark.</li> <li>Drag the fields that you would like to show in the tooltip into the Tooltip card.</li> <li>Double click the Tooltip card and customize the tooltip. Use the 'Insert' to add references to the attributes you want in the tooltip (you cannot just type open and close brackets).</li> <li>You can change the font for this field by selecting Format &gt; Font, select the 'More info' field from the dropdown menu for Fields, and under 'Pane' set the font to italics or however you would like to set it apart.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#54-add-to-the-dashboard","title":"5.4 Add to the Dashboard","text":"<ul> <li>Add a new container for the Table and pull the table into the container.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#55-add-a-search-bar","title":"5.5. Add a Search Bar","text":"<p>A search bar can help when you have a long table that you need to find specific rows within. When the table is also a filter (see step 6 below) the combination of search and select allows for quick filtering. We'll also add an informational tooltip to help users with this feature.</p> <ul> <li>In the Dashboard, select the Table element and click the drop down arrow (or right click the table). Hover over 'Filter' and select the field which you want to use as a search field. A filter should appear.</li> <li>Change the filter to 'Wildcard' type.</li> <li>Position the search bar in the Dashboard. Leave room to the right for an informational icon.</li> </ul>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#56-add-informational-icon","title":"5.6 Add informational icon","text":"<p>This step can be duplicated for any number of informational icons that have tooltips. </p> <ul> <li>Create a new worksheet and name it 'Search_Info'</li> <li>Drag any attribute to the canvas. I created a new calculated field called 'Blank', but it really can be anything.</li> <li>In the Marks card, change the mark to a Shape. Select 'Shape' and navigate to the shape you would like to use. If you want to add your own shape (I used a search glass icon), download a free icon image from the web and save it in the folder 'My Tableau Repository' in your 'My Documents' folder under 'shapes/'. Then open the Shape dialog box again, select 'More Shapes', and hit 'Reload Images'. You should see the image in the Custom Palette.</li> <li>Remove the column header (click the dropdown in the Rows button and uncheck Show Header)</li> <li>Remove the borders from the top and bottom of the element under Format &gt; Borders &gt; Row Divider &gt; Pane = None.</li> <li>Double click the Tooltip icon in the Marks card and update the text with the info you want to include. Uncheck 'Include command buttons' and 'Allow selection by category'.</li> <li>Return to the Dashboard worksheet. Drag the Search_Info worksheet to where you would like it on the Dashboard. Under the container elements, adjust the icon to fit Entire View.</li> </ul> <p>Tip</p> <p>Another option for adding information and instruction is to use a show/hide button on an image that creates an image overlay with instructions for the Dashboard. Unfortunately, this will only work with fixed size dashboards as the image has to be static.</p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#6-set-up-cross-filtering","title":"6. Set up Cross-filtering","text":"<ul> <li>Under Dashboard menu, select Actions. Add Action (select Filter) and set 'Run action on' to \"Select\"and then below it under 'Clearing the selection will' select \"Show All Values\". Click OK. The source and target should both be Activity_Map and Activity_Table, which will allow you to use either the map or the table to filter records.</li> </ul> <p>Note</p> <p>In Dashboard mode, you can connect filters across sheets by selecting (in the filter card) Apply to Worksheets and selecting an option within the dropdown menu. </p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#conclusion","title":"Conclusion","text":"<p>That's it! You should now have a functional map-forward dashboard that allows all kinds of filtering to help find the information you need. </p>"},{"location":"pages/knowledge/tutorials/tableau-tutorial/#bonus-embed-in-a-google-site","title":"Bonus: Embed in a Google Site","text":"<p>For extra credit, embed the Dashboard in a Google Site. </p> <p>See this guidance for options on import of the Dashboard.</p>"},{"location":"pages/usage/onboard-activity/","title":"Onboard an Activity","text":"<p>Tip</p> <p>Before onboarding an Activity to Atlas, ensure that the Activity contract includes the required provisions. If the Mission does not yet have standard Solicitation and Award Language that meets the requirements of Atlas, see the Playbook for guidance on creating the enabling conditions for successful roll out of Atlas with partners. Avoid \"voluntary\" data requests, as even voluntary requests can create undue burden on partners.  Always include the A/COR in communications with partners.</p> <p>Each Activity should be onboarded to Atlas during initial start up. Ideally, the Data Steward will coordinate with the Technical Team as early as Activity Design.</p> <p>Onboarding an Activity to Atlas will simplify the reporting process and ensure Activity documentation, Intellectual Works and the data behind them are accessible to the Data Steward and thus the Mission. More importantly, onboarding an Activity is an opportunity to engage with Technical Team and partners to help them manage data as an asset and use data responsibly.</p>"},{"location":"pages/usage/onboard-activity/#set-up-the-activity-folder","title":"Set up the Activity Folder","text":"<p>After inputting the Activity in the Activity Database, create the Activity Folder using the Activity Folder Template in Atlas.</p> <ol> <li>Create a new folder the Atlas <code>Activity Folders/</code> directory. Use the naming convention <code>1234 - Activity Name</code>, replacing <code>1234</code> with the Activity ID from the Activity Database and the <code>Activity Name</code> with the Activity's short name.</li> <li>Using the <code>copy-folder</code> script in the Data Steward Admin Tool (located at script.google.com), run the <code>runCopyFolderFunction</code> to copy the contents of the Activity Folder Template to the newly created Activity folder. Update the <code>source</code> variable within the function to the folder ID of the Activity Folder template (Admin &gt; Modules &gt; Activity Folder [template]). Update the <code>destination</code> variable to the folder ID of the newly created Activity folder in the <code>Activity Folders/</code> directory.</li> <li>Update the Activity Database with the link to the Activity Folder.</li> <li>Log the links to the Activity Folder and Activity Location Data Tracker in the Atlas Data Inventory.</li> </ol>"},{"location":"pages/usage/onboard-activity/#activity-post-award-conference","title":"Activity Post-Award Conference","text":"<p>The Activity Post-Award Conference is an important opportunity to introduce the partner to the data collection templates and reporting process of Atlas. We recommend including a representative of the MEL team to discuss all aspects of data collection, management and reporting. </p> <p>During this conference, the Data Steward will</p> <ul> <li>Review the components of the Data Management Plan and its role in relation to the Activity Monitoring, Evaluation and Learning Plan (AMELP)</li> <li>Review the Mission's Data Standards</li> <li>Introduce the partner to the Activity Folder and its contents</li> <li>Workshop with the partner to develop a rough Data Inventory</li> <li>Determine the appropriate level of geographic detail for Activity Location Data</li> </ul>"},{"location":"pages/usage/onboard-activity/#ongoing-support","title":"Ongoing Support","text":"<p>The Data Steward will provide ad-hoc support to partners during the lifetime of the Activity as needed. Additionally, the Data Steward will cultivate opportunities for partners to learn best practices and collaborate with other partners.</p>"},{"location":"pages/usage/overview/","title":"Usage Overview","text":"<p>Atlas runs automated scripts overnight to pipe data across applications. No action is required to keep each application up to date. An email will be sent to the Data Steward in the case that these automated processes fail for any reason. </p> <p>The following sections describe how to complete standard workflows with Atlas.</p>"},{"location":"pages/usage/overview/#common-workflows","title":"Common Workflows","text":"<ul> <li>Update Thematic Database: update thematic datasets or add a new thematic dataset to the Thematic Database.</li> <li>Update Activity Database: manage periodic updates to the Activity Database.</li> <li>Onboard an Activity: set up an Activity Folder for an Activity and support the partner to use Atlas tools and templates.</li> </ul>"},{"location":"pages/usage/overview/#advanced-workflows","title":"Advanced Workflows","text":"<ul> <li>Update the Mission Database: Upload data from Google Workplace to the Mission Database.</li> <li>Update Atlas documentation: Edit or expand this documentation.</li> </ul>"},{"location":"pages/usage/update-activitydb/","title":"Update Activity Database","text":"<p>Note</p> <p>This section is draft, pending development of the Activity Database 2.0.</p>"},{"location":"pages/usage/update-activitydb/#how-to-update","title":"How to Update","text":""},{"location":"pages/usage/update-activitydb/#add-field","title":"Add field","text":"<p>Do not add fields directly to the Activity Database Google Sheet. Add fields to the FormGen sheet. The new field will be appended to the Activity Database Google Sheet. Column order in this sheet does not matter.</p> <ol> <li>Open FormGen</li> <li>Add a row in the appropriate location</li> <li>Update all field attributes</li> <li>Insert a test record (DO NOT edit an existing record) to see the new field in the Activity Database </li> <li>Delete the inserted empty record</li> <li>Create a named range for the new column</li> <li>Select the column in the Activity Database Google Sheet</li> <li>Type the name in the upper left box; replace spaces with underscores</li> <li>Update the <code>Push Query - All</code> tab in the Activity Database with the new field</li> <li>Add the field to the Queries for any applications you need it in</li> <li>For the Thematic Database:<ul> <li>Add the field to the <code>Query - Thematic Database Query</code> function range</li> <li>Add the field to the activity data <code>_import_activities</code> query</li> <li>If needed, create a new tab to normalize any comma or pipe separated field</li> <li>Log the field in the <code>DEFINITIONS</code> tab</li> <li>Run the <code>generate_activity_sql</code> function in the Data Steward Admin Tool</li> <li>Open PgAdmin</li> <li>Create a new column to store the data (or simply drop the affected tables to reload with the new schema for tables using truncate and load)</li> <li>Reload all tabular data</li> </ul> </li> </ol>"},{"location":"pages/usage/update-docs/","title":"Update Atlas Documentation","text":"<p>We encourage you to expand and customize this documentation for your own use. To suggest edits to this documentation, please open an issue on this repository in GitHub.  </p>"},{"location":"pages/usage/update-docs/#fork-this-repo","title":"Fork this repo","text":"<p>The best way to update Atlas documentation is to fork this repository (requires a GitHub account). Once you  have forked the repository, follow the process below to serve this documentation in your own GitHub pages site.</p> <ul> <li>Edit the <code>edit_uri</code> variable in the file <code>mkdocs.yml</code> (line 3) by changing the user name in the url from <code>eanderson-ei</code> to your user name. This will allow you to edit your fork of the documentation.</li> </ul> <pre><code>edit_uri: https://github.com/&lt;your username&gt;/gtm-dms-alpha/edit/main/docs/\n</code></pre> <ul> <li>In your repo on GitHub, open the Settings tab and select Pages. </li> <li>In the Build and deployment section, under Source select \"Deploy from a branch\". </li> <li>Under Branch, select the <code>gh-pages</code> branch. </li> <li>Click Save.</li> </ul> <p>GitHub requires a few seconds to create the site, but soon you'll find the link to the site under Settings &gt; Pages. This is also where you can unpublish a site.</p> <p>You will now be able to edit the documentation directly by clicking the edit icon for each page.</p>"},{"location":"pages/usage/update-docs/#editing-pages","title":"Editing pages","text":"<ul> <li>To edit a page, first click the edit icon in the top right of the page (the home page is the only page that does not display this icon). </li> <li>Make the desired edits in the Markdown editor </li> <li>Click the Commit changes button to save your changes. </li> <li>A dialog box will open. Optionally, edit the commit message and extended description.</li> <li>Ensure that Commit directly to the <code>main</code> branch is selected.</li> <li>Click the Commit changes button.</li> </ul> <p>After a few minutes, the documentation will be updated. </p> <p>To learn more about the Markdown syntax check out the Markdown Guide. Some syntax is specific to the Material for MkDocs package and extensions. </p>"},{"location":"pages/usage/update-docs/#download-documentation","title":"Download documentation","text":"<p>Alternatively, you may download the contents of this repository as a zip file. On the repository home page, click the green Code button and select Download ZIP. Documentation is stored in the <code>docs/pages</code> directory. We recommend editing documents in a Markdown editor, however documents may be edited as plain text.</p>"},{"location":"pages/usage/update-docs/#update-mission-database-schema","title":"Update Mission Database Schema","text":"<p>The Mission Database schema is available through dbdocs.io. The link to the documentation is included in the <code>mission-database.md</code> file in <code>docs/components</code>.</p>"},{"location":"pages/usage/update-docs/#create-ddl-files","title":"Create DDL files","text":"<p>Using the Data Steward Admin Tool, create the SQL files to specify the DDL from each data source. The function is <code>write_ddl</code> for each <code>.gs</code> file.</p> <p>Download the files from the Data Admin SQL folder on Google Drive and save them to the <code>sql/ddl</code> directory in the project repository.</p>"},{"location":"pages/usage/update-docs/#upload-to-dbdocs","title":"Upload to dbdocs","text":"<ol> <li>Go to dbdocs.io and load the create_&lt;&gt;.sql files for the schema you want to display. Make sure <code>append</code> is selected.</li> <li>Copy the contents to the <code>dbdiagram.dbml</code> file in root directory.</li> <li>Upload to dbdocs.</li> </ol> <pre><code>dbdocs build dbdiagram.dbml\n</code></pre>"},{"location":"pages/usage/update-missiondb/","title":"Update Mission Database","text":"<p>Note</p> <p>This workflow is only for installations of Atlas that include the optional Mission Database, a PostgreSQL database.</p> <p>The Data Steward will update the Mission Database on a regular basis to ensure that data are up-to-date and available for query. The Data Steward Admin Tool produces <code>.sql</code> files in the Data Admin SQL folder in Atlas every 24 hours. Upload these file to the Mission Database each day. </p>"},{"location":"pages/usage/update-missiondb/#daily-update","title":"Daily update","text":""},{"location":"pages/usage/update-missiondb/#download-sql-files","title":"Download SQL files","text":"<p>Navigate to the Data Admin SQL folder (<code>Atlas/admin/sql/</code>).</p> <p>Download the files from the Data Admin SQL folder on Google Drive and save them to your local file system.</p> <p>Note</p> <p>If you need to create these files outside of the scheduled update process, see the instructions for ad-hoc updates, below.</p>"},{"location":"pages/usage/update-missiondb/#upload-to-mission-database","title":"Upload to Mission Database","text":"<p>Open pgAdmin and select the <code>missiondb</code> database.</p> <p>Launch the PSQL editor. The prompt should reference <code>missiondb</code>:    </p> <pre><code>missiondb#=\n</code></pre> <p>Copy/paste the code below into a new <code>.txt</code> document. Next, update the paths to each file to the full paths for your local file system using forward slashes (e.g., <code>sql/load-activity_data</code> to <code>C:/Users/Your_Name/atlas/sql/dml/load_activity_data.sql</code> ). Save the <code>.txt</code> file with these commands for later use.</p> <pre><code>\\encoding UTF8\n\\i full/path/to/load_activity_location_data.sql\n\\i full/path/to/load_thematic_data.sql\n\\i full/path/to/load_activity_data.sql\n</code></pre> <p>Copy/paste from the <code>.txt</code> file with the corrected paths into the PSQL editor prompt. Hit enter to run. If copying multiple lines at once, ensure the last line also runs by hitting enter again if needed.</p> <p>Warning</p> <p>If you are working on the Windows Operating System, change all backslashes to forward slashes in the path.</p> <p>Tip</p> <p>Google Sheets uses UTF8 encoding for CSV downloads. PostgreSQL defaults to the encoding of your system. If these two are not the same, you may experience errors on loading. Check the current encoding of the database with <code>\\encoding</code>. You must set the database encoding for each session! Learn more about encoding here.</p>"},{"location":"pages/usage/update-missiondb/#ad-hoc-update","title":"Ad-hoc update","text":"<p>To update the Mission Database when a data source is updated, re-create the <code>.sql</code> files corresponding to the data source that has been updated and then follow the steps defined above.</p> <p>Open the Data Steward Admin Tool (located at script.google.com).</p> <p>In the Editor window, locate the file corresponding to the data source that has been updated (i.e., <code>activity-data.gs</code> for the Activity Database) and select it.</p> <p>In the script editor menu bar, select the function that begins with <code>generate_</code> and click Run.</p> <p></p> <p>Proceed with the daily update process, downloading and uploading the <code>.sql</code> file that corresponds to the data source that has been updated.</p>"},{"location":"pages/usage/update-missiondb/#load-spatial-data","title":"Load spatial data","text":"<p>Spatial data include shapefiles and raster files. Augmenting tabular data with relevant spatial data will support geospatial analysis. The extensions installed previously including <code>postgis</code> and <code>postgis_raster</code> are needed to support spatial data in PostgreSQL. Spatial data may be read in using the command line or the application PostGIS Bundle Shapefile and DBF Importer which was installed with PostGIS. </p> <p>Spatial data must first be processed and stored in the Thematic Database before being read into the Mission Database. </p>"},{"location":"pages/usage/update-missiondb/#load-shapefiles","title":"Load shapefiles","text":"<p>Download the zipped shapefile(s) from the Spatial directory in the Thematic Database. Unzip the shapefile to your local file system. </p> <p>Open the PostGIS Bundle Shapefile and DBF Importer application.</p> <p>Connect to the <code>missiondb</code> by clicking View connection details and entering your login information. Click OK.</p> <p></p> <p>Click Add File and navigate to the location of your shapefile on the local file system. Repeat for all shapefiles.</p> <p>Set the SRID column for each file to <code>4326</code>. This corresponds with the WGS 84 coordinate reference system and should be set for the shapefile during the initial processing. The complete table should look like this:</p> Shapefile Schema Table Geo Column SRID Mode Rm /path/to/file.shp public table_name geom 4326 Create [ ] <p>Click Options and ensure all import options are correct. In particular, ensure the DBF file character encoding is the same as was used to save the spatial data after processing.</p> <p>Click Import.</p> <p>Tip</p> <p>If you cannot resolve encoding errors or any other issues when using PostGIS Bundle Shapefile and DBF Importer, try using the command line tool <code>shp2pgsql</code>. Find the instructions here.</p>"},{"location":"pages/usage/update-missiondb/#load-raster-data","title":"Load raster data","text":"<p>Advanced users may wish to load raster data into the database. Find the instructions here.</p>"},{"location":"pages/usage/update-thematicdb/","title":"Update the Thematic Database","text":"<p>Follow the guidance below to add new data to the thematic database or update existing data. The steps for each dataset will be unique; take care when processing data to ensure the accuracy of the processed data. Maintain a record of the processing steps. Ideally, create a repeatable workflow such that re-processing of the data can be accomplished quickly. For example, use a scripting language or create a Google Sheet that takes as input the data and uses formulas to output the processed data.</p> <p>Workflows are provided to </p> <ul> <li>Add tabular or spatial data</li> <li>Add reports</li> <li>Add a calculated field</li> <li>Add an external link</li> </ul> <p>Before updating the Thematic Database, review the considerations below.</p>"},{"location":"pages/usage/update-thematicdb/#considerations","title":"Considerations","text":""},{"location":"pages/usage/update-thematicdb/#which-data-should-be-stored","title":"Which data should be stored?","text":"<p>When you store data in the Thematic Database, you are signaling that the data are trustworthy and making a commitment to keeping the data up-to-date. If stakeholders do not trust the data in the Thematic Database, they will eventually stop referring to it. Therefore, consider the trustworthiness of the data source. Are the data trustworthy? Do the data represent the Mission's view of the subject of the data? Can you commit to updating the data source in a timely manner?</p> <p>The power of the Thematic Database is the ability to bring contextual information to decision making and analysis. Consider the usefulness of the data for the Mission. Will the data be used by stakeholders? Are the data at the right level of geographic detail for making decision? Most often, Mission stakeholders require sub-national level data to inform the geographic targeting of interventions. </p> <p>For datasets collected by partners through Activities, consider the secondary analysis value<sup>1</sup>. If the data will not have broad applicability outside of the Activity itself, it may be best to exclude it.</p> <p>Importantly, consider the license and sensitivity of the data. Do not distribute a dataset outside of the terms of your license to use it. Ensure data are properly attributed when required. </p>"},{"location":"pages/usage/update-thematicdb/#storage-formats","title":"Storage formats","text":"<p>Data should be stored in consistent formats with a preference toward open source formats whenever possible. (However, raw data should always be stored in the format it was accessed.)</p> <p>Within the thematic database, files are stored in the following formats and locations.</p> Data type Preferred Format Extension Location Tabular Google Sheet <code>.gsheet</code> <code>tabular/</code> Point, line, polygon zipped shapefile <code>.zip</code> <code>spatial/shapefiles</code> Gridded (raster) TIFF <code>.tif</code> <code>spatial/tif</code> <p>Shapefiles are stored as zipped files because zipped files display cleaning in Google Drive and zipped shapefiles can be read by many applications including Tableau. However, you may choose to also store the data in a repository of unzipped files or with specific software such as Esri ArcGIS Online, depending on how the data are used.</p>"},{"location":"pages/usage/update-thematicdb/#protecting-sensitive-data","title":"Protecting sensitive data","text":"<p>The Thematic Database includes the <code>Internal/</code> directory to store any data that should not be shared publicly or should be shared with constraints. Files in this directory should only be shared within the Agency, never publicly. The <code>SENSITIVE/</code> folder should only be shared with specific individuals. Review the README in the folder for specific distribution limitations.</p> <p>When adding data to the Data Catalog Google Sheet, do not check the box in the column 'Share Publicly?' if the data should not be made available publicly.</p>"},{"location":"pages/usage/update-thematicdb/#data-standards","title":"Data Standards","text":"<p>Whenever possible, use the Mission Data Standards when coding data. The Agency has data standards for gender, age and other variables. Missions will have additional data standards, including standard administrative boundary names.</p>"},{"location":"pages/usage/update-thematicdb/#encoding","title":"Encoding","text":"<p>Save files with encoding UTF-8.</p>"},{"location":"pages/usage/update-thematicdb/#translation","title":"Translation","text":"<p>If the data source is not in English, you may choose to translate the data to English depending on the audience and use cases (for example, any data submitted to the Development Data Library (DDL) for public sharing must be in English). Whenever data are translated, the translation should be stored with the data's metadata if not within the dataset itself.</p>"},{"location":"pages/usage/update-thematicdb/#workflows","title":"Workflows","text":""},{"location":"pages/usage/update-thematicdb/#data-flow-for-atlas","title":"Data flow for Atlas","text":"<p>Atlas employs a specific process for storing and cataloging data to maintain data provenance and support updates to the datasets over time. Data should be kept up to date if included in Atlas to maintain data trustworthiness.</p> <ol> <li>Once a dataset of interest has been identified, save the dataset in its unmanipulated form in the <code>raw/</code> data folder within the Thematic Database's <code>Internal/</code> directory. </li> <li> <p>Log the dataset in the Thematic Database's Internal Data Catalog (<code>Internal/internal - Data Catalog</code>). Include all available information. You will update the access link after processing.</p> </li> <li> <p>Process the data according to the process described in Processing steps (below).</p> </li> <li> <p>Save the process data in the Thematic Database under the appropriate folder in the Thematic Database based on access constraints and data format.</p> </li> <li>Update the access link in the Internal Data Catalog.</li> <li>Import the spatial file into the Mission Database (optional). See the Update Mission Database workflow for instructions.</li> </ol>"},{"location":"pages/usage/update-thematicdb/#using-the-data-catalog","title":"Using the Data Catalog","text":"<p>Always log new datasets in the Internal Data Catalog (<code>Internal/internal - Data Catalog</code>). Datasets for public sharing will be imported automatically into the publicly available Data Catalog. </p> <p>Before adding a dataset, add the dataset's provider to the <code>providers</code> sheet. </p> <p>See the <code>DEFINITIONS</code> tab for guidance on the use of each field.</p>"},{"location":"pages/usage/update-thematicdb/#add-tabular-data","title":"Add tabular data","text":"<p>Tabular data refers to any data that can be stored in a table. Given the ease of access for most users, we recommend storing as much data in tabular format as possible. For example, tabular data associated with spatial data can be extracted from the spatial data and stored in tabular format.</p> <p>Processing of tabular data may required specialized expertise and tools depending on the dataset size, source and format.</p>"},{"location":"pages/usage/update-thematicdb/#tabular-data-processing","title":"Tabular data processing","text":"<p>Warning</p> <p>Before continuing, review the general guidance in Data Flows for Atlas, above.</p> <ol> <li>Access the raw file in your data cleaning tool of choice (e.g., Google Sheets, Google Colab).</li> <li>Explore the dataset for data quality issues. Look for null values, values out of range, outliers, and other potential data quality issues. Use <code>#N/A</code> for null or missing values.</li> <li>Ensure data comply with the Data Standards. In particular, if sub-national administrative regions are included in the dataset, ensure they reflect the standard names.</li> <li>If necessary, normalize the data by splitting cells into multiple records or the table into multiple tables.</li> <li>Once processed, store the data in the appropriate Google Sheet within the Thematic Database. Use the <code>Shared Externally/Tabular</code> directory unless the data should not be shared publicly, in which case use the <code>Internal/Tabular</code> directory. </li> <li>Update <code>DEFINITIONS</code> tab of the Google Sheet to describe the fields in the new dataset. The <code>DEFINTIONS</code> tab is used by multiple applications in Atlas.</li> <li>Update the access link, worksheet ID and worksheet name in the Internal Data Catalog. </li> </ol> <p>It is highly recommended to save your processing steps for use later on to re-process the data if needed or update the dataset with more recent data once available.</p> <p>Note</p> <p>To learn more about working with null values in Atlas, see here.</p> <p>Tip</p> <p>You may find data in the proprietary formats of the software SPSS (<code>.sav</code>) or Stata (<code>.dta</code>). See this Google Colab notebook for guidance on reading data from these formats.</p>"},{"location":"pages/usage/update-thematicdb/#add-spatial-data","title":"Add spatial data","text":"<p>Processing spatial data requires the specialized expertise and tools of a GIS  Specialist. If you do not have access to the necessary tools or expertise, the GeoCenter can help you access Esri ArcGIS software and/or complete geospatial tasks. You may also be able to find these resources through a MEL Platform or a support services contract.</p> <p>The specific process will differ based on the Geographic Information System (GIS) tool used. We recommend setting up a workflow in your GIS tool of choice when processing the data so that data can be reprocessed quickly if needed. For example, create a Google Colab notebook with all processing steps or use Esri ArcGIS Pro's Model Builder to create a reproducible workflow. An example for the Guatemala Mission using Python can be found in this Google Colab notebook. </p> <p>Follow the data flow for atlas to download, log and upload the data. The processing steps below describe how to process raw spatial data files for use in Atlas.</p>"},{"location":"pages/usage/update-thematicdb/#spatial-data-processing-steps","title":"Spatial data processing steps","text":"<p>Warning</p> <p>Before continuing, review the general guidance in Data Flows for Atlas, above.</p> <ol> <li>Access the raw file in your GIS tool of choice.</li> <li>Visualize or otherwise review the data to familiarize yourself with the data and its attributes.</li> <li>If needed, construct a geometry for the file from spatial data in the file (most commonly from a latitude and longitude column).</li> <li>Check the coordinate reference system (CRS). If a CRS is not already associated with the data, review the data source's documentation and the data's metadata. See this Google Colab notebook for an example from Guatemala. </li> <li>Reproject to WGS 84 (EPSG code 4326). WGS 84 is the required CRS for geographic data submitted to the federal government (see ADS 579saa). In addition, spatial data in this CRS can be visualized directly within PostgreSQL and is the expected CRS for web mapping applications.</li> <li>Rename columns if needed to better communicate the contents of each field. Note that for shapefiles, column names greater than 10 characters will be truncated.</li> <li>Remap column values and data types if needed. Whenever possible, encode as much information as possible within the data itself. We are far more interested with human readability than with storage efficiency.</li> <li>Save the file in the appropriate file format and compress into a zipped file.</li> <li>Upload the file to the destination.</li> </ol>"},{"location":"pages/usage/update-thematicdb/#add-reports","title":"Add reports","text":"<p>Reports and analyses can be easily added to the Thematic Database, which will make them available through the Data Catalog web application.</p> <ol> <li>Store the report in the <code>Reports/</code> folder.</li> <li>Log the report in the Thematic Database's Internal Data Catalog (<code>Internal/internal - Data Catalog</code>).</li> </ol>"},{"location":"pages/usage/update-thematicdb/#add-a-calculated-field","title":"Add a calculated field","text":"<p>Adding a calculated field in the Thematic Database allows you to enrich the data and support users to visualize and make decisions from the calculated fields.</p> <ol> <li>Insert a column in the desired sheet within the Thematic Database.</li> <li>Use a descriptive column header that conforms to the Atlas naming conventions.</li> <li>Write the formula for the calculated field. (Use <code>Ctrl + Enter</code> to save the formula to all rows).</li> <li>Log the new column in the <code>DEFINITIONS</code> tab. In the <code>definition</code> field, describe how the calculation is completed. In the <code>dataset</code> field, append <code>[calcualted]</code> to the name of the data source to indicate that the data are derived from, not directly attributable to, that data source.</li> </ol> <p>We recommend leaving the equation in the sheet (rather than converting to plain text) so that other users may review how the calculation was completed.</p> <p>Tip</p> <p>Percentages should be scaled from 0 to 100 in the Thematic Database. For example, to calculate the percentage of English speakers per municipality, divide the number of English speakers by the population of the municipality and multiply by 100.</p>"},{"location":"pages/usage/update-thematicdb/#add-an-external-link","title":"Add an external link","text":"<p>In many cases, it is preferable to simply link to an external data source from the Data Catalog rather than read the data into the Thematic Database. Add an external link when the data source includes a useful dashboard or other data visualization platform or when you want to include the data but don't want to commit to keeping the data up-to-date. </p> <ol> <li>Log the data in the Thematic Database's Internal Data Catalog (<code>Internal/internal - Data Catalog</code>).</li> <li>Set the Access Link to be the same as the External Link.</li> </ol> <ol> <li> <p>Secondary analysis value indicates when a dataset might be used for purposes other than for which it was originally collected.\u00a0\u21a9</p> </li> </ol>"}]}